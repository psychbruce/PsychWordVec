% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/PsychWordVec.R
\name{train_wordvec}
\alias{train_wordvec}
\title{Train static word vectors using the Word2Vec, GloVe, or FastText algorithm.}
\usage{
train_wordvec(
  x,
  tokenizer,
  remove,
  method = c("word2vec", "glove", "fasttext"),
  dims = 300,
  window = 5,
  min.count = 5,
  iteration = 5,
  threads = 8,
  model = c("skip-gram", "cbow"),
  loss = c("ns", "hs"),
  negative = 5,
  subsample,
  learning = 0.05,
  ngrams = c(3, 6),
  stopwords = character(),
  encoding = "UTF-8",
  normalize = FALSE,
  file.save = NULL
)
}
\arguments{
\item{x}{A file path on disk containing the text
or a character vector of the text.}

\item{tokenizer}{Function used to tokenize the text.
Defaults to \code{\link[text2vec:tokenizers]{text2vec::word_tokenizer}}.}

\item{remove}{Strings (in regular expression) to be removed from the text.
Defaults to \code{"_|'|<br/>|<br />|e\\\\.g\\\\.|i\\\\.e\\\\."}.
You may turn off this by specifying \code{remove=NULL}.}

\item{method}{Training algorithm:
\itemize{
  \item{\code{"word2vec"} (default): using the \code{\link[word2vec:word2vec]{word2vec}} package}
  \item{\code{"glove"}: using the \code{\link[rsparse:GloVe]{rsparse}} package}
  \item{\code{"fasttext"}: using the \code{\link[fastTextR:ft_train]{fastTextR}} package}
}}

\item{dims}{Number of dimensions of word vectors to be trained.
Common choices include 50, 100, 200, 300, and 500.
Defaults to \code{300}.}

\item{window}{Window size (number of nearby words before/after the current word).
It defines how many surrounding words to be included in training:
[window] words behind and [window] words ahead (thus [window]*2 in total).
Defaults to \code{5}.}

\item{min.count}{Minimum frequency of words to be included in training.
Words that appear less than this value of times will be excluded from vocabulary.
Defaults to \code{5}.}

\item{iteration}{Number of training iterations.
More iterations makes a more precise model,
but computational cost is linearly proportional to iterations.
Defaults to \code{5}.}

\item{threads}{Number of CPU threads used for training.
A modest value produces the fastest training.
Too many threads are not always helpful.
Defaults to \code{8}.}

\item{model}{\strong{<Only for Word2Vec / FastText>}

Learning model architecture:
\itemize{
  \item{\code{"skip-gram"} (default): Skip-Gram, which predicts surrounding words given the current word}
  \item{\code{"cbow"}: Continuous Bag-of-Words, which predicts the current word based on the context}
}}

\item{loss}{\strong{<Only for Word2Vec / FastText>}

Loss function (computationally efficient approximation):
\itemize{
  \item{\code{"ns"} (default): Negative Sampling}
  \item{\code{"hs"}: Hierarchical Softmax}
}}

\item{negative}{\strong{<Only for Negative Sampling in Word2Vec / FastText>}

Number of negative examples.
Values in the range 5~20 are useful for small training datasets,
while for large datasets the value can be as small as 2~5.
Defaults to \code{5}.}

\item{subsample}{\strong{<Only for Word2Vec / FastText>}

Subsampling of frequent words (threshold for occurrence of words).
Those that appear with higher frequency in the training data will be randomly down-sampled.
Defaults to \code{0.001} for Word2Vec and \code{0.0001} for FastText.}

\item{learning}{\strong{<Only for Word2Vec / FastText>}

Initial (starting) learning rate, also known as alpha.
Defaults to \code{0.05}.}

\item{ngrams}{\strong{<Only for FastText>}

Minimal and maximal ngram length.
Defaults to \code{c(3, 6)}.}

\item{stopwords}{\strong{<Only for Word2Vec / GloVe>}

A character vector of stopwords to be excluded from training.}

\item{encoding}{Text encoding of \code{x} and \code{stop_words}.
Defaults to \code{"UTF-8"}.}

\item{normalize}{Normalize all word vectors to unit length?
Defaults to \code{FALSE}. See \code{\link{data_wordvec_normalize}}.}

\item{file.save}{File name of to-be-saved R data (must be .RData).}
}
\value{
A \code{data.table} (of new class \code{wordvec}) with two variables:
\describe{
  \item{\code{word}}{words}
  \item{\code{vec}}{\strong{raw} \emph{or} \strong{normalized} word vectors}
}
}
\description{
Train static word vectors using the
\code{\link[word2vec:word2vec]{Word2Vec}},
\code{\link[rsparse:GloVe]{GloVe}}, or
\code{\link[fastTextR:ft_train]{FastText}} algorithm.
}
\section{Download}{

Download pre-trained word vectors data (\code{.RData}):
\url{https://psychbruce.github.io/WordVector_RData.pdf}
}

\examples{
text = text2vec::movie_review
# View(text)

## Word2Vec (SGNS)
dt1 = train_wordvec(
  text$review,
  method="word2vec",
  model="skip-gram", loss="ns",
  dims=50, window=5,  # 50 dims for faster code check
  normalize=TRUE)

most_similar(dt1, "Ive")
most_similar(dt1, ~ man - he + she, topn=5)
most_similar(dt1, ~ boy - he + she, topn=5)

## GloVe

## FastText
dt3 = train_wordvec(
  text$review,
  method="fasttext",
  model="skip-gram", loss="ns",
  dims=50, window=5,  # 50 dims for faster code check
  normalize=TRUE)

most_similar(dt3, "Ive")
most_similar(dt3, ~ man - he + she, topn=5)
most_similar(dt3, ~ boy - he + she, topn=5)

}
\references{
All-in-one package:
\itemize{
  \item{\url{https://CRAN.R-project.org/package=wordsalad}}
}
Word2Vec:
\itemize{
  \item{\url{https://CRAN.R-project.org/package=word2vec}}
  \item{\url{https://github.com/maxoodf/word2vec}}
}
GloVe:
\itemize{
  \item{\url{https://CRAN.R-project.org/package=text2vec}}
  \item{\url{https://CRAN.R-project.org/package=rsparse}}
}
FastText:
\itemize{
  \item{\url{https://CRAN.R-project.org/package=fastTextR}}
}
}
\seealso{
\code{\link{tokenize}}
}
