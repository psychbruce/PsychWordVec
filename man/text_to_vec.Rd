% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/PWV2_dynamic.R
\name{text_to_vec}
\alias{text_to_vec}
\title{Extract contextualized word embeddings from transformers (pre-trained language models).}
\usage{
text_to_vec(text, model, layers = -2)
}
\arguments{
\item{text}{Can be:
\itemize{
  \item{a character string or vector of text (usually sentences)}
  \item{a file path on disk containing text}
  \item{a data frame with at least one character variable
  (for text from all character variables in a given data frame)}
}}

\item{model}{Model name at \href{https://huggingface.co/models}{HuggingFace}.
See \code{\link{text_model_download}}.
If the model has not been downloaded, it would automatically download the model.}

\item{layers}{Layers to be extracted from the \code{model},
which are then aggregated in the function
\code{\link[text:textEmbedLayerAggregation]{text::textEmbedLayerAggregation()}}.
Defaults to \code{-2} which extracts the second to last layers.
You may extract only the layers you need (e.g., \code{11:12}) or
all layers (\code{"all"}).
Note that layer 0 is the \emph{decontextualized} input layer
(i.e., not comprising hidden states) and is normally not used.}
}
\value{
A \code{list} of:
\describe{
  \item{\code{word.embed}}{
    Token (roughly word) embeddings}
  \item{\code{text.embed}}{
    Text embeddings, aggregated from token embeddings}
}
}
\description{
Extract model layers and aggregate them to
token (roughly word) embeddings and text embeddings
(all reshaped to \code{\link[PsychWordVec:as_wordvec]{wordvec}} data tables).
A wrapper function of \code{\link[text:textEmbed]{text::textEmbed()}}.
}
\examples{
\dontrun{
embed = text_to_vec(
  c("I love China.", "Beijing is the capital of China."),
  model="bert-base-cased"
)
embed$word.embed
embed$text.embed

embed1 = embed$word.embed[[1]]
embed2 = embed$word.embed[[2]]

View(embed1)
View(embed2)
View(embed$text.embed)
}

}
