[{"path":"https://psychbruce.github.io/PsychWordVec/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Han-Wu-Shuang Bao. Author, maintainer.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Bao H (2022). PsychWordVec: Toolkit Word Embedding Research Psychological Science. R package version 0.0.6, https://psychbruce.github.io/PsychWordVec/.","code":"@Manual{,   title = {PsychWordVec: Toolkit for Word Embedding Research in Psychological Science},   author = {Han-Wu-Shuang Bao},   year = {2022},   note = {R package version 0.0.6},   url = {https://psychbruce.github.io/PsychWordVec/}, }"},{"path":"https://psychbruce.github.io/PsychWordVec/index.html","id":"psychwordvec","dir":"","previous_headings":"","what":"Toolkit for Word Embedding Research in Psychological Science","title":"Toolkit for Word Embedding Research in Psychological Science","text":"Toolkit Word Embedding Research Psychological Science. integrated word-vector toolkit provides: (1) collection pre-trained word vectors .RData format (WordVector_RData.pdf); (2) variety functions processing, analyzing, visualizing word vectors (e.g., find similar words given word); (3) range tests (e.g., Word-Embedding Association Test) examining psychological effects (e.g., implicit attitude/bias); (4) set training methods (word2vec, GloVe, fastText) learning word vectors new text corpora (coming soon).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/index.html","id":"author","dir":"","previous_headings":"","what":"Author","title":"Toolkit for Word Embedding Research in Psychological Science","text":"Han-Wu-Shuang (Bruce) Bao 包寒吴霜 Email: baohws@foxmail.com Homepage: psychbruce.github.io","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Toolkit for Word Embedding Research in Psychological Science","text":"Bao, H.-W.-S. (2022). PsychWordVec: Toolkit word embedding research psychological science. R package version 0.0.x. https://CRAN.R-project.org/package=PsychWordVec","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/cosine_similarity.html","id":null,"dir":"Reference","previous_headings":"","what":"Cosine similarity/distance between two vectors. — cosine_similarity","title":"Cosine similarity/distance between two vectors. — cosine_similarity","text":"Cosine similarity/distance two vectors.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/cosine_similarity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cosine similarity/distance between two vectors. — cosine_similarity","text":"","code":"cosine_similarity(v1, v2, distance = FALSE)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/cosine_similarity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cosine similarity/distance between two vectors. — cosine_similarity","text":"v1, v2 Numeric vector (length). distance Compute cosine distance instead? Default FALSE (cosine similarity).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/cosine_similarity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cosine similarity/distance between two vectors. — cosine_similarity","text":"value cosine similarity/distance.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/cosine_similarity.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cosine similarity/distance between two vectors. — cosine_similarity","text":"Cosine similarity = sum(v1 * v2) / ( sqrt(sum(v1^2)) * sqrt(sum(v2^2)) ) Cosine distance = 1 - cosine_similarity(v1, v2)","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/cosine_similarity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cosine similarity/distance between two vectors. — cosine_similarity","text":"","code":"cosine_similarity(v1=c(1,1,1), v2=c(2,2,2))  # 1 #> [1] 1 cosine_similarity(v1=c(1,4,1), v2=c(4,1,1))  # 0.5 #> [1] 0.5 cosine_similarity(v1=c(1,1,0), v2=c(0,0,1))  # 0 #> [1] 0  cosine_similarity(v1=c(1,1,1), v2=c(2,2,2), distance=TRUE)  # 0 #> [1] -2.220446e-16 cosine_similarity(v1=c(1,4,1), v2=c(4,1,1), distance=TRUE)  # 0.5 #> [1] 0.5 cosine_similarity(v1=c(1,1,0), v2=c(0,0,1), distance=TRUE)  # 1 #> [1] 1"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_transform.html","id":null,"dir":"Reference","previous_headings":"","what":"Transform plain text data of word vectors into a compressed ","title":"Transform plain text data of word vectors into a compressed ","text":"Transform plain text data word vectors compressed \".RData\" file. Speed: total (preprocess + compress + save), can process 30000 words per minute slowest settings (compress=\"xz\", compress.level=9) modern computer (HP ProBook 450, Windows 11, Intel i7-1165G7 CPU, 32GB RAM, power supply mode).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_transform.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Transform plain text data of word vectors into a compressed ","text":"","code":"data_transform(   file.load,   file.save = NULL,   encoding = \"auto\",   sep = \" \",   header = \"auto\",   compress = \"bzip2\",   compress.level = 9 )"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_transform.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Transform plain text data of word vectors into a compressed ","text":"file.load File name raw data (must plain text). Data must format (values separated sep): cat 0.001 0.002 0.003 0.004 0.005 ... 0.300 dog 0.301 0.302 0.303 0.304 0.305 ... 0.600 file.save File name --saved R data (must .RData). encoding File encoding. Default \"auto\" (using vroom::vroom_lines() fast read file). specified value (e.g., \"UTF-8\"), uses readLines() read file, much slower vroom. sep Column separator. Default \" \". header 1st row header (e.g., meta-information \"2000000 300\")? Default \"auto\", automatically determines whether header. TRUE, 1st row dropped. compress Compression method saved file. Default \"bzip2\". Options can : 1 \"gzip\": modest file size (fastest) 2 \"bzip2\": small file size (fast) 3 \"xz\": minimized file size (slow) compress.level Compression level 0 (none) 9 (maximal compression minimal file size). Default 9.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_transform.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Transform plain text data of word vectors into a compressed ","text":"data.table two variables: word vec.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_transform.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Transform plain text data of word vectors into a compressed ","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_load.html","id":null,"dir":"Reference","previous_headings":"","what":"Load word vectors data from an ","title":"Load word vectors data from an ","text":"Load word vectors data \".RData\" file.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_load.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load word vectors data from an ","text":"","code":"data_wordvec_load(file.load, normalize = FALSE)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_load.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load word vectors data from an ","text":"file.load File name (must .RData transformed data_transform, two variables word vec). normalize Normalize word vectors unit length? Default FALSE. See data_wordvec_normalize.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_load.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load word vectors data from an ","text":"data.table two variables: word words vec raw normalized word vectors","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_load.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Load word vectors data from an ","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_normalize.html","id":null,"dir":"Reference","previous_headings":"","what":"Normalize all word vectors to unit length. — data_wordvec_normalize","title":"Normalize all word vectors to unit length. — data_wordvec_normalize","text":"L2-normalization (scaling unit euclidean length): norm vector vector space normalized 1. R formula: normalized_vec = vec / sqrt(sum(vec^2)) Note: Normalization change results cosine similarity can make computation faster.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_normalize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Normalize all word vectors to unit length. — data_wordvec_normalize","text":"","code":"data_wordvec_normalize(data)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_normalize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Normalize all word vectors to unit length. — data_wordvec_normalize","text":"data data.table variables word vec loaded data_wordvec_load.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_normalize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Normalize all word vectors to unit length. — data_wordvec_normalize","text":"data.table normalized word vectors.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_normalize.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Normalize all word vectors to unit length. — data_wordvec_normalize","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_normalize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Normalize all word vectors to unit length. — data_wordvec_normalize","text":"","code":"d = data_wordvec_normalize(demodata) #> √ All word vectors have now been normalized.  data_wordvec_normalize(d)  # already normalized #> √ Word vectors have already been normalized."},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_subset.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract a subset of word vectors data. — data_wordvec_subset","title":"Extract a subset of word vectors data. — data_wordvec_subset","text":"Extract subset word vectors data. may specify either data.table loaded data_wordvec_load) .RData file transformed data_transform).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_subset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract a subset of word vectors data. — data_wordvec_subset","text":"","code":"data_wordvec_subset(   x,   words = NULL,   pattern = NULL,   file.save = NULL,   compress = \"bzip2\",   compress.level = 9 )"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_subset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract a subset of word vectors data. — data_wordvec_subset","text":"x Can one following: data.table loaded data_wordvec_load .RData file transformed data_transform words [Option 1] Word string (NULL; single word; vector words). pattern [Option 2] Pattern regular expression (see str_subset). file.save File name --saved R data (must .RData). compress Compression method saved file. Default \"bzip2\". Options can : 1 \"gzip\": modest file size (fastest) 2 \"bzip2\": small file size (fast) 3 \"xz\": minimized file size (slow) compress.level Compression level 0 (none) 9 (maximal compression minimal file size). Default 9.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_subset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract a subset of word vectors data. — data_wordvec_subset","text":"subset word vectors data valid (available) words.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_subset.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Extract a subset of word vectors data. — data_wordvec_subset","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_subset.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract a subset of word vectors data. — data_wordvec_subset","text":"","code":"## specify `x` as a data.table: d = data_wordvec_subset(demodata, c(\"China\", \"Japan\", \"Korea\")) d #>     word #> 1: China #> 2: Japan #> 3: Korea #>                                                                                  vec #> 1:       -0.02697318, 0.04999036, 0.04010034, 0.03056975,-0.04711339,-0.08379688,... #> 2:        0.01832947, 0.09023783, 0.06027598, 0.03049064,-0.09587769,-0.02784667,... #> 3:  0.033778005, 0.133294179,-0.021963288, 0.001675763,-0.014011015,-0.112693960,...  ## specify `x` and `pattern`, and save with `file.save`: data_wordvec_subset(demodata, pattern=\"Chin[ae]|Japan|Korea\",                     file.save=\"subset.RData\") #> 0 words are matched... #>  #> Compressing and saving... #> √ Saved to \"subset.RData\" (time cost = 0.005 secs)  ## load the subset: d.subset = data_wordvec_load(\"subset.RData\") #> Loading... √ Word vector data: 6 words, 300 dims (loading time: 0.08 secs) d.subset #>        word #> 1:    China #> 2:    Japan #> 3:  Chinese #> 4: Japanese #> 5:   Korean #> 6:    Korea #>                                                                                  vec #> 1:       -0.02697318, 0.04999036, 0.04010034, 0.03056975,-0.04711339,-0.08379688,... #> 2:        0.01832947, 0.09023783, 0.06027598, 0.03049064,-0.09587769,-0.02784667,... #> 3:       -0.04985132, 0.04593451, 0.07940573, 0.02581589,-0.03204736,-0.09044442,... #> 4:  0.003658937, 0.091899336, 0.062968012, 0.029782102,-0.093941412,-0.062627841,... #> 5:        0.02635512, 0.11745195, 0.03810029, 0.02850357,-0.05127785,-0.08995103,... #> 6:  0.033778005, 0.133294179,-0.021963288, 0.001675763,-0.014011015,-0.112693960,...  ## specify `x` as an .RData file and save with `file.save`: data_wordvec_subset(\"subset.RData\",                     words=c(\"China\", \"Chinese\"),                     file.save=\"new.subset.RData\") #> Loading... √ Word vector data: 6 words, 300 dims (loading time: 0.08 secs) #>  #> Compressing and saving... #> √ Saved to \"new.subset.RData\" (time cost = 0.003 secs) d.new.subset = data_wordvec_load(\"new.subset.RData\") #> Loading... √ Word vector data: 2 words, 300 dims (loading time: 0.08 secs) d.new.subset #>       word #> 1:   China #> 2: Chinese #>                                                                            vec #> 1: -0.02697318, 0.04999036, 0.04010034, 0.03056975,-0.04711339,-0.08379688,... #> 2: -0.04985132, 0.04593451, 0.07940573, 0.02581589,-0.03204736,-0.09044442,...  unlink(\"subset.RData\")  # delete file for code check unlink(\"new.subset.RData\")  # delete file for code check"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/demodata.html","id":null,"dir":"Reference","previous_headings":"","what":"Demo data (corpus: Google News; algorithm: word2vec; vocabulary: 8000; dimensions: 300). — demodata","title":"Demo data (corpus: Google News; algorithm: word2vec; vocabulary: 8000; dimensions: 300). — demodata","text":"demo data contains sample 8000 English words 300-d word embeddings (word vectors) trained using \"word2vec\" algorithm based Google News corpus. words Top 8000 frequent wordlist, whereas selected less frequent words appended.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/demodata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Demo data (corpus: Google News; algorithm: word2vec; vocabulary: 8000; dimensions: 300). — demodata","text":"","code":"data(demodata)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/demodata.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Demo data (corpus: Google News; algorithm: word2vec; vocabulary: 8000; dimensions: 300). — demodata","text":"data.table two variables word vec, transformed raw data (see URL Source) .RData using data_transform function.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/demodata.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Demo data (corpus: Google News; algorithm: word2vec; vocabulary: 8000; dimensions: 300). — demodata","text":"Google Code - word2vec (https://code.google.com/archive/p/word2vec/)","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/get_wordvec.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract word vector of a single word. — get_wordvec","title":"Extract word vector of a single word. — get_wordvec","text":"Extract word vector single word.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/get_wordvec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract word vector of a single word. — get_wordvec","text":"","code":"get_wordvec(data, word)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/get_wordvec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract word vector of a single word. — get_wordvec","text":"data data.table variables word vec loaded data_wordvec_load. word Word string (single word).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/get_wordvec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract word vector of a single word. — get_wordvec","text":"numeric vector word (NA word data).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/get_wordvec.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Extract word vector of a single word. — get_wordvec","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/get_wordvec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract word vector of a single word. — get_wordvec","text":"","code":"d = data_wordvec_normalize(demodata) #> √ All word vectors have now been normalized.  v1 = get_wordvec(demodata, \"China\")  # raw vector v2 = get_wordvec(d, \"China\")  # normalized vector cor(v1, v2) #> [1] 1 cosine_similarity(v1, v2) #> [1] 1"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/get_wordvecs.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract word vectors of multiple words. — get_wordvecs","title":"Extract word vectors of multiple words. — get_wordvecs","text":"Extract word vectors multiple words, using either wordlist (vector words; using words) regular expression (pattern words; using pattern). (words pattern) specified, words wins.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/get_wordvecs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract word vectors of multiple words. — get_wordvecs","text":"","code":"get_wordvecs(   data,   words = NULL,   pattern = NULL,   plot = FALSE,   plot.dims = NULL,   plot.step = 0.05,   plot.border = \"white\" )"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/get_wordvecs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract word vectors of multiple words. — get_wordvecs","text":"data data.table variables word vec loaded data_wordvec_load. words [Option 1] Word string (NULL; single word; vector words). pattern [Option 2] Pattern regular expression (see str_subset). plot Generate plot illustrate word vectors? Default FALSE. plot.dims Dimensions plotted (e.g., 1:100). Default NULL (plot dimensions). plot.step Step value breaks. Default 0.05. plot.border Color tile border. Default \"white\". remove border color, set plot.border=NA.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/get_wordvecs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract word vectors of multiple words. — get_wordvecs","text":"data.table words columns dimensions rows.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/get_wordvecs.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Extract word vectors of multiple words. — get_wordvecs","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/get_wordvecs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract word vectors of multiple words. — get_wordvecs","text":"","code":"d = data_wordvec_normalize(demodata) #> √ All word vectors have now been normalized.  get_wordvecs(d, c(\"China\", \"Japan\", \"Korea\")) #>            China        Japan        Korea #>   1: -0.02697318  0.018329469  0.033778005 #>   2:  0.04999036  0.090237829  0.133294179 #>   3:  0.04010034  0.060275982 -0.021963288 #>   4:  0.03056975  0.030490640  0.001675763 #>   5: -0.04711339 -0.095877693 -0.014011015 #>  ---                                       #> 296: -0.01393625 -0.087418077 -0.072705831 #> 297: -0.06761304 -0.105042608 -0.115117643 #> 298:  0.05070960  0.048291315  0.015374394 #> 299:  0.03452576  0.002103985 -0.043017655 #> 300: -0.02931099  0.008636121  0.077552886 get_wordvecs(d, cc(\" China, Japan; Korea \")) #>            China        Japan        Korea #>   1: -0.02697318  0.018329469  0.033778005 #>   2:  0.04999036  0.090237829  0.133294179 #>   3:  0.04010034  0.060275982 -0.021963288 #>   4:  0.03056975  0.030490640  0.001675763 #>   5: -0.04711339 -0.095877693 -0.014011015 #>  ---                                       #> 296: -0.01393625 -0.087418077 -0.072705831 #> 297: -0.06761304 -0.105042608 -0.115117643 #> 298:  0.05070960  0.048291315  0.015374394 #> 299:  0.03452576  0.002103985 -0.043017655 #> 300: -0.02931099  0.008636121  0.077552886  ## specify `pattern`: get_wordvecs(d, pattern=\"Chin[ae]|Japan|Korea\") #> 0 words are matched... #>            China        Japan     Chinese     Japanese      Korean        Korea #>   1: -0.02697318  0.018329469 -0.04985132  0.003658937  0.02635512  0.033778005 #>   2:  0.04999036  0.090237829  0.04593451  0.091899336  0.11745195  0.133294179 #>   3:  0.04010034  0.060275982  0.07940573  0.062968012  0.03810029 -0.021963288 #>   4:  0.03056975  0.030490640  0.02581589  0.029782102  0.02850357  0.001675763 #>   5: -0.04711339 -0.095877693 -0.03204736 -0.093941412 -0.05127785 -0.014011015 #>  ---                                                                            #> 296: -0.01393625 -0.087418077  0.04593451 -0.055820224 -0.05213705 -0.072705831 #> 297: -0.06761304 -0.105042608 -0.06907951 -0.119128757 -0.08479435 -0.115117643 #> 298:  0.05070960  0.048291315  0.06694317  0.074540472  0.02363348  0.015374394 #> 299:  0.03452576  0.002103985  0.05448025  0.001999554 -0.05614764 -0.043017655 #> 300: -0.02931099  0.008636121 -0.03275947  0.009104821  0.03867319  0.077552886  ## plot word vectors: get_wordvecs(d, cc(\"China, Japan, Korea,                     Mac, Linux, Windows\"),              plot=TRUE, plot.dims=1:100)  #>            China        Japan       Windows        Korea          Mac #>   1: -0.02697318  0.018329469  0.0596512714  0.033778005 -0.017270569 #>   2:  0.04999036  0.090237829 -0.0797136210  0.133294179 -0.012575617 #>   3:  0.04010034  0.060275982 -0.0818534443 -0.021963288 -0.052985444 #>   4:  0.03056975  0.030490640 -0.0143777935  0.001675763  0.078136677 #>   5: -0.04711339 -0.095877693 -0.0738285592 -0.014011015 -0.057009737 #>  ---                                                                  #> 296: -0.01393625 -0.087418077  0.0235396994 -0.072705831 -0.088532396 #> 297: -0.06761304 -0.105042608  0.0005308471 -0.115117643 -0.063716664 #> 298:  0.05070960  0.048291315  0.0098973674  0.015374394 -0.005407505 #> 299:  0.03452576  0.002103985  0.0572438332 -0.043017655 -0.016767489 #> 300: -0.02931099  0.008636121  0.1321425780  0.077552886  0.040912906 #>             Linux #>   1:  0.068922067 #>   2: -0.118698961 #>   3: -0.071656892 #>   4:  0.030358376 #>   5: -0.045400893 #>  ---              #> 296: -0.001282133 #> 297:  0.025572363 #> 298:  0.072751102 #> 299: -0.063451858 #> 300:  0.056341033  ## a more complex example:  words = cc(\" China Chinese Japan Japanese good bad great terrible morning evening king queen man woman he she cat dog \")  dt = get_wordvecs(   d, words,   plot=TRUE,   plot.dims=1:100,   plot.step=0.06)   if (FALSE) {  # if you want to change something: attr(dt, \"ggplot\") +   scale_fill_viridis_b(n.breaks=10, show.limits=TRUE) +   theme(legend.key.height=unit(0.1, \"npc\"))  # or to save the plot: ggsave(attr(dt, \"ggplot\"),        filename=\"wordvecs.png\",        width=8, height=5, dpi=500) }"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/most_similar.html","id":null,"dir":"Reference","previous_headings":"","what":"Find the Top-N most similar words. — most_similar","title":"Find the Top-N most similar words. — most_similar","text":"Find Top-N similar words, replicates results produced Python gensim module most_similar() function. (Exact replication gensim requires word vectors data, demodata used examples.)","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/most_similar.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find the Top-N most similar words. — most_similar","text":"","code":"most_similar(data, x, keep = FALSE, topn = 10, above = NULL)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/most_similar.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find the Top-N most similar words. — most_similar","text":"data data.table variables word vec loaded data_wordvec_load. x Can one following: single word:  \"China\" list words:  c(\"king\", \"queen\") cc(\" king , queen ; man | woman\") R formula (~ xxx) specifying   words positively negatively   contribute similarity (word analogy):  ~ boy - +  ~ king - man + woman  ~ Beijing - China + Japan keep Keep words specified x results? Default FALSE. topn Top-N similar words. Default 10. Default NULL. Can one following: threshold value find words cosine similarities   higher value critical word find words cosine similarities   higher critical word topn specified, wins.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/most_similar.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find the Top-N most similar words. — most_similar","text":"data.table similar words cosine similarities. row number word raw data also returned, may help determine relative word frequency cases. Two attributes appended returned data.table (see examples): wordvec wordvec.formula. Users may extract use.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/most_similar.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Find the Top-N most similar words. — most_similar","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/most_similar.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find the Top-N most similar words. — most_similar","text":"","code":"d = data_wordvec_normalize(demodata) #> √ All word vectors have now been normalized.  most_similar(d, \"China\") #> [Word Vector] =~ China #> (normalized to unit length) #>          word   cos_sim row_id #>  1:   Chinese 0.7678081    924 #>  2:   Beijing 0.7648461   2086 #>  3:    Taiwan 0.7081156   3011 #>  4:  Shanghai 0.6727434   3932 #>  5:  Shenzhen 0.6239033   7990 #>  6: Guangzhou 0.6223897   7992 #>  7:      yuan 0.6005429   4619 #>  8:     India 0.6004212    485 #>  9:     Japan 0.5967756    821 #> 10:        Li 0.5882897   7559 most_similar(d, c(\"king\", \"queen\")) #> [Word Vector] =~ king + queen #> (normalized to unit length) #>          word   cos_sim row_id #>  1:     royal 0.5985594   6754 #>  2:     Queen 0.5487501   4267 #>  3:      King 0.4662653   1449 #>  4:    legend 0.3730853   5428 #>  5:     crown 0.3673733   5174 #>  6: superstar 0.3652350   7218 #>  7:  champion 0.3587134   1509 #>  8:      lady 0.3581903   4631 #>  9:     lover 0.3425792   7952 #> 10:      pope 0.3420032   6388 most_similar(d, cc(\" king , queen ; man | woman \")) #> [Word Vector] =~ king + queen + man + woman #> (normalized to unit length) #>            word   cos_sim row_id #>  1:        girl 0.6387542   1257 #>  2:         boy 0.5966774   1379 #>  3:    teenager 0.5941694   4345 #>  4:        lady 0.5923386   4631 #>  5: grandmother 0.5081522   5364 #>  6:      mother 0.5000261    754 #>  7:       lover 0.4826535   7952 #>  8:      victim 0.4806608   1537 #>  9:   boyfriend 0.4717627   4811 #> 10:  girlfriend 0.4709186   3922  # the same as above: most_similar(d, ~ China) #> [Word Vector] =~ China #> (normalized to unit length) #>          word   cos_sim row_id #>  1:   Chinese 0.7678081    924 #>  2:   Beijing 0.7648461   2086 #>  3:    Taiwan 0.7081156   3011 #>  4:  Shanghai 0.6727434   3932 #>  5:  Shenzhen 0.6239033   7990 #>  6: Guangzhou 0.6223897   7992 #>  7:      yuan 0.6005429   4619 #>  8:     India 0.6004212    485 #>  9:     Japan 0.5967756    821 #> 10:        Li 0.5882897   7559 most_similar(d, ~ king + queen) #> [Word Vector] =~ king + queen #> (normalized to unit length) #>          word   cos_sim row_id #>  1:     royal 0.5985594   6754 #>  2:     Queen 0.5487501   4267 #>  3:      King 0.4662653   1449 #>  4:    legend 0.3730853   5428 #>  5:     crown 0.3673733   5174 #>  6: superstar 0.3652350   7218 #>  7:  champion 0.3587134   1509 #>  8:      lady 0.3581903   4631 #>  9:     lover 0.3425792   7952 #> 10:      pope 0.3420032   6388 most_similar(d, ~ king + queen + man + woman) #> [Word Vector] =~ king + queen + man + woman #> (normalized to unit length) #>            word   cos_sim row_id #>  1:        girl 0.6387542   1257 #>  2:         boy 0.5966774   1379 #>  3:    teenager 0.5941694   4345 #>  4:        lady 0.5923386   4631 #>  5: grandmother 0.5081522   5364 #>  6:      mother 0.5000261    754 #>  7:       lover 0.4826535   7952 #>  8:      victim 0.4806608   1537 #>  9:   boyfriend 0.4717627   4811 #> 10:  girlfriend 0.4709186   3922  most_similar(d, ~ boy - he + she) #> [Word Vector] =~ boy - he + she #> (normalized to unit length) #>            word   cos_sim row_id #>  1:        girl 0.8635271   1257 #>  2:       woman 0.6822032    558 #>  3:      mother 0.6530156    754 #>  4:    daughter 0.6431009   1078 #>  5:    teenager 0.6310561   4345 #>  6:       child 0.5933921    702 #>  7: grandmother 0.5800967   5364 #>  8:        teen 0.5755065   3542 #>  9:        baby 0.5672891   1838 #> 10:       girls 0.5599151   1195 most_similar(d, ~ Jack - he + she) #> [Word Vector] =~ Jack - he + she #> (normalized to unit length) #>        word   cos_sim row_id #>  1:    Jane 0.6338590   5502 #>  2: Rebecca 0.6054167   6896 #>  3:   Julie 0.5988102   5721 #>  4:   Sarah 0.5867900   3717 #>  5:     Amy 0.5831094   4899 #>  6:   Susan 0.5812214   4141 #>  7:    Lisa 0.5766206   4521 #>  8:     Ann 0.5765654   4955 #>  9:   Alice 0.5649283   7340 #> 10:   Carol 0.5647291   5861 most_similar(d, ~ Rose - she + he) #> [Word Vector] =~ Rose - she + he #> (normalized to unit length) #>         word   cos_sim row_id #>  1:  Leonard 0.4443190   6245 #>  2:   Martin 0.4206247   1425 #>  3:   Thomas 0.4078561   1214 #>  4:  Wallace 0.3833057   4350 #>  5:  Francis 0.3793813   5220 #>  6:  Johnson 0.3757961    761 #>  7:    Allen 0.3736620   2257 #>  8: Robinson 0.3725750   2698 #>  9:    Evans 0.3639734   3379 #> 10:   Duncan 0.3635448   4738  most_similar(d, ~ king - man + woman) #> [Word Vector] =~ king - man + woman #> (normalized to unit length) #>         word   cos_sim row_id #>  1:    queen 0.7118192   7855 #>  2:    royal 0.4938203   6754 #>  3:    Queen 0.4346379   4267 #>  4:     King 0.3749903   1449 #>  5:      she 0.3341126     65 #>  6:     lady 0.3282869   4631 #>  7:   mother 0.3241257    754 #>  8:    crown 0.3164823   5174 #>  9:     hers 0.3073009   7987 #> 10: daughter 0.3021213   1078 most_similar(d, ~ Tokyo - Japan + China) #> [Word Vector] =~ Tokyo - Japan + China #> (normalized to unit length) #>          word   cos_sim row_id #>  1:   Beijing 0.8216199   2086 #>  2:  Shanghai 0.7951419   3932 #>  3: Guangzhou 0.6529652   7992 #>  4:   Chinese 0.6439487    924 #>  5:  Shenzhen 0.6439113   7990 #>  6:     Seoul 0.5868835   5180 #>  7:      yuan 0.5821307   4619 #>  8:        Li 0.5712056   7559 #>  9:      Wang 0.5192575   7083 #> 10:    Moscow 0.5082187   3163 most_similar(d, ~ Beijing - China + Japan) #> [Word Vector] =~ Beijing - China + Japan #> (normalized to unit length) #>          word   cos_sim row_id #>  1:     Tokyo 0.8115592   3017 #>  2:     Seoul 0.6568831   5180 #>  3:  Japanese 0.6475989   1562 #>  4: Pyongyang 0.5348969   6322 #>  5:   Bangkok 0.4677356   6510 #>  6:     Korea 0.4660699   3768 #>  7:       yen 0.4631333   2695 #>  8:    Taiwan 0.4330458   3011 #>  9:    Moscow 0.4217667   3163 #> 10: Guangzhou 0.4154183   7992  most_similar(d, \"China\", above=0.7) #> [Word Vector] =~ China #> (normalized to unit length) #>       word   cos_sim row_id #> 1: Chinese 0.7678081    924 #> 2: Beijing 0.7648461   2086 #> 3:  Taiwan 0.7081156   3011 most_similar(d, \"China\", above=\"Shanghai\") #> [Word Vector] =~ China #> (normalized to unit length) #>        word   cos_sim row_id #> 1:  Chinese 0.7678081    924 #> 2:  Beijing 0.7648461   2086 #> 3:   Taiwan 0.7081156   3011 #> 4: Shanghai 0.6727434   3932  # automatically normalized for more accurate results ms = most_similar(demodata, ~ king - man + woman) #> * Results may be inaccurate if word vectors are not normalized. #> √ All word vectors have now been normalized. #> [Word Vector] =~ king - man + woman #> (normalized to unit length) ms #>         word   cos_sim row_id #>  1:    queen 0.7118192   7855 #>  2:    royal 0.4938203   6754 #>  3:    Queen 0.4346379   4267 #>  4:     King 0.3749903   1449 #>  5:      she 0.3341126     65 #>  6:     lady 0.3282869   4631 #>  7:   mother 0.3241257    754 #>  8:    crown 0.3164823   5174 #>  9:     hers 0.3073009   7987 #> 10: daughter 0.3021213   1078 str(ms) #> Classes ‘data.table’ and 'data.frame':\t10 obs. of  3 variables: #>  $ word   : chr  \"queen\" \"royal\" \"Queen\" \"King\" ... #>  $ cos_sim: num  0.712 0.494 0.435 0.375 0.334 ... #>  $ row_id : int  7855 6754 4267 1449 65 4631 754 5174 7987 1078 #>  - attr(*, \".internal.selfref\")=<externalptr>  #>  - attr(*, \"dims\")= int 300 #>  - attr(*, \"normalized\")= logi TRUE #>  - attr(*, \"wordvec\")= num [1:300] -0.0055 -0.06705 -0.04519 0.03875 -0.00286 ... #>  - attr(*, \"wordvec.formula\")=Class 'formula'  language ~king - man + woman #>   .. ..- attr(*, \".Environment\")=<environment: 0x55fb62df8e10>  attr(ms, \"dims\") #> [1] 300 attr(ms, \"normalized\") #> [1] TRUE attr(ms, \"wordvec.formula\") #> ~king - man + woman #> <environment: 0x55fb62df8e10> attr(ms, \"wordvec\") #>   [1] -0.0055029992 -0.0670502053 -0.0451917763  0.0387523404 -0.0028627068 #>   [6] -0.0311541918  0.0722744639 -0.0547962213 -0.0020958381  0.1267937027 #>  [11] -0.1177978229 -0.1116193754 -0.0432157999  0.0087648568 -0.0212594939 #>  [16] -0.0361886454 -0.0145757764 -0.0391255380  0.0236719225  0.0679695260 #>  [21]  0.0825006128  0.0179503178 -0.0102744498  0.0096805959  0.0097418516 #>  [26] -0.0583066547 -0.1043284916  0.0066451406  0.0774529645 -0.0630043398 #>  [31]  0.0176932143 -0.0357660002 -0.0127675640  0.0952755743 -0.1004202304 #>  [36] -0.0307725420  0.0191817489  0.0342936599 -0.0381161430 -0.0036010425 #>  [41] -0.0222736867 -0.0654523468  0.0581761410  0.0386649344  0.0625108822 #>  [46]  0.0144455335 -0.0298451219  0.0179820393 -0.0070950816  0.0410382461 #>  [51]  0.0193902936  0.0009054039 -0.0824461123  0.0896711960 -0.1071550823 #>  [56]  0.0090518215 -0.0452947722 -0.0242425101  0.0499748883  0.0017788248 #>  [61]  0.0896162255  0.0778694536  0.0043599153  0.0777055479  0.0090822422 #>  [66] -0.0272763870  0.0142941177  0.0595841030 -0.0188500428  0.0720441406 #>  [71]  0.0662750676  0.0333189751 -0.0275026411  0.0473774977  0.0302065682 #>  [76]  0.0363555415 -0.0305552844  0.0234168307  0.1017575023  0.0411732703 #>  [81]  0.0094623327  0.0278133126 -0.0067087575  0.0304040180 -0.0629132792 #>  [86]  0.0630248067 -0.0340541402  0.0388550926  0.0546983130  0.0218934638 #>  [91]  0.0433205797 -0.1001378189 -0.0860689768 -0.0450885649  0.0072867091 #>  [96]  0.0317569448  0.0645638429 -0.0160440819  0.0639956819 -0.0188932674 #> [101]  0.0053826412 -0.0191800667 -0.0261438897 -0.1247332173 -0.1010494892 #> [106]  0.0587875995 -0.1849463247 -0.0674423763  0.0657176846 -0.0205700926 #> [111]  0.0666514408  0.0815598320  0.0569012270 -0.0101953971  0.0484093475 #> [116] -0.1185521372  0.0030313936 -0.0933326263  0.0544090501  0.0927743854 #> [121]  0.0572259111  0.0394707134 -0.0271925329 -0.0265718034  0.0412028096 #> [126]  0.0438836936  0.0418592306  0.0034390046  0.0029262269  0.0134619940 #> [131]  0.0060545735  0.0513732397  0.0327279879  0.1054238844  0.0668562286 #> [136]  0.0769686804 -0.0227940919 -0.0422079731  0.0703340490 -0.0155774872 #> [141]  0.0372660027 -0.1356710009  0.0350093190  0.0236513594  0.0484942080 #> [146] -0.0400414708  0.0188626798  0.0274173715 -0.0486294492  0.0038126260 #> [151]  0.0133522101  0.0559058596  0.0022117502 -0.0837040985 -0.0849908637 #> [156] -0.0533659570 -0.0608346184  0.0319297479  0.0282798544  0.0078445029 #> [161] -0.0496067834 -0.0011413436  0.1585030627 -0.0257539005 -0.0556869160 #> [166] -0.0376501897  0.0119025195 -0.0722575073 -0.0134277815  0.0713891951 #> [171] -0.1004959954  0.0377733481  0.1079291741  0.0487607486  0.0176890786 #> [176] -0.0642323537  0.0939240962  0.0323515907  0.0062917470  0.0628913182 #> [181] -0.1168540632  0.0042852646 -0.1256198725 -0.0767676420  0.0159300785 #> [186]  0.0294246554  0.0809427655  0.0357663118 -0.0218166578  0.0638628914 #> [191]  0.0283086395 -0.0002039863 -0.0330815284  0.0067946163  0.0198031884 #> [196]  0.0572634491  0.0025753907 -0.0179017809  0.0275942861 -0.0879453025 #> [201] -0.0565598458  0.0206358787  0.0228473515 -0.0461282294 -0.0145729556 #> [206] -0.0143933569  0.0095678299  0.0631869552  0.0659021305  0.0042216349 #> [211]  0.0293649478  0.1033812547 -0.0516775577 -0.1253424596  0.0270994610 #> [216]  0.0385621533 -0.0036571112 -0.0278117809  0.0295294855  0.0024058853 #> [221] -0.0144733111  0.0180931805  0.0579378103 -0.0116669835  0.0087559466 #> [226] -0.0283575865 -0.0278965844 -0.0272467575  0.0342410954  0.0878225024 #> [231] -0.0741101358 -0.0684089597 -0.1740935244 -0.0044601583  0.0376029740 #> [236]  0.0221138776  0.0207988624 -0.0113099903 -0.0275133395 -0.0000199454 #> [241]  0.0563714279 -0.1243700668  0.0584321684  0.0294712221  0.0939427080 #> [246] -0.0011363452 -0.0734356129  0.0425710531  0.0715560074 -0.0782001568 #> [251]  0.0274794287  0.0326764105  0.0565690520 -0.0124150119  0.0545865281 #> [256]  0.0131866584 -0.1333997208 -0.0673208573 -0.0237475804 -0.0439831482 #> [261]  0.0048958304  0.0900008028 -0.0692655794  0.0177480897 -0.0185615675 #> [266] -0.0438003657  0.0037071559  0.0152207093  0.0955843067  0.1068395259 #> [271]  0.0044241142  0.1174178922 -0.0404572738  0.0672502162 -0.0424039690 #> [276] -0.1056443456 -0.0766069551  0.0035567801 -0.0748190562  0.0503630702 #> [281]  0.0909204843 -0.0668049051  0.0951348101 -0.0171847044  0.0625944510 #> [286] -0.0035357800 -0.0205028927 -0.0161002229 -0.0182009416  0.0601725192 #> [291] -0.0265177060 -0.0849906153 -0.0761131339 -0.1206830347  0.0004351351 #> [296]  0.0062921084 -0.0137075363  0.0623639418  0.0806824072  0.0592501645 # final word vector computed according to the formula"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/pair_similarity.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute cosine similarity/distance for a pair of words. — pair_similarity","title":"Compute cosine similarity/distance for a pair of words. — pair_similarity","text":"Compute cosine similarity/distance pair words.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/pair_similarity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute cosine similarity/distance for a pair of words. — pair_similarity","text":"","code":"pair_similarity(data, word1, word2, distance = FALSE)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/pair_similarity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute cosine similarity/distance for a pair of words. — pair_similarity","text":"data data.table variables word vec loaded data_wordvec_load. word1, word2 Word string (single word). distance Compute cosine distance instead? Default FALSE (cosine similarity).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/pair_similarity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute cosine similarity/distance for a pair of words. — pair_similarity","text":"value cosine similarity/distance.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/pair_similarity.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Compute cosine similarity/distance for a pair of words. — pair_similarity","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/pair_similarity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute cosine similarity/distance for a pair of words. — pair_similarity","text":"","code":"pair_similarity(demodata, \"China\", \"Chinese\") #> [1] 0.7678081"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvecs.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize word vectors. — plot_wordvecs","title":"Visualize word vectors. — plot_wordvecs","text":"Visualize word vectors.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvecs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize word vectors. — plot_wordvecs","text":"","code":"plot_wordvecs(dt, dims = NULL, step = 0.05, border = \"white\")"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvecs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize word vectors. — plot_wordvecs","text":"dt data.table returned get_wordvecs. dims Dimensions plotted (e.g., 1:100). Default NULL (plot dimensions). step Step value breaks. Default 0.05. border Color tile border. Default \"white\". remove border color, set border=NA.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvecs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize word vectors. — plot_wordvecs","text":"ggplot object.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvecs.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Visualize word vectors. — plot_wordvecs","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvecs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize word vectors. — plot_wordvecs","text":"","code":"d = data_wordvec_normalize(demodata) #> √ All word vectors have now been normalized.  dt = get_wordvecs(d, cc(\"king, queen, man, woman\")) dt[, QUEEN := king - man + woman] #>              man       woman         king        queen        QUEEN #>   1:  0.14116230  0.09156568  0.043406531  0.001733313 -0.006190088 #>   2:  0.05663379 -0.02905080  0.010262695 -0.047404412 -0.075421897 #>   3:  0.01500378 -0.03879578  0.002965276 -0.022895979 -0.050834289 #>   4: -0.03592460 -0.04045076  0.048116999  0.040793453  0.043590844 #>   5:  0.03888312  0.04449576 -0.008832774  0.043534590 -0.003220136 #>  ---                                                                #> 296: -0.02768308  0.04063452 -0.061239879 -0.013624785  0.007077723 #> 297: -0.03613580  0.04467952 -0.096234342  0.055466349 -0.015419019 #> 298: -0.13101869 -0.03125744 -0.029610726 -0.015478958  0.070150521 #> 299: -0.03465654  0.02463828  0.031461353  0.053853896  0.090756176 #> 300:  0.00908675 -0.01107800  0.086812717  0.050951612  0.066647966 dt[, QUEEN := QUEEN / sqrt(sum(QUEEN^2))]  # normalize #>              man       woman         king        queen        QUEEN #>   1:  0.14116230  0.09156568  0.043406531  0.001733313 -0.005502999 #>   2:  0.05663379 -0.02905080  0.010262695 -0.047404412 -0.067050205 #>   3:  0.01500378 -0.03879578  0.002965276 -0.022895979 -0.045191776 #>   4: -0.03592460 -0.04045076  0.048116999  0.040793453  0.038752340 #>   5:  0.03888312  0.04449576 -0.008832774  0.043534590 -0.002862707 #>  ---                                                                #> 296: -0.02768308  0.04063452 -0.061239879 -0.013624785  0.006292108 #> 297: -0.03613580  0.04467952 -0.096234342  0.055466349 -0.013707536 #> 298: -0.13101869 -0.03125744 -0.029610726 -0.015478958  0.062363942 #> 299: -0.03465654  0.02463828  0.031461353  0.053853896  0.080682407 #> 300:  0.00908675 -0.01107800  0.086812717  0.050951612  0.059250164 names(dt)[5] = \"king - man + woman\" plot_wordvecs(dt[, c(1,3,4,5,2)], dims=1:50)   dt = get_wordvecs(d, cc(\"boy, girl, he, she\")) dt[, GIRL := boy - he + she] #>                he         she         girl          boy         GIRL #>   1:  0.109257091  0.04049595  0.047750749  0.083967962  0.015206823 #>   2:  0.072653299 -0.01454260  0.001287155  0.058881966 -0.028313934 #>   3: -0.010884081 -0.05615750  0.022193947  0.033273650 -0.011999766 #>   4: -0.016638191 -0.01778671 -0.026229304 -0.045990576 -0.047139098 #>   5:  0.017608756  0.05861853  0.008532822  0.005705206  0.046714981 #>  ---                                                                 #> 296:  0.061006509  0.03311285  0.023538951 -0.019162800 -0.047056460 #> 297:  0.002339808  0.09754833  0.023034833 -0.026479563  0.068728958 #> 298: -0.098165154  0.01073948 -0.029928238 -0.094768644  0.014135985 #> 299:  0.069880173  0.11365708 -0.003089446 -0.036757568  0.007019343 #> 300: -0.090954911 -0.05392007  0.012946613  0.048429498  0.085464341 dt[, GIRL := GIRL / sqrt(sum(GIRL^2))]  # normalize #>                he         she         girl          boy         GIRL #>   1:  0.109257091  0.04049595  0.047750749  0.083967962  0.011744750 #>   2:  0.072653299 -0.01454260  0.001287155  0.058881966 -0.021867820 #>   3: -0.010884081 -0.05615750  0.022193947  0.033273650 -0.009267829 #>   4: -0.016638191 -0.01778671 -0.026229304 -0.045990576 -0.036407138 #>   5:  0.017608756  0.05861853  0.008532822  0.005705206  0.036079578 #>  ---                                                                 #> 296:  0.061006509  0.03311285  0.023538951 -0.019162800 -0.036343314 #> 297:  0.002339808  0.09754833  0.023034833 -0.026479563  0.053081725 #> 298: -0.098165154  0.01073948 -0.029928238 -0.094768644  0.010917705 #> 299:  0.069880173  0.11365708 -0.003089446 -0.036757568  0.005421279 #> 300: -0.090954911 -0.05392007  0.012946613  0.048429498  0.066007034 names(dt)[5] = \"boy - he + she\" plot_wordvecs(dt[, c(1,3,4,5,2)], dims=1:50)   if (FALSE) {  dt = get_wordvecs(d, cc(\"   male, man, boy, he, his,   female, woman, girl, she, her\"))  p = plot_wordvecs(dt, dims=1:100)  # if you want to change something: p + theme(legend.key.height=unit(0.1, \"npc\"))  # or to save the plot: ggsave(p, filename=\"wordvecs.png\",        width=8, height=5, dpi=500) }"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. bruceR cc","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tab_WEAT.html","id":null,"dir":"Reference","previous_headings":"","what":"Tabulate cosine similarity for WEAT / WEFAT analysis. — tab_WEAT","title":"Tabulate cosine similarity for WEAT / WEFAT analysis. — tab_WEAT","text":"Tabulate cosine similarity WEAT / WEFAT analysis.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tab_WEAT.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tabulate cosine similarity for WEAT / WEFAT analysis. — tab_WEAT","text":"","code":"tab_WEAT(data, T1, T2, A1, A2, use.pattern = FALSE, labels)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tab_WEAT.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tabulate cosine similarity for WEAT / WEFAT analysis. — tab_WEAT","text":"data data.table variables word vec loaded data_wordvec_load. T1, T2 Target words (vector words pattern regular expression). T1 specified, tabulate data single-target WEAT (.e., WEFAT; Caliskan et al., 2017). A1, A2 Attribute words (vector words pattern regular expression). specified. use.pattern Default FALSE (using vector words). use regular expression T1, T2, A1, A2, please specify argument TRUE. labels Labels target attribute concepts (named list), (default) list(T1=\"Target1\", T2=\"Target2\", A1=\"Attrib1\", A2=\"Attrib2\").","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tab_WEAT.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tabulate cosine similarity for WEAT / WEFAT analysis. — tab_WEAT","text":"list objects: words.valid valid (actually matched) words data.raw data.table cosine similarities word pairs data.mean data.table mean cosine similarities     across attribute words data.diff data.table differential mean cosine similarities     two attribute concepts code.diff description difference two attribute concepts eff.type effect type: WEAT WEFAT eff.size effect size WEAT (single value) WEFAT (data table)","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tab_WEAT.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Tabulate cosine similarity for WEAT / WEFAT analysis. — tab_WEAT","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tab_WEAT.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Tabulate cosine similarity for WEAT / WEFAT analysis. — tab_WEAT","text":"Caliskan, ., Bryson, J. J., & Narayanan, . (2017). Semantics derived automatically language corpora contain human-like biases. Science, 356(6334), 183-186.","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tab_WEAT.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tabulate cosine similarity for WEAT / WEFAT analysis. — tab_WEAT","text":"","code":"## Remember: cc() is more convenient than c()!  weat = tab_WEAT(   demodata,   T1=cc(\"king, King\"),   T2=cc(\"queen, Queen\"),   A1=cc(\"male, man, boy, brother, he, him, his, son\"),   A2=cc(\"female, woman, girl, sister, she, her, hers, daughter\"),   labels=list(T1=\"King\", T2=\"Queen\", A1=\"Male\", A2=\"Female\")) weat #> $words.valid #> $words.valid$T1 #> [1] \"king\" \"King\" #>  #> $words.valid$T2 #> [1] \"queen\" \"Queen\" #>  #> $words.valid$A1 #> [1] \"male\"    \"man\"     \"boy\"     \"brother\" \"he\"      \"him\"     \"his\"     #> [8] \"son\"     #>  #> $words.valid$A2 #> [1] \"female\"   \"woman\"    \"girl\"     \"sister\"   \"she\"      \"her\"      \"hers\"     #> [8] \"daughter\" #>  #>  #> $data.raw #>     Target Attrib T_word   A_word    cos_sim #>  1:   King   Male   king     male 0.13960424 #>  2:   King   Male   King     male 0.09070985 #>  3:   King   Male   king      man 0.22942676 #>  4:   King   Male   King      man 0.15777646 #>  5:   King   Male   king      boy 0.25351544 #>  6:   King   Male   King      boy 0.16933196 #>  7:   King   Male   king  brother 0.23432925 #>  8:   King   Male   King  brother 0.22089012 #>  9:   King   Male   king       he 0.22512579 #> 10:   King   Male   King       he 0.14808663 #> 11:   King   Male   king      him 0.24867882 #> 12:   King   Male   King      him 0.21191115 #> 13:   King   Male   king      his 0.17143112 #> 14:   King   Male   King      his 0.13784300 #> 15:   King   Male   king      son 0.21417183 #> 16:   King   Male   King      son 0.18987593 #> 17:   King Female   king   female 0.08971824 #> 18:   King Female   King   female 0.07212608 #> 19:   King Female   king    woman 0.12847968 #> 20:   King Female   King    woman 0.06369529 #> 21:   King Female   king     girl 0.17236953 #> 22:   King Female   King     girl 0.08804782 #> 23:   King Female   king   sister 0.11340389 #> 24:   King Female   King   sister 0.08357969 #> 25:   King Female   king      she 0.06542464 #> 26:   King Female   King      she 0.04830619 #> 27:   King Female   king      her 0.06532056 #> 28:   King Female   King      her 0.07439990 #> 29:   King Female   king     hers 0.10306190 #> 30:   King Female   King     hers 0.06238802 #> 31:   King Female   king daughter 0.14881282 #> 32:   King Female   King daughter 0.11301357 #> 33:  Queen   Male  queen     male 0.18550913 #> 34:  Queen   Male  Queen     male 0.11256271 #> 35:  Queen   Male  queen      man 0.16658216 #> 36:  Queen   Male  Queen      man 0.09365463 #> 37:  Queen   Male  queen      boy 0.20767608 #> 38:  Queen   Male  Queen      boy 0.10297925 #> 39:  Queen   Male  queen  brother 0.11763060 #> 40:  Queen   Male  Queen  brother 0.04219659 #> 41:  Queen   Male  queen       he 0.09282878 #> 42:  Queen   Male  Queen       he 0.05188371 #> 43:  Queen   Male  queen      him 0.13145000 #> 44:  Queen   Male  Queen      him 0.07582127 #> 45:  Queen   Male  queen      his 0.06322216 #> 46:  Queen   Male  Queen      his 0.02713799 #> 47:  Queen   Male  queen      son 0.15659811 #> 48:  Queen   Male  Queen      son 0.05476228 #> 49:  Queen Female  queen   female 0.26985589 #> 50:  Queen Female  Queen   female 0.18863183 #> 51:  Queen Female  queen    woman 0.31618132 #> 52:  Queen Female  Queen    woman 0.20171619 #> 53:  Queen Female  queen     girl 0.35005602 #> 54:  Queen Female  Queen     girl 0.18701451 #> 55:  Queen Female  queen   sister 0.25180591 #> 56:  Queen Female  Queen   sister 0.16349528 #> 57:  Queen Female  queen      she 0.36242807 #> 58:  Queen Female  Queen      she 0.23824302 #> 59:  Queen Female  queen      her 0.35522259 #> 60:  Queen Female  Queen      her 0.23308440 #> 61:  Queen Female  queen     hers 0.31618069 #> 62:  Queen Female  Queen     hers 0.21118412 #> 63:  Queen Female  queen daughter 0.30513862 #> 64:  Queen Female  Queen daughter 0.16699306 #>     Target Attrib T_word   A_word    cos_sim #>  #> $data.mean #>    Target T_word Attrib cos_sim_mean    std_dev  std_mean #> 1:   King   king   Male   0.21453541 0.06536807 3.2819604 #> 2:   King   king Female   0.11082391 0.06536807 1.6953830 #> 3:   King   King   Male   0.16580314 0.05640498 2.9395122 #> 4:   King   King Female   0.07569457 0.05640498 1.3419837 #> 5:  Queen  queen   Male   0.14018713 0.10032951 1.3972671 #> 6:  Queen  queen Female   0.31585864 0.10032951 3.1482127 #> 7:  Queen  Queen   Male   0.07012480 0.07224076 0.9707096 #> 8:  Queen  Queen Female   0.19879530 0.07224076 2.7518439 #>  #> $data.diff #>    Target T_word cos_sim_diff   std_dev #> 1:   King   king   0.10371150 0.1451877 #> 2:   King   King   0.09010857 0.1451877 #> 3:  Queen  queen  -0.17567151 0.1451877 #> 4:  Queen  Queen  -0.12867050 0.1451877 #>  #> $code.diff #> [1] \"King vs. Queen :: Male vs. Female\" #>  #> $eff.type #> [1] \"WEAT (Word-Embedding Association Test)\" #>  #> $eff.size #> [1] 1.715579 #>   weat = tab_WEAT(   demodata,   T1=\"^[kK]ing$\",   T2=\"^[qQ]ueen$\",   A1=\"^male$|^man$|^boy$|^brother$|^he$|^him$|^his$|^son$\",   A2=\"^female$|^woman$|^girl$|^sister$|^she$|^her$|^hers$|^daughter$\",   use.pattern=TRUE,   labels=list(T1=\"King\", T2=\"Queen\", A1=\"Male\", A2=\"Female\")) #> T1 (King): #> 0 words are matched... #> T2 (Queen): #> 0 words are matched... #> A1 (Male): #> 0 words are matched... #> A2 (Female): #> 0 words are matched... weat #> $words.valid #> $words.valid$T1 #> [1] \"King\" \"king\" #>  #> $words.valid$T2 #> [1] \"Queen\" \"queen\" #>  #> $words.valid$A1 #> [1] \"he\"      \"his\"     \"him\"     \"man\"     \"son\"     \"boy\"     \"brother\" #> [8] \"male\"    #>  #> $words.valid$A2 #> [1] \"her\"      \"she\"      \"woman\"    \"daughter\" \"girl\"     \"female\"   \"sister\"   #> [8] \"hers\"     #>  #>  #> $data.raw #>     Target Attrib T_word   A_word    cos_sim #>  1:   King   Male   King       he 0.14808663 #>  2:   King   Male   king       he 0.22512579 #>  3:   King   Male   King      his 0.13784300 #>  4:   King   Male   king      his 0.17143112 #>  5:   King   Male   King      him 0.21191115 #>  6:   King   Male   king      him 0.24867882 #>  7:   King   Male   King      man 0.15777646 #>  8:   King   Male   king      man 0.22942676 #>  9:   King   Male   King      son 0.18987593 #> 10:   King   Male   king      son 0.21417183 #> 11:   King   Male   King      boy 0.16933196 #> 12:   King   Male   king      boy 0.25351544 #> 13:   King   Male   King  brother 0.22089012 #> 14:   King   Male   king  brother 0.23432925 #> 15:   King   Male   King     male 0.09070985 #> 16:   King   Male   king     male 0.13960424 #> 17:   King Female   King      her 0.07439990 #> 18:   King Female   king      her 0.06532056 #> 19:   King Female   King      she 0.04830619 #> 20:   King Female   king      she 0.06542464 #> 21:   King Female   King    woman 0.06369529 #> 22:   King Female   king    woman 0.12847968 #> 23:   King Female   King daughter 0.11301357 #> 24:   King Female   king daughter 0.14881282 #> 25:   King Female   King     girl 0.08804782 #> 26:   King Female   king     girl 0.17236953 #> 27:   King Female   King   female 0.07212608 #> 28:   King Female   king   female 0.08971824 #> 29:   King Female   King   sister 0.08357969 #> 30:   King Female   king   sister 0.11340389 #> 31:   King Female   King     hers 0.06238802 #> 32:   King Female   king     hers 0.10306190 #> 33:  Queen   Male  Queen       he 0.05188371 #> 34:  Queen   Male  queen       he 0.09282878 #> 35:  Queen   Male  Queen      his 0.02713799 #> 36:  Queen   Male  queen      his 0.06322216 #> 37:  Queen   Male  Queen      him 0.07582127 #> 38:  Queen   Male  queen      him 0.13145000 #> 39:  Queen   Male  Queen      man 0.09365463 #> 40:  Queen   Male  queen      man 0.16658216 #> 41:  Queen   Male  Queen      son 0.05476228 #> 42:  Queen   Male  queen      son 0.15659811 #> 43:  Queen   Male  Queen      boy 0.10297925 #> 44:  Queen   Male  queen      boy 0.20767608 #> 45:  Queen   Male  Queen  brother 0.04219659 #> 46:  Queen   Male  queen  brother 0.11763060 #> 47:  Queen   Male  Queen     male 0.11256271 #> 48:  Queen   Male  queen     male 0.18550913 #> 49:  Queen Female  Queen      her 0.23308440 #> 50:  Queen Female  queen      her 0.35522259 #> 51:  Queen Female  Queen      she 0.23824302 #> 52:  Queen Female  queen      she 0.36242807 #> 53:  Queen Female  Queen    woman 0.20171619 #> 54:  Queen Female  queen    woman 0.31618132 #> 55:  Queen Female  Queen daughter 0.16699306 #> 56:  Queen Female  queen daughter 0.30513862 #> 57:  Queen Female  Queen     girl 0.18701451 #> 58:  Queen Female  queen     girl 0.35005602 #> 59:  Queen Female  Queen   female 0.18863183 #> 60:  Queen Female  queen   female 0.26985589 #> 61:  Queen Female  Queen   sister 0.16349528 #> 62:  Queen Female  queen   sister 0.25180591 #> 63:  Queen Female  Queen     hers 0.21118412 #> 64:  Queen Female  queen     hers 0.31618069 #>     Target Attrib T_word   A_word    cos_sim #>  #> $data.mean #>    Target T_word Attrib cos_sim_mean    std_dev  std_mean #> 1:   King   King   Male   0.16580314 0.05640498 2.9395122 #> 2:   King   King Female   0.07569457 0.05640498 1.3419837 #> 3:   King   king   Male   0.21453541 0.06536807 3.2819604 #> 4:   King   king Female   0.11082391 0.06536807 1.6953830 #> 5:  Queen  Queen   Male   0.07012480 0.07224076 0.9707096 #> 6:  Queen  Queen Female   0.19879530 0.07224076 2.7518439 #> 7:  Queen  queen   Male   0.14018713 0.10032951 1.3972671 #> 8:  Queen  queen Female   0.31585864 0.10032951 3.1482127 #>  #> $data.diff #>    Target T_word cos_sim_diff   std_dev #> 1:   King   King   0.09010857 0.1451877 #> 2:   King   king   0.10371150 0.1451877 #> 3:  Queen  Queen  -0.12867050 0.1451877 #> 4:  Queen  queen  -0.17567151 0.1451877 #>  #> $code.diff #> [1] \"King vs. Queen :: Male vs. Female\" #>  #> $eff.type #> [1] \"WEAT (Word-Embedding Association Test)\" #>  #> $eff.size #> [1] 1.715579 #>   wefat = tab_WEAT(   demodata,   T1=cc(\"     architect, boss, leader, engineer, CEO, officer, manager,     lawyer, scientist, doctor, psychologist, investigator,     consultant, programmer, teacher, clerk, counselor,     salesperson, therapist, psychotherapist, nurse\"),   A1=cc(\"male, man, boy, brother, he, him, his, son\"),   A2=cc(\"female, woman, girl, sister, she, her, hers, daughter\"),   labels=list(T1=\"Occupation\", A1=\"Male\", A2=\"Female\")) wefat #> $words.valid #> $words.valid$T1 #>  [1] \"architect\"       \"boss\"            \"leader\"          \"engineer\"        #>  [5] \"CEO\"             \"officer\"         \"manager\"         \"lawyer\"          #>  [9] \"scientist\"       \"doctor\"          \"psychologist\"    \"investigator\"    #> [13] \"consultant\"      \"programmer\"      \"teacher\"         \"clerk\"           #> [17] \"counselor\"       \"salesperson\"     \"therapist\"       \"psychotherapist\" #> [21] \"nurse\"           #>  #> $words.valid$T2 #> NULL #>  #> $words.valid$A1 #> [1] \"male\"    \"man\"     \"boy\"     \"brother\" \"he\"      \"him\"     \"his\"     #> [8] \"son\"     #>  #> $words.valid$A2 #> [1] \"female\"   \"woman\"    \"girl\"     \"sister\"   \"she\"      \"her\"      \"hers\"     #> [8] \"daughter\" #>  #>  #> $data.raw #>          Target Attrib          T_word   A_word     cos_sim #>   1: Occupation   Male       architect     male -0.07107298 #>   2: Occupation   Male            boss     male  0.02509828 #>   3: Occupation   Male          leader     male -0.04058901 #>   4: Occupation   Male        engineer     male -0.04795309 #>   5: Occupation   Male             CEO     male -0.11093326 #>  ---                                                        #> 332: Occupation Female       counselor daughter  0.24606261 #> 333: Occupation Female     salesperson daughter  0.13918887 #> 334: Occupation Female       therapist daughter  0.29122729 #> 335: Occupation Female psychotherapist daughter  0.26725654 #> 336: Occupation Female           nurse daughter  0.36904112 #>  #> $data.mean #>         Target          T_word Attrib  cos_sim_mean    std_dev     std_mean #>  1: Occupation       architect   Male  8.692525e-02 0.08302492  1.046977869 #>  2: Occupation       architect Female -8.863421e-05 0.08302492 -0.001067562 #>  3: Occupation            boss   Male  2.060022e-01 0.07892258  2.610181026 #>  4: Occupation            boss Female  1.249453e-01 0.07892258  1.583138058 #>  5: Occupation          leader   Male  1.237260e-01 0.08049820  1.537003534 #>  6: Occupation          leader Female  4.439556e-02 0.08049820  0.551509980 #>  7: Occupation        engineer   Male  1.030868e-01 0.09144904  1.127259846 #>  8: Occupation        engineer Female  3.219294e-02 0.09144904  0.352031432 #>  9: Occupation             CEO   Male -2.858738e-02 0.07028655 -0.406726174 #> 10: Occupation             CEO Female -7.334349e-02 0.07028655 -1.043492558 #> 11: Occupation         officer   Male  1.308768e-01 0.07290262  1.795228269 #> 12: Occupation         officer Female  9.821684e-02 0.07290262  1.347233392 #> 13: Occupation         manager   Male  4.827265e-02 0.06566996  0.735079676 #> 14: Occupation         manager Female  2.579163e-02 0.06566996  0.392746207 #> 15: Occupation          lawyer   Male  1.848924e-01 0.08545002  2.163749414 #> 16: Occupation          lawyer Female  1.645807e-01 0.08545002  1.926046604 #> 17: Occupation       scientist   Male  9.010405e-02 0.04787808  1.881947739 #> 18: Occupation       scientist Female  8.196547e-02 0.04787808  1.711962285 #> 19: Occupation          doctor   Male  2.683077e-01 0.07933172  3.382098104 #> 20: Occupation          doctor Female  2.549294e-01 0.07933172  3.213461245 #> 21: Occupation    psychologist   Male  1.508973e-01 0.03388385  4.453369388 #> 22: Occupation    psychologist Female  1.498728e-01 0.03388385  4.423133632 #> 23: Occupation    investigator   Male  1.343693e-01 0.05140167  2.614104205 #> 24: Occupation    investigator Female  1.337600e-01 0.05140167  2.602249123 #> 25: Occupation      consultant   Male  4.682858e-02 0.05764734  0.812328417 #> 26: Occupation      consultant Female  5.000916e-02 0.05764734  0.867501577 #> 27: Occupation      programmer   Male  4.665029e-02 0.04187294  1.114091656 #> 28: Occupation      programmer Female  5.267160e-02 0.04187294  1.257891213 #> 29: Occupation         teacher   Male  2.060090e-01 0.08399303  2.452691471 #> 30: Occupation         teacher Female  2.731964e-01 0.08399303  3.252607343 #> 31: Occupation           clerk   Male  1.870184e-01 0.07531653  2.483099547 #> 32: Occupation           clerk Female  2.649577e-01 0.07531653  3.517923009 #> 33: Occupation       counselor   Male  1.408918e-01 0.05387167  2.615322725 #> 34: Occupation       counselor Female  2.053143e-01 0.05387167  3.811174881 #> 35: Occupation     salesperson   Male  1.029099e-01 0.06846796  1.503037241 #> 36: Occupation     salesperson Female  1.855234e-01 0.06846796  2.709638701 #> 37: Occupation       therapist   Male  1.730993e-01 0.05566177  3.109841850 #> 38: Occupation       therapist Female  2.423886e-01 0.05566177  4.354669527 #> 39: Occupation psychotherapist   Male  1.119207e-01 0.06973167  1.605019706 #> 40: Occupation psychotherapist Female  2.041495e-01 0.06973167  2.927643750 #> 41: Occupation           nurse   Male  1.796695e-01 0.10881512  1.651144349 #> 42: Occupation           nurse Female  3.418709e-01 0.10881512  3.141758950 #>         Target          T_word Attrib  cos_sim_mean    std_dev     std_mean #>  #> $data.diff #>         Target          T_word cos_sim_diff    std_diff #>  1: Occupation       architect  0.087013885  1.04804543 #>  2: Occupation            boss  0.081056880  1.02704297 #>  3: Occupation          leader  0.079330460  0.98549355 #>  4: Occupation        engineer  0.070893895  0.77522841 #>  5: Occupation             CEO  0.044756111  0.63676638 #>  6: Occupation         officer  0.032659998  0.44799488 #>  7: Occupation         manager  0.022481025  0.34233347 #>  8: Occupation          lawyer  0.020311709  0.23770281 #>  9: Occupation       scientist  0.008138578  0.16998545 #> 10: Occupation          doctor  0.013378252  0.16863686 #> 11: Occupation    psychologist  0.001024504  0.03023576 #> 12: Occupation    investigator  0.000609371  0.01185508 #> 13: Occupation      consultant -0.003180586 -0.05517316 #> 14: Occupation      programmer -0.006021310 -0.14379956 #> 15: Occupation         teacher -0.067187362 -0.79991587 #> 16: Occupation           clerk -0.077939310 -1.03482346 #> 17: Occupation       counselor -0.064422548 -1.19585216 #> 18: Occupation     salesperson -0.082613539 -1.20660146 #> 19: Occupation       therapist -0.069289306 -1.24482768 #> 20: Occupation psychotherapist -0.092228780 -1.32262404 #> 21: Occupation           nurse -0.162201404 -1.49061460 #>         Target          T_word cos_sim_diff    std_diff #>  #> $code.diff #> [1] \"Occupation :: Male vs. Female\" #>  #> $eff.type #> [1] \"WEFAT (Word-Embedding Factual Association Test)\" #>  #> $eff.size #>              T_word    eff_size #>  1:       architect  1.04804543 #>  2:            boss  1.02704297 #>  3:          leader  0.98549355 #>  4:        engineer  0.77522841 #>  5:             CEO  0.63676638 #>  6:         officer  0.44799488 #>  7:         manager  0.34233347 #>  8:          lawyer  0.23770281 #>  9:       scientist  0.16998545 #> 10:          doctor  0.16863686 #> 11:    psychologist  0.03023576 #> 12:    investigator  0.01185508 #> 13:      consultant -0.05517316 #> 14:      programmer -0.14379956 #> 15:         teacher -0.79991587 #> 16:           clerk -1.03482346 #> 17:       counselor -1.19585216 #> 18:     salesperson -1.20660146 #> 19:       therapist -1.24482768 #> 20: psychotherapist -1.32262404 #> 21:           nurse -1.49061460 #>              T_word    eff_size #>"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tab_similarity.html","id":null,"dir":"Reference","previous_headings":"","what":"Tabulate cosine similarity/distance of all word pairs. — tab_similarity","title":"Tabulate cosine similarity/distance of all word pairs. — tab_similarity","text":"Tabulate cosine similarity/distance word pairs.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tab_similarity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tabulate cosine similarity/distance of all word pairs. — tab_similarity","text":"","code":"tab_similarity(   data,   words = NULL,   pattern = NULL,   unique = FALSE,   distance = FALSE )"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tab_similarity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tabulate cosine similarity/distance of all word pairs. — tab_similarity","text":"data data.table variables word vec loaded data_wordvec_load. words [Option 1] Word string (NULL; single word; vector words). pattern [Option 2] Pattern regular expression (see str_subset). unique Word pairs: unique pairs (TRUE) full pairs duplicates (FALSE; default). distance Compute cosine distance instead? Default FALSE (cosine similarity).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tab_similarity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tabulate cosine similarity/distance of all word pairs. — tab_similarity","text":"data.table words unique full pairs, cosine similarity (cos_sim) cosine distance (cos_dist).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tab_similarity.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Tabulate cosine similarity/distance of all word pairs. — tab_similarity","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tab_similarity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tabulate cosine similarity/distance of all word pairs. — tab_similarity","text":"","code":"tab_similarity(demodata, cc(\"king, queen, man, woman\")) #>     word1 word2    wordpair   cos_sim #>  1:  king  king   king-king 1.0000000 #>  2:  king queen  king-queen 0.6510958 #>  3:  king   man    king-man 0.2294268 #>  4:  king woman  king-woman 0.1284797 #>  5: queen  king  queen-king 0.6510958 #>  6: queen queen queen-queen 1.0000000 #>  7: queen   man   queen-man 0.1665822 #>  8: queen woman queen-woman 0.3161813 #>  9:   man  king    man-king 0.2294268 #> 10:   man queen   man-queen 0.1665822 #> 11:   man   man     man-man 1.0000000 #> 12:   man woman   man-woman 0.7664012 #> 13: woman  king  woman-king 0.1284797 #> 14: woman queen woman-queen 0.3161813 #> 15: woman   man   woman-man 0.7664012 #> 16: woman woman woman-woman 1.0000000  tab_similarity(demodata, cc(\"king, queen, man, woman\"),                unique=TRUE) #>    word1 word2    wordpair   cos_sim #> 1:  king queen  king-queen 0.6510958 #> 2:  king   man    king-man 0.2294268 #> 3:  king woman  king-woman 0.1284797 #> 4: queen   man   queen-man 0.1665822 #> 5: queen woman queen-woman 0.3161813 #> 6:   man woman   man-woman 0.7664012  tab_similarity(demodata, cc(\"Beijing, China, Tokyo, Japan\")) #>       word1   word2        wordpair   cos_sim #>  1: Beijing Beijing Beijing-Beijing 1.0000000 #>  2: Beijing   China   Beijing-China 0.7648461 #>  3: Beijing   Tokyo   Beijing-Tokyo 0.5229628 #>  4: Beijing   Japan   Beijing-Japan 0.3995245 #>  5:   China Beijing   China-Beijing 0.7648461 #>  6:   China   China     China-China 1.0000000 #>  7:   China   Tokyo     China-Tokyo 0.3814305 #>  8:   China   Japan     China-Japan 0.5967756 #>  9:   Tokyo Beijing   Tokyo-Beijing 0.5229628 #> 10:   Tokyo   China     Tokyo-China 0.3814305 #> 11:   Tokyo   Tokyo     Tokyo-Tokyo 1.0000000 #> 12:   Tokyo   Japan     Tokyo-Japan 0.7002254 #> 13:   Japan Beijing   Japan-Beijing 0.3995245 #> 14:   Japan   China     Japan-China 0.5967756 #> 15:   Japan   Tokyo     Japan-Tokyo 0.7002254 #> 16:   Japan   Japan     Japan-Japan 1.0000000  tab_similarity(demodata, cc(\"Beijing, China, Tokyo, Japan\"),                unique=TRUE) #>      word1 word2      wordpair   cos_sim #> 1: Beijing China Beijing-China 0.7648461 #> 2: Beijing Tokyo Beijing-Tokyo 0.5229628 #> 3: Beijing Japan Beijing-Japan 0.3995245 #> 4:   China Tokyo   China-Tokyo 0.3814305 #> 5:   China Japan   China-Japan 0.5967756 #> 6:   Tokyo Japan   Tokyo-Japan 0.7002254"},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/news/index.html","id":"new-features-0-0-6","dir":"Changelog","previous_headings":"","what":"New Features","title":"PsychWordVec 0.0.6 (Jul 2022)","text":"Enhanced functions. New data_wordvec_subset() function. Added unique argument tab_similarity(). Added support using regular expression pattern tab_WEAT().","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/news/index.html","id":"psychwordvec-004-apr-2022","dir":"Changelog","previous_headings":"","what":"PsychWordVec 0.0.4 (Apr 2022)","title":"PsychWordVec 0.0.4 (Apr 2022)","text":"Initial public release GitHub.","code":""}]
