[{"path":"https://psychbruce.github.io/PsychWordVec/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Han-Wu-Shuang Bao. Author, maintainer.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Bao H (2022). PsychWordVec: Word Embedding Research Framework Psychological Science. R package version 0.2.1, https://psychbruce.github.io/PsychWordVec/.","code":"@Manual{,   title = {PsychWordVec: Word Embedding Research Framework for Psychological Science},   author = {Han-Wu-Shuang Bao},   year = {2022},   note = {R package version 0.2.1},   url = {https://psychbruce.github.io/PsychWordVec/}, }"},{"path":"https://psychbruce.github.io/PsychWordVec/index.html","id":"psychwordvec-","dir":"","previous_headings":"","what":"Word Embedding Research Framework for Psychological Science","title":"Word Embedding Research Framework for Psychological Science","text":"Word Embedding Research Framework Psychological Science. integrated toolbox word embedding research provides: collection pre-trained static word vectors .RData compressed format; series functions process, analyze, visualize word vectors; range tests examine conceptual associations, including Word Embedding Association Test (Caliskan et al., 2017) Relative Norm Distance (Garg et al., 2018), permutation test significance; set training methods locally train (static) word vectors text corpora, including Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017); group functions download pre-trained language models (e.g., GPT, BERT), extract contextualized (dynamic) word vectors (based R package text), perform language analysis tasks (e.g., fill blank masks).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/index.html","id":"author","dir":"","previous_headings":"","what":"Author","title":"Word Embedding Research Framework for Psychological Science","text":"Han-Wu-Shuang (Bruce) Bao 包寒吴霜 Email: baohws@foxmail.com Homepage: psychbruce.github.io","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Word Embedding Research Framework for Psychological Science","text":"Bao, H.-W.-S. (2022). PsychWordVec: Word embedding research framework psychological science. R package version 0.2.x. https://CRAN.R-project.org/package=PsychWordVec","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Word Embedding Research Framework for Psychological Science","text":"","code":"## Method 1: Install from CRAN install.packages(\"PsychWordVec\")  ## Method 2: Install from GitHub install.packages(\"devtools\") devtools::install_github(\"psychbruce/PsychWordVec\", force=TRUE)"},{"path":"https://psychbruce.github.io/PsychWordVec/index.html","id":"types-of-data-for-psychwordvec","dir":"","previous_headings":"","what":"Types of Data for PsychWordVec","title":"Word Embedding Research Framework for Psychological Science","text":"Note: Word embedding refers natural language processing technique embeds word semantics low-dimensional embedding matrix, word (actually token) quantified numeric vector representing (uninterpretable) semantic features. Users suggested import word vectors data embed class using function load_embed(), automatically normalize word vectors unit length 1 (see normalize() function) accelerate running functions PsychWordVec.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/index.html","id":"functions-in-psychwordvec","dir":"","previous_headings":"","what":"Functions in PsychWordVec","title":"Word Embedding Research Framework for Psychological Science","text":"as_embed(): wordvec (data.table) embed (matrix) as_wordvec(): embed (matrix) wordvec (data.table) load_embed(): load word embeddings data embed (matrix) load_wordvec(): load word embeddings data wordvec (data.table) data_transform(): transform plain text word vectors wordvec embed subset(): extract subset wordvec embed normalize(): normalize word vectors unit length 1 get_wordvec() sum_wordvec() plot_wordvec() plot_wordvec_tSNE(): 2D 3D visualization t-SNE orth_procrustes(): Orthogonal Procrustes matrix alignment cos_sim() cos_dist() pair_similarity() plot_similarity() tab_similarity() most_similar(): find Top-N similar words test_WEAT(): WEAT SC-WEAT permutation test significance test_RND(): RND permutation test significance dict_expand(): expand dictionary similar words dict_reliability(): reliability analysis PCA dictionary tokenize() train_wordvec() text_init(): set Python environment PLM text_model_download(): download PLMs HuggingFace local “.cache” folder text_model_remove(): remove PLMs local “.cache” folder text_to_vec(): extract contextualized token text embeddings text_unmask(): fill blank mask(s) query See documentation (help pages) usage details.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/as_embed.html","id":null,"dir":"Reference","previous_headings":"","what":"Word vectors data class: wordvec and embed. — as_embed","title":"Word vectors data class: wordvec and embed. — as_embed","text":"PsychWordVec uses two types word vectors data: wordvec (data.table, two variables word vec) embed (matrix, dimensions columns words row names). Note matrix operation makes embed much faster wordvec. Users suggested reshape data embed using functions.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/as_embed.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Word vectors data class: wordvec and embed. — as_embed","text":"","code":"as_embed(x, normalize = FALSE)  as_wordvec(x, normalize = FALSE)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/as_embed.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Word vectors data class: wordvec and embed. — as_embed","text":"x Object reshaped. See examples. normalize Normalize word vectors unit length? Defaults FALSE. See normalize.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/as_embed.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Word vectors data class: wordvec and embed. — as_embed","text":"wordvec (data.table) embed (matrix).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/as_embed.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Word vectors data class: wordvec and embed. — as_embed","text":"as_embed(): wordvec (data.table) embed (matrix). as_wordvec(): embed (matrix) wordvec (data.table).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/as_embed.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Word vectors data class: wordvec and embed. — as_embed","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/as_embed.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Word vectors data class: wordvec and embed. — as_embed","text":"","code":"dt = head(demodata, 10)  embed = as_embed(dt) embed #> # embed (matrix): 10 × 300 (NOT normalized) #>             dim1 ...     dim300 #>  1: in    0.0703 ... <300 dims> #>  2: for  -0.0118 ... <300 dims> #>  3: that -0.0157 ... <300 dims> #>  4: is    0.0070 ... <300 dims> #>  5: on    0.0267 ... <300 dims> #>  6: with -0.0249 ... <300 dims> #>  7: said -0.0091 ... <300 dims> #>  8: was   0.0260 ... <300 dims> #>  9: the   0.0801 ... <300 dims> #> 10: at   -0.0586 ... <300 dims>  wordvec = as_wordvec(embed) wordvec #> # wordvec (data.table): 10 × 2 (NOT normalized) #>     word                      vec #>  1:   in [ 0.0703, ...<300 dims>] #>  2:  for [-0.0118, ...<300 dims>] #>  3: that [-0.0157, ...<300 dims>] #>  4:   is [ 0.0070, ...<300 dims>] #>  5:   on [ 0.0267, ...<300 dims>] #>  6: with [-0.0249, ...<300 dims>] #>  7: said [-0.0091, ...<300 dims>] #>  8:  was [ 0.0260, ...<300 dims>] #>  9:  the [ 0.0801, ...<300 dims>] #> 10:   at [-0.0586, ...<300 dims>]  df = data.frame(token=LETTERS, D1=1:26/10000, D2=26:1/10000) as_embed(df) #> # embed (matrix): 26 × 2 (NOT normalized) #>          dim1 ...     dim2 #>  1: A  0.0001 ... <2 dims> #>  2: B  0.0002 ... <2 dims> #>  3: C  0.0003 ... <2 dims> #>  4: D  0.0004 ... <2 dims> #>  5: E  0.0005 ... <2 dims> #>  6: F  0.0006 ... <2 dims> #>  7: G  0.0007 ... <2 dims> #>  8: H  0.0008 ... <2 dims> #>  9: I  0.0009 ... <2 dims> #> 10: J  0.0010 ... <2 dims> #> 11: K  0.0011 ... <2 dims> #> 12: L  0.0012 ... <2 dims> #> 13: M  0.0013 ... <2 dims> #> 14: N  0.0014 ... <2 dims> #> 15: O  0.0015 ... <2 dims> #> 16: P  0.0016 ... <2 dims> #> 17: Q  0.0017 ... <2 dims> #> 18: R  0.0018 ... <2 dims> #> 19: S  0.0019 ... <2 dims> #> 20: T  0.0020 ... <2 dims> #> 21: U  0.0021 ... <2 dims> #> 22: V  0.0022 ... <2 dims> #> 23: W  0.0023 ... <2 dims> #> 24: X  0.0024 ... <2 dims> #> 25: Y  0.0025 ... <2 dims> #> 26: Z  0.0026 ... <2 dims> as_wordvec(df) #> # wordvec (data.table): 26 × 2 (NOT normalized) #>     word                    vec #>  1:    A [ 0.0001, ...<2 dims>] #>  2:    B [ 0.0002, ...<2 dims>] #>  3:    C [ 0.0003, ...<2 dims>] #>  4:    D [ 0.0004, ...<2 dims>] #>  5:    E [ 0.0005, ...<2 dims>] #>  6:    F [ 0.0006, ...<2 dims>] #>  7:    G [ 0.0007, ...<2 dims>] #>  8:    H [ 0.0008, ...<2 dims>] #>  9:    I [ 0.0009, ...<2 dims>] #> 10:    J [ 0.0010, ...<2 dims>] #> 11:    K [ 0.0011, ...<2 dims>] #> 12:    L [ 0.0012, ...<2 dims>] #> 13:    M [ 0.0013, ...<2 dims>] #> 14:    N [ 0.0014, ...<2 dims>] #> 15:    O [ 0.0015, ...<2 dims>] #> 16:    P [ 0.0016, ...<2 dims>] #> 17:    Q [ 0.0017, ...<2 dims>] #> 18:    R [ 0.0018, ...<2 dims>] #> 19:    S [ 0.0019, ...<2 dims>] #> 20:    T [ 0.0020, ...<2 dims>] #> 21:    U [ 0.0021, ...<2 dims>] #> 22:    V [ 0.0022, ...<2 dims>] #> 23:    W [ 0.0023, ...<2 dims>] #> 24:    X [ 0.0024, ...<2 dims>] #> 25:    Y [ 0.0025, ...<2 dims>] #> 26:    Z [ 0.0026, ...<2 dims>]"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/cosine_similarity.html","id":null,"dir":"Reference","previous_headings":"","what":"Cosine similarity/distance between two vectors. — cosine_similarity","title":"Cosine similarity/distance between two vectors. — cosine_similarity","text":"Cosine similarity/distance two vectors.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/cosine_similarity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cosine similarity/distance between two vectors. — cosine_similarity","text":"","code":"cosine_similarity(v1, v2, distance = FALSE)  cos_sim(v1, v2)  cos_dist(v1, v2)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/cosine_similarity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cosine similarity/distance between two vectors. — cosine_similarity","text":"v1, v2 Numeric vector (length). distance Compute cosine distance instead? Defaults FALSE (cosine similarity).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/cosine_similarity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cosine similarity/distance between two vectors. — cosine_similarity","text":"value cosine similarity/distance.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/cosine_similarity.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cosine similarity/distance between two vectors. — cosine_similarity","text":"Cosine similarity = sum(v1 * v2) / ( sqrt(sum(v1^2)) * sqrt(sum(v2^2)) ) Cosine distance = 1 - cosine_similarity(v1, v2)","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/cosine_similarity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cosine similarity/distance between two vectors. — cosine_similarity","text":"","code":"cos_sim(v1=c(1,1,1), v2=c(2,2,2))  # 1 #> [1] 1 cos_sim(v1=c(1,4,1), v2=c(4,1,1))  # 0.5 #> [1] 0.5 cos_sim(v1=c(1,1,0), v2=c(0,0,1))  # 0 #> [1] 0  cos_dist(v1=c(1,1,1), v2=c(2,2,2))  # 0 #> [1] -2.220446e-16 cos_dist(v1=c(1,4,1), v2=c(4,1,1))  # 0.5 #> [1] 0.5 cos_dist(v1=c(1,1,0), v2=c(0,0,1))  # 1 #> [1] 1"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_transform.html","id":null,"dir":"Reference","previous_headings":"","what":"Transform plain text of word vectors into\nwordvec (data.table) or embed (matrix),\nsaved in a compressed ","title":"Transform plain text of word vectors into\nwordvec (data.table) or embed (matrix),\nsaved in a compressed ","text":"Transform plain text word vectors wordvec (data.table) embed (matrix), saved compressed \".RData\" file. Speed: total (preprocess + compress + save), can process 30000 words/min slowest settings (compress=\"xz\", compress.level=9) modern computer (HP ProBook 450, Windows 11, Intel i7-1165G7 CPU, 32GB RAM).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_transform.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Transform plain text of word vectors into\nwordvec (data.table) or embed (matrix),\nsaved in a compressed ","text":"","code":"data_transform(   file.load,   file.save,   as = c(\"wordvec\", \"embed\"),   sep = \" \",   header = \"auto\",   encoding = \"auto\",   compress = \"bzip2\",   compress.level = 9,   verbose = TRUE )"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_transform.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Transform plain text of word vectors into\nwordvec (data.table) or embed (matrix),\nsaved in a compressed ","text":"file.load File name raw text (must plain text). Data must format (values separated sep): cat 0.001 0.002 0.003 0.004 0.005 ... 0.300 dog 0.301 0.302 0.303 0.304 0.305 ... 0.600 file.save File name --saved R data (must .RData). Transform text R object? wordvec (data.table) embed (matrix). Defaults wordvec. sep Column separator. Defaults \" \". header 1st row header (e.g., meta-information \"2000000 300\")? Defaults \"auto\", automatically determines whether header. TRUE, 1st row dropped. encoding File encoding. Defaults \"auto\" (using vroom::vroom_lines() fast read file). specified value (e.g., \"UTF-8\"), uses readLines() read file, much slower vroom. compress Compression method saved file. Defaults \"bzip2\". Options include: 1 \"gzip\": modest file size (fastest) 2 \"bzip2\": small file size (fast) 3 \"xz\": minimized file size (slow) compress.level Compression level 0 (none) 9 (maximal compression minimal file size). Defaults 9. verbose Print information console? Defaults TRUE.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_transform.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Transform plain text of word vectors into\nwordvec (data.table) or embed (matrix),\nsaved in a compressed ","text":"wordvec (data.table) embed (matrix).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_transform.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Transform plain text of word vectors into\nwordvec (data.table) or embed (matrix),\nsaved in a compressed ","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_transform.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Transform plain text of word vectors into\nwordvec (data.table) or embed (matrix),\nsaved in a compressed ","text":"","code":"if (FALSE) { # please first manually download plain text data of word vectors # e.g., from: https://fasttext.cc/docs/en/crawl-vectors.html  # the text file must be on your disk # the following code cannot run unless you have the file library(bruceR) set.wd() data_transform(file.load=\"cc.zh.300.vec\",   # plain text file                file.save=\"cc.zh.300.vec.RData\",  # RData file                header=TRUE, compress=\"xz\")  # of minimal size }"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_load.html","id":null,"dir":"Reference","previous_headings":"","what":"Load word vectors data (wordvec or embed) from ","title":"Load word vectors data (wordvec or embed) from ","text":"Load word vectors data (wordvec embed) \".RData\" file.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_load.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load word vectors data (wordvec or embed) from ","text":"","code":"data_wordvec_load(   file,   as = c(\"wordvec\", \"embed\"),   normalize = FALSE,   verbose = TRUE )  load_wordvec(file, normalize = TRUE)  load_embed(file, normalize = TRUE)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_load.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load word vectors data (wordvec or embed) from ","text":"file File name .RData transformed data_transform. Can also .RData file containing embedding matrix words row names. Load wordvec (data.table) embed (matrix). Defaults original class R object file. two wrapper functions load_wordvec load_embed automatically reshape data corresponding class normalize word vectors (faster future use). normalize Normalize word vectors unit length? Defaults FALSE. See normalize. verbose Print information console? Defaults TRUE.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_load.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load word vectors data (wordvec or embed) from ","text":"wordvec (data.table) embed (matrix).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_load.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Load word vectors data (wordvec or embed) from ","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_load.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Load word vectors data (wordvec or embed) from ","text":"","code":"d = demodata[1:200] save(d, file=\"demo.RData\") d = load_wordvec(\"demo.RData\") #> Loading... #> ✔ Word vectors data: 200 vocab, 300 dims (time cost = 0.02 secs) #> ✔ All word vectors have been normalized to unit length 1. d #> # wordvec (data.table): 200 × 2 (normalized) #>        word                      vec #>   1:     in [ 0.0703, ...<300 dims>] #>   2:    for [-0.0118, ...<300 dims>] #>   3:   that [-0.0157, ...<300 dims>] #>   4:     is [ 0.0070, ...<300 dims>] #>   5:     on [ 0.0267, ...<300 dims>] #> ----                                 #> 196: really [ 0.0962, ...<300 dims>] #> 197:  found [-0.0332, ...<300 dims>] #> 198:   used [ 0.2021, ...<300 dims>] #> 199:    lot [ 0.1729, ...<300 dims>] #> 200:  money [ 0.1582, ...<300 dims>] d = load_embed(\"demo.RData\") #> Loading... #> ✔ Word vectors data: 200 vocab, 300 dims (time cost = 0.005 secs) #> ✔ All word vectors have been normalized to unit length 1. d #> # embed (matrix): 200 × 300 (normalized) #>                dim1 ...     dim300 #>   1: in      0.0703 ... <300 dims> #>   2: for    -0.0118 ... <300 dims> #>   3: that   -0.0157 ... <300 dims> #>   4: is      0.0070 ... <300 dims> #>   5: on      0.0267 ... <300 dims> #> ----                               #> 196: really  0.0962 ... <300 dims> #> 197: found  -0.0332 ... <300 dims> #> 198: used    0.2021 ... <300 dims> #> 199: lot     0.1729 ... <300 dims> #> 200: money   0.1582 ... <300 dims> unlink(\"demo.RData\")  # delete file for code check  if (FALSE) { # please first manually download the .RData file # (see https://psychbruce.github.io/WordVector_RData.pdf) # or transform plain text data by using `data_transform()`  # the RData file must be on your disk # the following code cannot run unless you have the file library(bruceR) set.wd() d = load_embed(\"../data-raw/GloVe/glove_wiki_50d.RData\") d }"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_subset.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract a subset of word vectors data (with S3 methods). — data_wordvec_subset","title":"Extract a subset of word vectors data (with S3 methods). — data_wordvec_subset","text":"Extract subset word vectors data (S3 methods). may specify either wordvec embed loaded data_wordvec_load) .RData file transformed data_transform).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_subset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract a subset of word vectors data (with S3 methods). — data_wordvec_subset","text":"","code":"data_wordvec_subset(   x,   words = NULL,   pattern = NULL,   as = c(\"wordvec\", \"embed\"),   file.save,   compress = \"bzip2\",   compress.level = 9,   verbose = TRUE )  # S3 method for wordvec subset(x, ...)  # S3 method for embed subset(x, ...)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_subset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract a subset of word vectors data (with S3 methods). — data_wordvec_subset","text":"x Can : wordvec embed loaded data_wordvec_load .RData file transformed data_transform words [Option 1] Character string(s). pattern [Option 2] Regular expression (see str_subset). neither words pattern specified (.e., NULL), words data extracted. Reshape wordvec (data.table) embed (matrix). Defaults original class x. file.save File name --saved R data (must .RData). compress Compression method saved file. Defaults \"bzip2\". Options include: 1 \"gzip\": modest file size (fastest) 2 \"bzip2\": small file size (fast) 3 \"xz\": minimized file size (slow) compress.level Compression level 0 (none) 9 (maximal compression minimal file size). Defaults 9. verbose Print information console? Defaults TRUE. ... Parameters passed data_wordvec_subset using S3 method subset.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_subset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract a subset of word vectors data (with S3 methods). — data_wordvec_subset","text":"subset wordvec embed valid (available) words.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_subset.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Extract a subset of word vectors data (with S3 methods). — data_wordvec_subset","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_subset.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract a subset of word vectors data (with S3 methods). — data_wordvec_subset","text":"","code":"## specify `x` as a `wordvec` or `embed` object: subset(demodata, c(\"China\", \"Japan\", \"Korea\")) #> # wordvec (data.table): 3 × 2 (NOT normalized) #>     word                      vec #> 1: China [-0.0732, ...<300 dims>] #> 2: Japan [ 0.0508, ...<300 dims>] #> 3: Korea [ 0.1089, ...<300 dims>] subset(as_embed(demodata), pattern=\"^Chi\") #> 4 words matched... #> # embed (matrix): 4 × 300 (NOT normalized) #>               dim1 ...     dim300 #> 1: China   -0.0732 ... <300 dims> #> 2: Chicago -0.0786 ... <300 dims> #> 3: Chinese -0.1367 ... <300 dims> #> 4: Chile   -0.2754 ... <300 dims>  ## specify `x` and `pattern`, and save with `file.save`: subset(demodata, pattern=\"Chin[ae]|Japan|Korea\",        file.save=\"subset.RData\") #> 6 words matched... #>  #> Compressing and saving... #> ✔ Saved to \"subset.RData\" (time cost = 0.006 secs) #> # wordvec (data.table): 6 × 2 (NOT normalized) #>        word                      vec #> 1:    China [-0.0732, ...<300 dims>] #> 2:    Japan [ 0.0508, ...<300 dims>] #> 3:  Chinese [-0.1367, ...<300 dims>] #> 4: Japanese [ 0.0105, ...<300 dims>] #> 5:   Korean [ 0.0898, ...<300 dims>] #> 6:    Korea [ 0.1089, ...<300 dims>]  ## load the subset: d.subset = load_wordvec(\"subset.RData\") #> Loading... #> ✔ Word vectors data: 6 vocab, 300 dims (time cost = 0.002 secs) #> ✔ All word vectors have been normalized to unit length 1. d.subset #> # wordvec (data.table): 6 × 2 (normalized) #>        word                      vec #> 1:    China [-0.0732, ...<300 dims>] #> 2:    Japan [ 0.0508, ...<300 dims>] #> 3:  Chinese [-0.1367, ...<300 dims>] #> 4: Japanese [ 0.0105, ...<300 dims>] #> 5:   Korean [ 0.0898, ...<300 dims>] #> 6:    Korea [ 0.1089, ...<300 dims>]  ## specify `x` as an .RData file and save with `file.save`: data_wordvec_subset(\"subset.RData\",                     words=c(\"China\", \"Chinese\"),                     file.save=\"new.subset.RData\") #> Loading... #> ✔ Word vectors data: 6 vocab, 300 dims (time cost = 0.002 secs) #>  #> Compressing and saving... #> ✔ Saved to \"new.subset.RData\" (time cost = 0.003 secs) #> # wordvec (data.table): 2 × 2 (NOT normalized) #>       word                      vec #> 1:   China [-0.0732, ...<300 dims>] #> 2: Chinese [-0.1367, ...<300 dims>] d.new.subset = load_embed(\"new.subset.RData\") #> Loading... #> ✔ Word vectors data: 2 vocab, 300 dims (time cost = 0.002 secs) #> ✔ All word vectors have been normalized to unit length 1. d.new.subset #> # embed (matrix): 2 × 300 (normalized) #>               dim1 ...     dim300 #> 1: China   -0.0732 ... <300 dims> #> 2: Chinese -0.1367 ... <300 dims>  unlink(\"subset.RData\")  # delete file for code check unlink(\"new.subset.RData\")  # delete file for code check"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/demodata.html","id":null,"dir":"Reference","previous_headings":"","what":"Demo data (corpus: Google News; algorithm: word2vec; vocabulary: 8000; dimensions: 300). — demodata","title":"Demo data (corpus: Google News; algorithm: word2vec; vocabulary: 8000; dimensions: 300). — demodata","text":"demo data contains sample 8000 English words 300-d word embeddings (word vectors) trained using \"word2vec\" algorithm based Google News corpus. words Top 8000 frequent wordlist, whereas selected less frequent words appended.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/demodata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Demo data (corpus: Google News; algorithm: word2vec; vocabulary: 8000; dimensions: 300). — demodata","text":"","code":"data(demodata)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/demodata.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Demo data (corpus: Google News; algorithm: word2vec; vocabulary: 8000; dimensions: 300). — demodata","text":"data.table (new class wordvec) two variables word vec, transformed raw data (see URL Source) .RData using data_transform function.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/demodata.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Demo data (corpus: Google News; algorithm: word2vec; vocabulary: 8000; dimensions: 300). — demodata","text":"Google Code - word2vec (https://code.google.com/archive/p/word2vec/)","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/demodata.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Demo data (corpus: Google News; algorithm: word2vec; vocabulary: 8000; dimensions: 300). — demodata","text":"","code":"class(demodata) #> [1] \"wordvec\"    \"data.table\" \"data.frame\" demodata #> # wordvec (data.table): 8000 × 2 (NOT normalized) #>                  word                      vec #>    1:              in [ 0.0703, ...<300 dims>] #>    2:             for [-0.0118, ...<300 dims>] #>    3:            that [-0.0157, ...<300 dims>] #>    4:              is [ 0.0070, ...<300 dims>] #>    5:              on [ 0.0267, ...<300 dims>] #> -----                                          #> 7996:     salesperson [ 0.1245, ...<300 dims>] #> 7997:     computation [ 0.0791, ...<300 dims>] #> 7998:   psychotherapy [ 0.1445, ...<300 dims>] #> 7999:       equations [ 0.3242, ...<300 dims>] #> 8000: psychotherapist [ 0.1357, ...<300 dims>]  embed = as_embed(demodata, normalize=TRUE) class(embed) #> [1] \"embed\"  \"matrix\" \"array\"  embed #> # embed (matrix): 8000 × 300 (normalized) #>                          dim1 ...     dim300 #>    1: in               0.0530 ... <300 dims> #>    2: for             -0.0085 ... <300 dims> #>    3: that            -0.0124 ... <300 dims> #>    4: is               0.0037 ... <300 dims> #>    5: on               0.0167 ... <300 dims> #> -----                                        #> 7996: salesperson      0.0381 ... <300 dims> #> 7997: computation      0.0227 ... <300 dims> #> 7998: psychotherapy    0.0405 ... <300 dims> #> 7999: equations        0.0957 ... <300 dims> #> 8000: psychotherapist  0.0364 ... <300 dims>"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/dict_expand.html","id":null,"dir":"Reference","previous_headings":"","what":"Expand a dictionary from the most similar words. — dict_expand","title":"Expand a dictionary from the most similar words. — dict_expand","text":"Expand dictionary similar words.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/dict_expand.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Expand a dictionary from the most similar words. — dict_expand","text":"","code":"dict_expand(data, words, threshold = 0.5, iteration = 5, verbose = TRUE)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/dict_expand.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Expand a dictionary from the most similar words. — dict_expand","text":"data wordvec (data.table) embed (matrix), see data_wordvec_load. words single word list words, used calculate sum vector. threshold Threshold cosine similarity, used find words similarities higher value. Defaults 0.5. low threshold may lead failure convergence. iteration Number maximum iterations. Defaults 5. verbose Print information console? Defaults TRUE.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/dict_expand.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Expand a dictionary from the most similar words. — dict_expand","text":"expanded list (character vector) words.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/dict_expand.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Expand a dictionary from the most similar words. — dict_expand","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/dict_expand.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Expand a dictionary from the most similar words. — dict_expand","text":"","code":"dict = dict_expand(demodata, \"king\") #> ! Results may be inaccurate if word vectors are not normalized. #> ✔ All word vectors now have been automatically normalized. #>  #> ── Iteration 1 (threshold of cosine similarity = 0.5) ────────────────────────── #> ✔ 3 more words appended: \"queen\", \"royal\", and \"King\" #>  #> ── Iteration 2 (threshold of cosine similarity = 0.5) ────────────────────────── #> ✔ 2 more words appended: \"Queen\" and \"Prince\" #>  #> ── Iteration 3 (threshold of cosine similarity = 0.5) ────────────────────────── #> ✔ No more words appended. Successfully convergent. #>  #> ── Finish (convergent) ── #>  dict #> [1] \"king\"   \"queen\"  \"royal\"  \"King\"   \"Queen\"  \"Prince\"  dict = dict_expand(demodata, cc(\"king, queen\")) #> ! Results may be inaccurate if word vectors are not normalized. #> ✔ All word vectors now have been automatically normalized. #>  #> ── Iteration 1 (threshold of cosine similarity = 0.5) ────────────────────────── #> ✔ 2 more words appended: \"royal\" and \"Queen\" #>  #> ── Iteration 2 (threshold of cosine similarity = 0.5) ────────────────────────── #> ✔ 1 more words appended: \"King\" #>  #> ── Iteration 3 (threshold of cosine similarity = 0.5) ────────────────────────── #> ✔ 1 more words appended: \"Prince\" #>  #> ── Iteration 4 (threshold of cosine similarity = 0.5) ────────────────────────── #> ✔ No more words appended. Successfully convergent. #>  #> ── Finish (convergent) ── #>  dict #> [1] \"king\"   \"queen\"  \"royal\"  \"Queen\"  \"King\"   \"Prince\"  most_similar(demodata, dict) #> ! Results may be inaccurate if word vectors are not normalized. #> ✔ All word vectors now have been automatically normalized. #> [Word Vector] =~ king + queen + royal + Queen + King + Prince #> (normalized to unit length) #>          word   cos_sim row_id #>  1:   Jackson 0.3615837   1066 #>  2:    Albert 0.3607588   4068 #>  3:     Crown 0.3587386   5054 #>  4:     crown 0.3399563   5174 #>  5: superstar 0.3375027   5428 #>  6:   royalty 0.3353748   6388 #>  7:     Kings 0.3330198   6472 #>  8:    legend 0.3305279   6775 #>  9:       Sir 0.3121715   7218 #> 10:      pope 0.3093004   7893  dict.cn = dict_expand(demodata, \"China\") #> ! Results may be inaccurate if word vectors are not normalized. #> ✔ All word vectors now have been automatically normalized. #>  #> ── Iteration 1 (threshold of cosine similarity = 0.5) ────────────────────────── #> ✔ 19 more words appended: \"Chinese\", \"Beijing\", \"Taiwan\", \"Shanghai\", \"Shenzhen\", \"Guangzhou\", \"yuan\", \"India\", \"Japan\", \"Li\", \"Asia\", \"Korea\", \"Taiwanese\", \"mainland\", \"Russia\", \"Tibet\", \"Wang\", \"Chen\", and \"Vietnam\" #>  #> ── Iteration 2 (threshold of cosine similarity = 0.5) ────────────────────────── #> ✔ 14 more words appended: \"Seoul\", \"HK\", \"Singapore\", \"Korean\", \"Thailand\", \"Japanese\", \"Pyongyang\", \"Vietnamese\", \"Asian\", \"Tokyo\", \"Malaysia\", \"Xinhua\", \"Thai\", and \"Malaysian\" #>  #> ── Iteration 3 (threshold of cosine similarity = 0.5) ────────────────────────── #> ✔ 5 more words appended: \"Indonesian\", \"Indonesia\", \"Bangkok\", \"Philippine\", and \"Philippines\" #>  #> ── Iteration 4 (threshold of cosine similarity = 0.5) ────────────────────────── #> ✔ 1 more words appended: \"Nepal\" #>  #> ── Iteration 5 (threshold of cosine similarity = 0.5) ────────────────────────── #> ✔ No more words appended. Successfully convergent. #>  #> ── Finish (convergent) ── #>  dict.cn  # too inclusive if setting threshold = 0.5 #>  [1] \"China\"       \"Chinese\"     \"Beijing\"     \"Taiwan\"      \"Shanghai\"    #>  [6] \"Shenzhen\"    \"Guangzhou\"   \"yuan\"        \"India\"       \"Japan\"       #> [11] \"Li\"          \"Asia\"        \"Korea\"       \"Taiwanese\"   \"mainland\"    #> [16] \"Russia\"      \"Tibet\"       \"Wang\"        \"Chen\"        \"Vietnam\"     #> [21] \"Seoul\"       \"HK\"          \"Singapore\"   \"Korean\"      \"Thailand\"    #> [26] \"Japanese\"    \"Pyongyang\"   \"Vietnamese\"  \"Asian\"       \"Tokyo\"       #> [31] \"Malaysia\"    \"Xinhua\"      \"Thai\"        \"Malaysian\"   \"Indonesian\"  #> [36] \"Indonesia\"   \"Bangkok\"     \"Philippine\"  \"Philippines\" \"Nepal\"        dict.cn = dict_expand(demodata,                       cc(\"China, Chinese\"),                       threshold=0.6) #> ! Results may be inaccurate if word vectors are not normalized. #> ✔ All word vectors now have been automatically normalized. #>  #> ── Iteration 1 (threshold of cosine similarity = 0.6) ────────────────────────── #> ✔ 8 more words appended: \"Beijing\", \"Taiwan\", \"Taiwanese\", \"Shanghai\", \"Li\", \"Guangzhou\", \"Shenzhen\", and \"yuan\" #>  #> ── Iteration 2 (threshold of cosine similarity = 0.6) ────────────────────────── #> ✔ 4 more words appended: \"Wang\", \"Chen\", \"mainland\", and \"HK\" #>  #> ── Iteration 3 (threshold of cosine similarity = 0.6) ────────────────────────── #> ✔ No more words appended. Successfully convergent. #>  #> ── Finish (convergent) ── #>  dict.cn  # adequate to represent \"China\" #>  [1] \"China\"     \"Chinese\"   \"Beijing\"   \"Taiwan\"    \"Taiwanese\" \"Shanghai\"  #>  [7] \"Li\"        \"Guangzhou\" \"Shenzhen\"  \"yuan\"      \"Wang\"      \"Chen\"      #> [13] \"mainland\"  \"HK\""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/dict_reliability.html","id":null,"dir":"Reference","previous_headings":"","what":"Reliability analysis and PCA of a dictionary. — dict_reliability","title":"Reliability analysis and PCA of a dictionary. — dict_reliability","text":"Reliability analysis (Cronbach's \\(\\alpha\\) average cosine similarity) Principal Component Analysis (PCA) dictionary, visualization cosine similarities words (ordered first principal component loading). Note Cronbach's \\(\\alpha\\) can misleading number items/words large.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/dict_reliability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reliability analysis and PCA of a dictionary. — dict_reliability","text":"","code":"dict_reliability(   data,   words = NULL,   pattern = NULL,   alpha = TRUE,   sort = TRUE,   plot = TRUE,   ... )"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/dict_reliability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reliability analysis and PCA of a dictionary. — dict_reliability","text":"data wordvec (data.table) embed (matrix), see data_wordvec_load. words [Option 1] Character string(s). pattern [Option 2] Regular expression (see str_subset). neither words pattern specified (.e., NULL), words data extracted. alpha Estimate Cronbach's \\(\\alpha\\)? Defaults TRUE. Note can misleading time-consuming number items/words large. sort Sort items first principal component loading (PC1)? Defaults TRUE. plot Visualize cosine similarities? Defaults TRUE. ... parameters passed plot_similarity.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/dict_reliability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reliability analysis and PCA of a dictionary. — dict_reliability","text":"list object new class reliability: alpha Cronbach's \\(\\alpha\\) eigen Eigen values PCA pca PCA (1 principal component) pca.rotation PCA varimax rotation (potential principal components > 1) items Item statistics cos.sim.mat matrix cosine similarities word pairs cos.sim Lower triangular part matrix cosine similarities","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/dict_reliability.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Reliability analysis and PCA of a dictionary. — dict_reliability","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/dict_reliability.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Reliability analysis and PCA of a dictionary. — dict_reliability","text":"Nicolas, G., Bai, X., & Fiske, S. T. (2021). Comprehensive stereotype content dictionaries using semi-automated method. European Journal Social Psychology, 51(1), 178--196.","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/dict_reliability.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Reliability analysis and PCA of a dictionary. — dict_reliability","text":"","code":"d = normalize(demodata)  dict = dict_expand(d, \"king\") #> ── Iteration 1 (threshold of cosine similarity = 0.5) ────────────────────────── #> ✔ 3 more words appended: \"queen\", \"royal\", and \"King\" #>  #> ── Iteration 2 (threshold of cosine similarity = 0.5) ────────────────────────── #> ✔ 2 more words appended: \"Queen\" and \"Prince\" #>  #> ── Iteration 3 (threshold of cosine similarity = 0.5) ────────────────────────── #> ✔ No more words appended. Successfully convergent. #>  #> ── Finish (convergent) ── #>  dict_reliability(d, dict) #> ! Results may be inaccurate if word vectors are not normalized. #> ✔ All word vectors now have been automatically normalized.  #>  #> ── Reliability Analysis and PCA of Dictionary ────────────────────────────────── #>  #> Number of items = 6 #> Mean cosine similarity = 0.459 #> Cronbach’s α = 0.836 (misleading when N of items is large) #> Variance explained by PC1 = 55.2% #> Potential principal components = 1 (with eigen value > 1) #>  #> Cosine Similarities Between Words: #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>  0.2584  0.3715  0.4588  0.4590  0.5273  0.6511  #>  #> Item Statistics: #> ──────────────────────────────────────────────────── #>         PC1 Loading Item-SumVec Sim. Item-Rest Corr. #> ──────────────────────────────────────────────────── #> queen         0.785            0.769           0.650 #> king          0.781            0.772           0.653 #> Queen         0.760            0.755           0.631 #> royal         0.754            0.751           0.624 #> King          0.723            0.732           0.597 #> Prince        0.645            0.668           0.511 #> ──────────────────────────────────────────────────── #> PC1 Loading = the first principal component loading #> Item-SumVec Sim. = cosine similarity with the sum vector #> Item-Rest Corr. = corrected item-total correlation  dict.cn = dict_expand(d, \"China\", threshold=0.65) #>  #> ── Iteration 1 (threshold of cosine similarity = 0.65) ───────────────────────── #> ✔ 4 more words appended: \"Chinese\", \"Beijing\", \"Taiwan\", and \"Shanghai\" #>  #> ── Iteration 2 (threshold of cosine similarity = 0.65) ───────────────────────── #> ✔ 4 more words appended: \"Guangzhou\", \"Taiwanese\", \"Shenzhen\", and \"Li\" #>  #> ── Iteration 3 (threshold of cosine similarity = 0.65) ───────────────────────── #> ✔ 3 more words appended: \"Wang\", \"Chen\", and \"yuan\" #>  #> ── Iteration 4 (threshold of cosine similarity = 0.65) ───────────────────────── #> ✔ No more words appended. Successfully convergent. #>  #> ── Finish (convergent) ── #>  dict_reliability(d, dict.cn) #> ! Results may be inaccurate if word vectors are not normalized. #> ✔ All word vectors now have been automatically normalized.  #>  #> ── Reliability Analysis and PCA of Dictionary ────────────────────────────────── #>  #> Number of items = 12 #> Mean cosine similarity = 0.596 #> Cronbach’s α = 0.946 (misleading when N of items is large) #> Variance explained by PC1 = 63.0% #> Potential principal components = 2 (with eigen value > 1) #>  #> Cosine Similarities Between Words: #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>  0.4474  0.5266  0.5666  0.5963  0.6395  0.8555  #>  #> Item Statistics: #> ─────────────────────────────────────────────────────── #>            PC1 Loading Item-SumVec Sim. Item-Rest Corr. #> ─────────────────────────────────────────────────────── #> China            0.839            0.837           0.801 #> Chinese          0.823            0.822           0.781 #> Beijing          0.822            0.817           0.780 #> Li               0.818            0.819           0.778 #> Shanghai         0.817            0.815           0.775 #> Wang             0.793            0.793           0.749 #> Guangzhou        0.791            0.792           0.745 #> Chen             0.786            0.787           0.741 #> Shenzhen         0.781            0.783           0.735 #> Taiwan           0.773            0.775           0.726 #> Taiwanese        0.770            0.773           0.723 #> yuan             0.704            0.711           0.651 #> ─────────────────────────────────────────────────────── #> PC1 Loading = the first principal component loading #> Item-SumVec Sim. = cosine similarity with the sum vector #> Item-Rest Corr. = corrected item-total correlation  dict_reliability(d, c(dict, dict.cn)) #> ! Results may be inaccurate if word vectors are not normalized. #> ✔ All word vectors now have been automatically normalized.  #>  #> ── Reliability Analysis and PCA of Dictionary ────────────────────────────────── #>  #> Number of items = 18 #> Mean cosine similarity = 0.331 #> Cronbach’s α = 0.899 (misleading when N of items is large) #> Variance explained by PC1 = 42.4% #> Potential principal components = 4 (with eigen value > 1) #>  #> Cosine Similarities Between Words: #>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max.  #> -0.06071  0.07339  0.38084  0.33065  0.56430  0.85546  #>  #> Item Statistics: #> ─────────────────────────────────────────────────────── #>            PC1 Loading Item-SumVec Sim. Item-Rest Corr. #> ─────────────────────────────────────────────────────── #> China            0.832            0.741           0.695 #> Chinese          0.821            0.751           0.705 #> Li               0.819            0.763           0.719 #> Beijing          0.818            0.743           0.699 #> Shanghai         0.812            0.741           0.695 #> Wang             0.794            0.742           0.695 #> Chen             0.789            0.747           0.702 #> Guangzhou        0.786            0.714           0.663 #> Taiwan           0.774            0.723           0.673 #> Taiwanese        0.772            0.726           0.676 #> Shenzhen         0.772            0.685           0.630 #> yuan             0.697            0.631           0.569 #> royal            0.178            0.406           0.329 #> Queen            0.154            0.392           0.308 #> king             0.141            0.382           0.302 #> King             0.127            0.361           0.277 #> queen            0.124            0.364           0.286 #> Prince           0.085            0.305           0.218 #> ─────────────────────────────────────────────────────── #> PC1 Loading = the first principal component loading #> Item-SumVec Sim. = cosine similarity with the sum vector #> Item-Rest Corr. = corrected item-total correlation # low-loading items should be removed"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/get_wordvec.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract word vector(s). — get_wordvec","title":"Extract word vector(s). — get_wordvec","text":"Extract word vector(s), using either list words regular expression.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/get_wordvec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract word vector(s). — get_wordvec","text":"","code":"get_wordvec(   data,   words = NULL,   pattern = NULL,   plot = FALSE,   plot.dims = NULL,   plot.step = 0.05,   plot.border = \"white\" )"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/get_wordvec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract word vector(s). — get_wordvec","text":"data wordvec (data.table) embed (matrix), see data_wordvec_load. words [Option 1] Character string(s). pattern [Option 2] Regular expression (see str_subset). neither words pattern specified (.e., NULL), words data extracted. plot Generate plot illustrate word vectors? Defaults FALSE. plot.dims Dimensions plotted (e.g., 1:100). Defaults NULL (plot dimensions). plot.step Step value breaks. Defaults 0.05. plot.border Color tile border. Defaults \"white\". remove border color, set plot.border=NA.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/get_wordvec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract word vector(s). — get_wordvec","text":"data.table words columns dimensions rows.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/get_wordvec.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Extract word vector(s). — get_wordvec","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/get_wordvec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract word vector(s). — get_wordvec","text":"","code":"d = normalize(demodata)  get_wordvec(d, c(\"China\", \"Japan\", \"Korea\")) #>            China        Japan        Korea #>   1: -0.02697318  0.018329469  0.033778005 #>   2:  0.04999036  0.090237829  0.133294179 #>   3:  0.04010034  0.060275982 -0.021963288 #>   4:  0.03056975  0.030490640  0.001675763 #>   5: -0.04711339 -0.095877693 -0.014011015 #>  ---                                       #> 296: -0.01393625 -0.087418077 -0.072705831 #> 297: -0.06761304 -0.105042608 -0.115117643 #> 298:  0.05070960  0.048291315  0.015374394 #> 299:  0.03452576  0.002103985 -0.043017655 #> 300: -0.02931099  0.008636121  0.077552886 get_wordvec(d, cc(\" China, Japan; Korea \")) #>            China        Japan        Korea #>   1: -0.02697318  0.018329469  0.033778005 #>   2:  0.04999036  0.090237829  0.133294179 #>   3:  0.04010034  0.060275982 -0.021963288 #>   4:  0.03056975  0.030490640  0.001675763 #>   5: -0.04711339 -0.095877693 -0.014011015 #>  ---                                       #> 296: -0.01393625 -0.087418077 -0.072705831 #> 297: -0.06761304 -0.105042608 -0.115117643 #> 298:  0.05070960  0.048291315  0.015374394 #> 299:  0.03452576  0.002103985 -0.043017655 #> 300: -0.02931099  0.008636121  0.077552886  ## specify `pattern`: get_wordvec(d, pattern=\"Chin[ae]|Japan|Korea\") #> 6 words matched... #>            China        Japan     Chinese     Japanese      Korean        Korea #>   1: -0.02697318  0.018329469 -0.04985132  0.003658937  0.02635512  0.033778005 #>   2:  0.04999036  0.090237829  0.04593451  0.091899336  0.11745195  0.133294179 #>   3:  0.04010034  0.060275982  0.07940573  0.062968012  0.03810029 -0.021963288 #>   4:  0.03056975  0.030490640  0.02581589  0.029782102  0.02850357  0.001675763 #>   5: -0.04711339 -0.095877693 -0.03204736 -0.093941412 -0.05127785 -0.014011015 #>  ---                                                                            #> 296: -0.01393625 -0.087418077  0.04593451 -0.055820224 -0.05213705 -0.072705831 #> 297: -0.06761304 -0.105042608 -0.06907951 -0.119128757 -0.08479435 -0.115117643 #> 298:  0.05070960  0.048291315  0.06694317  0.074540472  0.02363348  0.015374394 #> 299:  0.03452576  0.002103985  0.05448025  0.001999554 -0.05614764 -0.043017655 #> 300: -0.02931099  0.008636121 -0.03275947  0.009104821  0.03867319  0.077552886  ## plot word vectors: get_wordvec(d, cc(\"China, Japan, Korea,                    Mac, Linux, Windows\"),             plot=TRUE, plot.dims=1:100)  #>            China        Japan        Korea          Mac        Linux #>   1: -0.02697318  0.018329469  0.033778005 -0.017270569  0.068922067 #>   2:  0.04999036  0.090237829  0.133294179 -0.012575617 -0.118698961 #>   3:  0.04010034  0.060275982 -0.021963288 -0.052985444 -0.071656892 #>   4:  0.03056975  0.030490640  0.001675763  0.078136677  0.030358376 #>   5: -0.04711339 -0.095877693 -0.014011015 -0.057009737 -0.045400893 #>  ---                                                                 #> 296: -0.01393625 -0.087418077 -0.072705831 -0.088532396 -0.001282133 #> 297: -0.06761304 -0.105042608 -0.115117643 -0.063716664  0.025572363 #> 298:  0.05070960  0.048291315  0.015374394 -0.005407505  0.072751102 #> 299:  0.03452576  0.002103985 -0.043017655 -0.016767489 -0.063451858 #> 300: -0.02931099  0.008636121  0.077552886  0.040912906  0.056341033 #>            Windows #>   1:  0.0596512714 #>   2: -0.0797136210 #>   3: -0.0818534443 #>   4: -0.0143777935 #>   5: -0.0738285592 #>  ---               #> 296:  0.0235396994 #> 297:  0.0005308471 #> 298:  0.0098973674 #> 299:  0.0572438332 #> 300:  0.1321425780  ## a more complex example:  words = cc(\" China Chinese Japan Japanese good bad great terrible morning evening king queen man woman he she cat dog \")  dt = get_wordvec(   d, words,   plot=TRUE,   plot.dims=1:100,   plot.step=0.06)   # if you want to change something: attr(dt, \"ggplot\") +   scale_fill_viridis_b(n.breaks=10, show.limits=TRUE) +   theme(legend.key.height=unit(0.1, \"npc\")) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale.   # or to save the plot: ggsave(attr(dt, \"ggplot\"),        filename=\"wordvecs.png\",        width=8, height=5, dpi=500) unlink(\"wordvecs.png\")  # delete file for code check"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/most_similar.html","id":null,"dir":"Reference","previous_headings":"","what":"Find the Top-N most similar words. — most_similar","title":"Find the Top-N most similar words. — most_similar","text":"Find Top-N similar words, replicates results produced Python gensim module most_similar() function. (Exact replication gensim requires word vectors data, demodata used examples.)","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/most_similar.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find the Top-N most similar words. — most_similar","text":"","code":"most_similar(   data,   x = NULL,   topn = 10,   above = NULL,   keep = FALSE,   row.id = TRUE,   verbose = TRUE )"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/most_similar.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find the Top-N most similar words. — most_similar","text":"data wordvec (data.table) embed (matrix), see data_wordvec_load. x Can : NULL: use sum word vectors data single word:  \"China\" list words:  c(\"king\", \"queen\") cc(\" king , queen ; man | woman\") R formula (~ xxx) specifying   words positively negatively   contribute similarity (word analogy):  ~ boy - +  ~ king - man + woman  ~ Beijing - China + Japan topn Top-N similar words. Defaults 10. Defaults NULL. Can : threshold value find words cosine similarities   higher value critical word find words cosine similarities   higher critical word topn specified, wins. keep Keep words specified x results? Defaults FALSE. row.id Return row number word? Defaults TRUE, may help determine relative word frequency cases. verbose Print information console? Defaults TRUE.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/most_similar.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find the Top-N most similar words. — most_similar","text":"data.table similar words cosine similarities.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/most_similar.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Find the Top-N most similar words. — most_similar","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/most_similar.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find the Top-N most similar words. — most_similar","text":"","code":"d = normalize(demodata)  most_similar(d) #> [Word Vector] =~ <all> #> (normalized to unit length) #>       word   cos_sim row_id #>  1:   just 0.5165516      3 #>  2:     do 0.5067575      9 #>  3:   that 0.4886930     11 #>  4:    the 0.4882641     13 #>  5: anyway 0.4879882     51 #>  6:   even 0.4823844     67 #>  7:    not 0.4755315     74 #>  8:     it 0.4727806    136 #>  9:     so 0.4703010    196 #> 10: really 0.4693804   3598 most_similar(d, \"China\") #> [Word Vector] =~ China #> (normalized to unit length) #>          word   cos_sim row_id #>  1:   Chinese 0.7678081    485 #>  2:   Beijing 0.7648461    821 #>  3:    Taiwan 0.7081156    924 #>  4:  Shanghai 0.6727434   2086 #>  5:  Shenzhen 0.6239033   3011 #>  6: Guangzhou 0.6223897   3932 #>  7:      yuan 0.6005429   4619 #>  8:     India 0.6004212   7558 #>  9:     Japan 0.5967756   7984 #> 10:        Li 0.5882897   7986 most_similar(d, c(\"king\", \"queen\")) #> [Word Vector] =~ king + queen #> (normalized to unit length) #>          word   cos_sim row_id #>  1:     royal 0.5985594   1449 #>  2:     Queen 0.5487501   1509 #>  3:      King 0.4662653   4267 #>  4:    legend 0.3730853   4631 #>  5:     crown 0.3673733   5174 #>  6: superstar 0.3652350   5428 #>  7:  champion 0.3587134   6388 #>  8:      lady 0.3581903   6754 #>  9:     lover 0.3425792   7218 #> 10:      pope 0.3420032   7947 most_similar(d, cc(\" king , queen ; man | woman \")) #> [Word Vector] =~ king + queen + man + woman #> (normalized to unit length) #>            word   cos_sim row_id #>  1:        girl 0.6387542    754 #>  2:         boy 0.5966774   1257 #>  3:    teenager 0.5941694   1379 #>  4:        lady 0.5923386   1537 #>  5: grandmother 0.5081522   3922 #>  6:      mother 0.5000261   4345 #>  7:       lover 0.4826535   4631 #>  8:      victim 0.4806608   4811 #>  9:   boyfriend 0.4717627   5364 #> 10:  girlfriend 0.4709186   7947  # the same as above: most_similar(d, ~ China) #> [Word Vector] =~ China #> (normalized to unit length) #>          word   cos_sim row_id #>  1:   Chinese 0.7678081    485 #>  2:   Beijing 0.7648461    821 #>  3:    Taiwan 0.7081156    924 #>  4:  Shanghai 0.6727434   2086 #>  5:  Shenzhen 0.6239033   3011 #>  6: Guangzhou 0.6223897   3932 #>  7:      yuan 0.6005429   4619 #>  8:     India 0.6004212   7558 #>  9:     Japan 0.5967756   7984 #> 10:        Li 0.5882897   7986 most_similar(d, ~ king + queen) #> [Word Vector] =~ king + queen #> (normalized to unit length) #>          word   cos_sim row_id #>  1:     royal 0.5985594   1449 #>  2:     Queen 0.5487501   1509 #>  3:      King 0.4662653   4267 #>  4:    legend 0.3730853   4631 #>  5:     crown 0.3673733   5174 #>  6: superstar 0.3652350   5428 #>  7:  champion 0.3587134   6388 #>  8:      lady 0.3581903   6754 #>  9:     lover 0.3425792   7218 #> 10:      pope 0.3420032   7947 most_similar(d, ~ king + queen + man + woman) #> [Word Vector] =~ king + queen + man + woman #> (normalized to unit length) #>            word   cos_sim row_id #>  1:        girl 0.6387542    754 #>  2:         boy 0.5966774   1257 #>  3:    teenager 0.5941694   1379 #>  4:        lady 0.5923386   1537 #>  5: grandmother 0.5081522   3922 #>  6:      mother 0.5000261   4345 #>  7:       lover 0.4826535   4631 #>  8:      victim 0.4806608   4811 #>  9:   boyfriend 0.4717627   5364 #> 10:  girlfriend 0.4709186   7947  most_similar(d, ~ boy - he + she) #> [Word Vector] =~ boy - he + she #> (normalized to unit length) #>            word   cos_sim row_id #>  1:        girl 0.8635271    558 #>  2:       woman 0.6822032    702 #>  3:      mother 0.6530156    754 #>  4:    daughter 0.6431009   1078 #>  5:    teenager 0.6310561   1195 #>  6:       child 0.5933921   1257 #>  7: grandmother 0.5800967   1838 #>  8:        teen 0.5755065   3542 #>  9:        baby 0.5672891   4345 #> 10:       girls 0.5599151   5364 most_similar(d, ~ Jack - he + she) #> [Word Vector] =~ Jack - he + she #> (normalized to unit length) #>        word   cos_sim row_id #>  1:    Jane 0.6338590   3717 #>  2: Rebecca 0.6054167   4141 #>  3:   Julie 0.5988102   4521 #>  4:   Sarah 0.5867900   4899 #>  5:     Amy 0.5831094   4955 #>  6:   Susan 0.5812214   5502 #>  7:    Lisa 0.5766206   5721 #>  8:     Ann 0.5765654   5861 #>  9:   Alice 0.5649283   6896 #> 10:   Carol 0.5647291   7340 most_similar(d, ~ Rose - she + he) #> [Word Vector] =~ Rose - she + he #> (normalized to unit length) #>         word   cos_sim row_id #>  1:  Leonard 0.4443190    761 #>  2:   Martin 0.4206247   1214 #>  3:   Thomas 0.4078561   1425 #>  4:  Wallace 0.3833057   2257 #>  5:  Francis 0.3793813   2698 #>  6:  Johnson 0.3757961   3379 #>  7:    Allen 0.3736620   4350 #>  8: Robinson 0.3725750   4738 #>  9:    Evans 0.3639734   5220 #> 10:   Duncan 0.3635448   6245  most_similar(d, ~ king - man + woman) #> [Word Vector] =~ king - man + woman #> (normalized to unit length) #>         word   cos_sim row_id #>  1:    queen 0.7118192     65 #>  2:    royal 0.4938203    754 #>  3:    Queen 0.4346379   1078 #>  4:     King 0.3749903   1449 #>  5:      she 0.3341126   4267 #>  6:     lady 0.3282869   4631 #>  7:   mother 0.3241257   5174 #>  8:    crown 0.3164823   6754 #>  9:     hers 0.3073009   7852 #> 10: daughter 0.3021213   7981 most_similar(d, ~ Tokyo - Japan + China) #> [Word Vector] =~ Tokyo - Japan + China #> (normalized to unit length) #>          word   cos_sim row_id #>  1:   Beijing 0.8216199    924 #>  2:  Shanghai 0.7951419   2086 #>  3: Guangzhou 0.6529652   3163 #>  4:   Chinese 0.6439487   3932 #>  5:  Shenzhen 0.6439113   4619 #>  6:     Seoul 0.5868835   5180 #>  7:      yuan 0.5821307   7083 #>  8:        Li 0.5712056   7558 #>  9:      Wang 0.5192575   7984 #> 10:    Moscow 0.5082187   7986 most_similar(d, ~ Beijing - China + Japan) #> [Word Vector] =~ Beijing - China + Japan #> (normalized to unit length) #>          word   cos_sim row_id #>  1:     Tokyo 0.8115592   1562 #>  2:     Seoul 0.6568831   2695 #>  3:  Japanese 0.6475989   3011 #>  4: Pyongyang 0.5348969   3017 #>  5:   Bangkok 0.4677356   3163 #>  6:     Korea 0.4660699   3768 #>  7:       yen 0.4631333   5180 #>  8:    Taiwan 0.4330458   6322 #>  9:    Moscow 0.4217667   6510 #> 10: Guangzhou 0.4154183   7986  most_similar(d, \"China\", above=0.7) #> [Word Vector] =~ China #> (normalized to unit length) #>       word   cos_sim row_id #> 1: Chinese 0.7678081    924 #> 2: Beijing 0.7648461   2086 #> 3:  Taiwan 0.7081156   3011 most_similar(d, \"China\", above=\"Shanghai\") #> [Word Vector] =~ China #> (normalized to unit length) #>        word   cos_sim row_id #> 1:  Chinese 0.7678081    924 #> 2:  Beijing 0.7648461   2086 #> 3:   Taiwan 0.7081156   3011 #> 4: Shanghai 0.6727434   3932  # automatically normalized for more accurate results ms = most_similar(demodata, ~ king - man + woman) #> ! Results may be inaccurate if word vectors are not normalized. #> ✔ All word vectors now have been automatically normalized. #> [Word Vector] =~ king - man + woman #> (normalized to unit length) ms #>         word   cos_sim row_id #>  1:    queen 0.7118192     65 #>  2:    royal 0.4938203    754 #>  3:    Queen 0.4346379   1078 #>  4:     King 0.3749903   1449 #>  5:      she 0.3341126   4267 #>  6:     lady 0.3282869   4631 #>  7:   mother 0.3241257   5174 #>  8:    crown 0.3164823   6754 #>  9:     hers 0.3073009   7852 #> 10: daughter 0.3021213   7981 str(ms) #> Classes ‘data.table’ and 'data.frame':\t10 obs. of  3 variables: #>  $ word   : chr  \"queen\" \"royal\" \"Queen\" \"King\" ... #>  $ cos_sim: num  0.712 0.494 0.435 0.375 0.334 ... #>  $ row_id : int  65 754 1078 1449 4267 4631 5174 6754 7852 7981 #>  - attr(*, \".internal.selfref\")=<externalptr>  #>  - attr(*, \"formula\")= chr \"king - man + woman\""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/normalize.html","id":null,"dir":"Reference","previous_headings":"","what":"Normalize all word vectors to the unit length 1. — normalize","title":"Normalize all word vectors to the unit length 1. — normalize","text":"L2-normalization (scaling unit euclidean length): norm vector vector space normalized 1. necessary linear operation word vectors. R code: Vector: vec / sqrt(sum(vec^2)) Matrix: mat / sqrt(rowSums(mat^2))","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/normalize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Normalize all word vectors to the unit length 1. — normalize","text":"","code":"normalize(x)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/normalize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Normalize all word vectors to the unit length 1. — normalize","text":"x wordvec (data.table) embed (matrix), see data_wordvec_load.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/normalize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Normalize all word vectors to the unit length 1. — normalize","text":"wordvec (data.table) embed (matrix) normalized word vectors.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/normalize.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Normalize all word vectors to the unit length 1. — normalize","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/normalize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Normalize all word vectors to the unit length 1. — normalize","text":"","code":"d = normalize(demodata) # the same: d = as_wordvec(demodata, normalize=TRUE)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/orth_procrustes.html","id":null,"dir":"Reference","previous_headings":"","what":"Orthogonal Procrustes rotation for matrix alignment. — orth_procrustes","title":"Orthogonal Procrustes rotation for matrix alignment. — orth_procrustes","text":"order compare word embeddings different time periods, must ensure embedding matrices aligned semantic space (coordinate axes). Orthogonal Procrustes solution (Schönemann, 1966) commonly used align historical embeddings time (Hamilton et al., 2016; Li et al., 2020). Note kind rotation change relative relationships vectors space, thus affect semantic similarities distances within embedding matrix. influence semantic relationships different embedding matrices, thus necessary purposes \"semantic drift analysis\" (e.g., Hamilton et al., 2016; Li et al., 2020). function produces results cds::orthprocr(), psych::Procrustes(), pracma::procrustes().","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/orth_procrustes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Orthogonal Procrustes rotation for matrix alignment. — orth_procrustes","text":"","code":"orth_procrustes(M, X)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/orth_procrustes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Orthogonal Procrustes rotation for matrix alignment. — orth_procrustes","text":"M, X Two embedding matrices size (rows columns), can embed wordvec objects. M reference (anchor/baseline/target) matrix,         e.g., embedding matrix learned         later year (\\(t + 1\\)). X matrix transformed/rotated. Note: function automatically extracts intersection (overlapped part) words M X sorts order (according M).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/orth_procrustes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Orthogonal Procrustes rotation for matrix alignment. — orth_procrustes","text":"matrix wordvec object X rotation, depending class M X.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/orth_procrustes.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Orthogonal Procrustes rotation for matrix alignment. — orth_procrustes","text":"Hamilton, W. L., Leskovec, J., & Jurafsky, D. (2016). Diachronic word embeddings reveal statistical laws semantic change. Proceedings 54th Annual Meeting Association Computational Linguistics (Vol. 1, pp. 1489--1501). Association Computational Linguistics. Li, Y., Hills, T., & Hertwig, R. (2020). brief history risk. Cognition, 203, 104344. Schönemann, P. H. (1966). generalized solution orthogonal Procrustes problem. Psychometrika, 31(1), 1--10.","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/orth_procrustes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Orthogonal Procrustes rotation for matrix alignment. — orth_procrustes","text":"","code":"M = matrix(c(0,0,  1,2,  2,0,  3,2,  4,0), ncol=2, byrow=TRUE) X = matrix(c(0,0, -2,1,  0,2, -2,3,  0,4), ncol=2, byrow=TRUE) rownames(M) = rownames(X) = cc(\"A, B, C, D, E\")  # words colnames(M) = colnames(X) = cc(\"dim1, dim2\")  # dimensions  ggplot() +   geom_path(data=as.data.frame(M), aes(x=dim1, y=dim2),             color=\"red\") +   geom_path(data=as.data.frame(X), aes(x=dim1, y=dim2),             color=\"blue\") +   coord_equal()   # Usage 1: input two matrices (can be `embed` objects) XR = orth_procrustes(M, X) XR  # aligned with M #>   dim1          dim2 #> A    0  0.000000e+00 #> B    1  2.000000e+00 #> C    2 -6.337448e-16 #> D    3  2.000000e+00 #> E    4 -1.267490e-15  ggplot() +   geom_path(data=as.data.frame(XR), aes(x=dim1, y=dim2)) +   coord_equal()   # Usage 2: input two `wordvec` objects M.wv = as_wordvec(M) X.wv = as_wordvec(X) XR.wv = orth_procrustes(M.wv, X.wv) XR.wv  # aligned with M.wv #> # wordvec (data.table): 5 × 2 (NOT normalized) #>    word                    vec #> 1:    A [ 0.0000, ...<2 dims>] #> 2:    B [ 1.0000, ...<2 dims>] #> 3:    C [ 2.0000, ...<2 dims>] #> 4:    D [ 3.0000, ...<2 dims>] #> 5:    E [ 4.0000, ...<2 dims>]  # M and X must have the same set and order of words # and the same number of word vector dimensions. # The function extracts only the intersection of words # and sorts them in the same order according to M.  Y = rbind(X, X[rev(rownames(X)),]) rownames(Y)[1:5] = cc(\"F, G, H, I, J\") M.wv = as_wordvec(M) Y.wv = as_wordvec(Y) M.wv  # words: A, B, C, D, E #> # wordvec (data.table): 5 × 2 (NOT normalized) #>    word                    vec #> 1:    A [ 0.0000, ...<2 dims>] #> 2:    B [ 1.0000, ...<2 dims>] #> 3:    C [ 2.0000, ...<2 dims>] #> 4:    D [ 3.0000, ...<2 dims>] #> 5:    E [ 4.0000, ...<2 dims>] Y.wv  # words: F, G, H, I, J, E, D, C, B, A #> # wordvec (data.table): 10 × 2 (NOT normalized) #>     word                    vec #>  1:    F [ 0.0000, ...<2 dims>] #>  2:    G [-2.0000, ...<2 dims>] #>  3:    H [ 0.0000, ...<2 dims>] #>  4:    I [-2.0000, ...<2 dims>] #>  5:    J [ 0.0000, ...<2 dims>] #>  6:    E [ 0.0000, ...<2 dims>] #>  7:    D [-2.0000, ...<2 dims>] #>  8:    C [ 0.0000, ...<2 dims>] #>  9:    B [-2.0000, ...<2 dims>] #> 10:    A [ 0.0000, ...<2 dims>] YR.wv = orth_procrustes(M.wv, Y.wv) YR.wv  # aligned with M.wv, with the same order of words #> # wordvec (data.table): 5 × 2 (NOT normalized) #>    word                    vec #> 1:    A [ 0.0000, ...<2 dims>] #> 2:    B [ 1.0000, ...<2 dims>] #> 3:    C [ 2.0000, ...<2 dims>] #> 4:    D [ 3.0000, ...<2 dims>] #> 5:    E [ 4.0000, ...<2 dims>]"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/pair_similarity.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute a matrix of cosine similarity/distance of word pairs. — pair_similarity","title":"Compute a matrix of cosine similarity/distance of word pairs. — pair_similarity","text":"Compute matrix cosine similarity/distance word pairs.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/pair_similarity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute a matrix of cosine similarity/distance of word pairs. — pair_similarity","text":"","code":"pair_similarity(   data,   words = NULL,   pattern = NULL,   words1 = NULL,   words2 = NULL,   distance = FALSE )"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/pair_similarity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute a matrix of cosine similarity/distance of word pairs. — pair_similarity","text":"data wordvec (data.table) embed (matrix), see data_wordvec_load. words [Option 1] Character string(s). pattern [Option 2] Regular expression (see str_subset). neither words pattern specified (.e., NULL), words data extracted. words1, words2 [Option 3] Two sets words n1 * n2 word pairs. See examples. distance Compute cosine distance instead? Defaults FALSE (cosine similarity).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/pair_similarity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute a matrix of cosine similarity/distance of word pairs. — pair_similarity","text":"matrix pairwise cosine similarity/distance.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/pair_similarity.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Compute a matrix of cosine similarity/distance of word pairs. — pair_similarity","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/pair_similarity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute a matrix of cosine similarity/distance of word pairs. — pair_similarity","text":"","code":"pair_similarity(demodata, c(\"China\", \"Chinese\")) #>             China   Chinese #> China   1.0000000 0.7678081 #> Chinese 0.7678081 1.0000000  pair_similarity(demodata, pattern=\"^Chi\") #> 4 words matched... #>             China    Chicago    Chinese      Chile #> China   1.0000000 0.13040186 0.76780811 0.38012317 #> Chicago 0.1304019 1.00000000 0.09174141 0.08685822 #> Chinese 0.7678081 0.09174141 1.00000000 0.21538189 #> Chile   0.3801232 0.08685822 0.21538189 1.00000000  pair_similarity(demodata,                 words1=c(\"China\", \"Chinese\"),                 words2=c(\"Japan\", \"Japanese\")) #>             Japan Japanese #> China   0.5967756 0.413391 #> Chinese 0.4226447 0.642242"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_similarity.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize cosine similarity of word pairs. — plot_similarity","title":"Visualize cosine similarity of word pairs. — plot_similarity","text":"Visualize cosine similarity word pairs.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_similarity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize cosine similarity of word pairs. — plot_similarity","text":"","code":"plot_similarity(   data,   words = NULL,   pattern = NULL,   words1 = NULL,   words2 = NULL,   label = \"auto\",   value.color = NULL,   value.percent = FALSE,   order = c(\"original\", \"AOE\", \"FPC\", \"hclust\", \"alphabet\"),   hclust.method = c(\"complete\", \"ward\", \"ward.D\", \"ward.D2\", \"single\", \"average\",     \"mcquitty\", \"median\", \"centroid\"),   hclust.n = NULL,   hclust.color = \"black\",   hclust.line = 2,   file = NULL,   width = 8,   height = 6,   dpi = 500,   ... )"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_similarity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize cosine similarity of word pairs. — plot_similarity","text":"data wordvec (data.table) embed (matrix), see data_wordvec_load. words [Option 1] Character string(s). pattern [Option 2] Regular expression (see str_subset). neither words pattern specified (.e., NULL), words data extracted. words1, words2 [Option 3] Two sets words n1 * n2 word pairs. See examples. label Position text labels. Defaults \"auto\" (add labels less 20 words). Can TRUE (left top), FALSE (add labels words), character string (see usage tl.pos corrplot. value.color Color values added plot. Defaults NULL (add values). value.percent Whether transform values percentage style space saving. Defaults FALSE. order Character, ordering method correlation matrix. 'original' original order (default). 'AOE' angular order eigenvectors. 'FPC' first principal component order. 'hclust' hierarchical clustering order. 'alphabet' alphabetical order. See function corrMatOrder details. hclust.method Character, agglomeration method used order hclust. one 'ward', 'ward.D', 'ward.D2', 'single', 'complete', 'average', 'mcquitty', 'median' 'centroid'. hclust.n Number rectangles drawn plot according hierarchical clusters, valid order=\"hclust\". Defaults NULL (add rectangles). hclust.color Color rectangle border, valid hclust.n >= 1. Defaults \"black\". hclust.line Line width rectangle border, valid hclust.n >= 1. Defaults 2. file File name saved, png pdf. width, height Width height (inches) saved file. Defaults 8 6. dpi Dots per inch. Defaults 500 (.e., file resolution: 4000 * 3000). ... parameters passed corrplot.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_similarity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize cosine similarity of word pairs. — plot_similarity","text":"Invisibly return matrix cosine similarity pair words.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_similarity.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Visualize cosine similarity of word pairs. — plot_similarity","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_similarity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize cosine similarity of word pairs. — plot_similarity","text":"","code":"w1 = cc(\"king, queen, man, woman\") plot_similarity(demodata, w1)  plot_similarity(demodata, w1,                 value.color=\"grey\",                 value.percent=TRUE)  plot_similarity(demodata, w1,                 value.color=\"grey\",                 order=\"hclust\",                 hclust.n=2)   plot_similarity(   demodata,   words1=cc(\"man, woman, king, queen\"),   words2=cc(\"he, she, boy, girl, father, mother\") )   w2 = cc(\"China, Chinese,          Japan, Japanese,          Korea, Korean,          man, woman, boy, girl,          good, bad, positive, negative\") plot_similarity(demodata, w2,                 order=\"hclust\",                 hclust.n=3)  plot_similarity(demodata, w2,                 order=\"hclust\",                 hclust.n=7,                 file=\"plot.png\") #> ✔ Saved to /home/runner/work/PsychWordVec/PsychWordVec/docs/reference/plot.png  unlink(\"plot.png\")  # delete file for code check"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvec.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize word vectors. — plot_wordvec","title":"Visualize word vectors. — plot_wordvec","text":"Visualize word vectors.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize word vectors. — plot_wordvec","text":"","code":"plot_wordvec(x, dims = NULL, step = 0.05, border = \"white\")"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize word vectors. — plot_wordvec","text":"x Can : data.table returned get_wordvec wordvec (data.table)   embed (matrix)   loaded data_wordvec_load dims Dimensions plotted (e.g., 1:100). Defaults NULL (plot dimensions). step Step value breaks. Defaults 0.05. border Color tile border. Defaults \"white\". remove border color, set border=NA.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize word vectors. — plot_wordvec","text":"ggplot object.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvec.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Visualize word vectors. — plot_wordvec","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize word vectors. — plot_wordvec","text":"","code":"d = normalize(demodata)  plot_wordvec(d[1:10])  plot_wordvec(as_embed(d[1:10]))   dt = get_wordvec(d, cc(\"king, queen, man, woman\")) dt[, QUEEN := king - man + woman] #>              king        queen         man       woman        QUEEN #>   1:  0.043406531  0.001733313  0.14116230  0.09156568 -0.006190088 #>   2:  0.010262695 -0.047404412  0.05663379 -0.02905080 -0.075421897 #>   3:  0.002965276 -0.022895979  0.01500378 -0.03879578 -0.050834289 #>   4:  0.048116999  0.040793453 -0.03592460 -0.04045076  0.043590844 #>   5: -0.008832774  0.043534590  0.03888312  0.04449576 -0.003220136 #>  ---                                                                #> 296: -0.061239879 -0.013624785 -0.02768308  0.04063452  0.007077723 #> 297: -0.096234342  0.055466349 -0.03613580  0.04467952 -0.015419019 #> 298: -0.029610726 -0.015478958 -0.13101869 -0.03125744  0.070150521 #> 299:  0.031461353  0.053853896 -0.03465654  0.02463828  0.090756176 #> 300:  0.086812717  0.050951612  0.00908675 -0.01107800  0.066647966 dt[, QUEEN := QUEEN / sqrt(sum(QUEEN^2))]  # normalize #>              king        queen         man       woman        QUEEN #>   1:  0.043406531  0.001733313  0.14116230  0.09156568 -0.005502999 #>   2:  0.010262695 -0.047404412  0.05663379 -0.02905080 -0.067050205 #>   3:  0.002965276 -0.022895979  0.01500378 -0.03879578 -0.045191776 #>   4:  0.048116999  0.040793453 -0.03592460 -0.04045076  0.038752340 #>   5: -0.008832774  0.043534590  0.03888312  0.04449576 -0.002862707 #>  ---                                                                #> 296: -0.061239879 -0.013624785 -0.02768308  0.04063452  0.006292108 #> 297: -0.096234342  0.055466349 -0.03613580  0.04467952 -0.013707536 #> 298: -0.029610726 -0.015478958 -0.13101869 -0.03125744  0.062363942 #> 299:  0.031461353  0.053853896 -0.03465654  0.02463828  0.080682407 #> 300:  0.086812717  0.050951612  0.00908675 -0.01107800  0.059250164 names(dt)[5] = \"king - man + woman\" plot_wordvec(dt[, c(1,3,4,5,2)], dims=1:50)   dt = get_wordvec(d, cc(\"boy, girl, he, she\")) dt[, GIRL := boy - he + she] #>               boy         girl           he         she         GIRL #>   1:  0.083967962  0.047750749  0.109257091  0.04049595  0.015206823 #>   2:  0.058881966  0.001287155  0.072653299 -0.01454260 -0.028313934 #>   3:  0.033273650  0.022193947 -0.010884081 -0.05615750 -0.011999766 #>   4: -0.045990576 -0.026229304 -0.016638191 -0.01778671 -0.047139098 #>   5:  0.005705206  0.008532822  0.017608756  0.05861853  0.046714981 #>  ---                                                                 #> 296: -0.019162800  0.023538951  0.061006509  0.03311285 -0.047056460 #> 297: -0.026479563  0.023034833  0.002339808  0.09754833  0.068728958 #> 298: -0.094768644 -0.029928238 -0.098165154  0.01073948  0.014135985 #> 299: -0.036757568 -0.003089446  0.069880173  0.11365708  0.007019343 #> 300:  0.048429498  0.012946613 -0.090954911 -0.05392007  0.085464341 dt[, GIRL := GIRL / sqrt(sum(GIRL^2))]  # normalize #>               boy         girl           he         she         GIRL #>   1:  0.083967962  0.047750749  0.109257091  0.04049595  0.011744750 #>   2:  0.058881966  0.001287155  0.072653299 -0.01454260 -0.021867820 #>   3:  0.033273650  0.022193947 -0.010884081 -0.05615750 -0.009267829 #>   4: -0.045990576 -0.026229304 -0.016638191 -0.01778671 -0.036407138 #>   5:  0.005705206  0.008532822  0.017608756  0.05861853  0.036079578 #>  ---                                                                 #> 296: -0.019162800  0.023538951  0.061006509  0.03311285 -0.036343314 #> 297: -0.026479563  0.023034833  0.002339808  0.09754833  0.053081725 #> 298: -0.094768644 -0.029928238 -0.098165154  0.01073948  0.010917705 #> 299: -0.036757568 -0.003089446  0.069880173  0.11365708  0.005421279 #> 300:  0.048429498  0.012946613 -0.090954911 -0.05392007  0.066007034 names(dt)[5] = \"boy - he + she\" plot_wordvec(dt[, c(1,3,4,5,2)], dims=1:50)   dt = get_wordvec(d, cc(\"   male, man, boy, he, his,   female, woman, girl, she, her\"))  p = plot_wordvec(dt, dims=1:100)  # if you want to change something: p + theme(legend.key.height=unit(0.1, \"npc\"))   # or to save the plot: ggsave(p, filename=\"wordvecs.png\",        width=8, height=5, dpi=500) unlink(\"wordvecs.png\")  # delete file for code check"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvec_tSNE.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize word vectors with dimensionality reduced using t-SNE. — plot_wordvec_tSNE","title":"Visualize word vectors with dimensionality reduced using t-SNE. — plot_wordvec_tSNE","text":"Visualize word vectors dimensionality reduced using t-Distributed Stochastic Neighbor Embedding (t-SNE) method (.e., projecting high-dimensional vectors low-dimensional vector space), implemented Rtsne::Rtsne(). specify random seed expect reproducible results.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvec_tSNE.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize word vectors with dimensionality reduced using t-SNE. — plot_wordvec_tSNE","text":"","code":"plot_wordvec_tSNE(   x,   dims = 2,   perplexity,   theta = 0.5,   colors = NULL,   seed = NULL,   custom.Rtsne = NULL )"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvec_tSNE.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize word vectors with dimensionality reduced using t-SNE. — plot_wordvec_tSNE","text":"x Can : data.table returned get_wordvec wordvec (data.table)   embed (matrix)   loaded data_wordvec_load dims Output dimensionality: 2 (default, common choice) 3. perplexity Perplexity parameter, larger (number words - 1) / 3. Defaults floor((length(dt)-1)/3) (columns dt words). See Rtsne package details. theta Speed/accuracy trade-(increase less accuracy), set 0 exact t-SNE. Defaults 0.5. colors character vector specifying (1) categories words (2-D plot ) (2) exact colors words (2-D 3-D plot). See examples usage. seed Random seed reproducible results. Defaults NULL. custom.Rtsne User-defined Rtsne object using dt.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvec_tSNE.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize word vectors with dimensionality reduced using t-SNE. — plot_wordvec_tSNE","text":"2-D: ggplot object. may extract data object using $data. 3-D: Nothing data invisibly returned, rgl::plot3d() \"called side effect drawing plot\" thus return 3-D plot object.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvec_tSNE.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Visualize word vectors with dimensionality reduced using t-SNE. — plot_wordvec_tSNE","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvec_tSNE.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Visualize word vectors with dimensionality reduced using t-SNE. — plot_wordvec_tSNE","text":"Hinton, G. E., & Salakhutdinov, R. R. (2006). Reducing dimensionality data neural networks. Science, 313(5786), 504--507. van der Maaten, L., & Hinton, G. (2008). Visualizing data using t-SNE. Journal Machine Learning Research, 9, 2579--2605.","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvec_tSNE.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize word vectors with dimensionality reduced using t-SNE. — plot_wordvec_tSNE","text":"","code":"d = normalize(demodata)  dt = get_wordvec(d, cc(\"   man, woman,   king, queen,   China, Beijing,   Japan, Tokyo\"))  ## 2-D (default): plot_wordvec_tSNE(dt, seed=1234)   plot_wordvec_tSNE(dt, seed=1234)$data #>      word        V1         V2 #> 1     man -125.7483 -198.68583 #> 2   woman -169.4172 -198.64312 #> 3    king -204.3606  -28.40194 #> 4   queen -229.3772  -64.29560 #> 5   China  199.8315   72.23791 #> 6 Beijing  156.7961   57.66972 #> 7   Japan  206.4292  169.03203 #> 8   Tokyo  165.8464  191.08683  colors = c(rep(\"#2B579A\", 4), rep(\"#B7472A\", 4)) plot_wordvec_tSNE(dt, colors=colors, seed=1234)   category = c(rep(\"gender\", 4), rep(\"country\", 4)) plot_wordvec_tSNE(dt, colors=category, seed=1234) +   scale_x_continuous(limits=c(-200, 200),                      labels=function(x) x/100) +   scale_y_continuous(limits=c(-200, 200),                      labels=function(x) x/100) +   scale_color_manual(values=c(\"#B7472A\", \"#2B579A\")) #> Warning: Removed 3 rows containing missing values (`geom_point()`). #> Warning: Removed 3 rows containing missing values (`geom_text_repel()`).   ## 3-D: colors = c(rep(\"#2B579A\", 4), rep(\"#B7472A\", 4)) plot_wordvec_tSNE(dt, dims=3, colors=colors, seed=1) #> Warning: RGL: unable to open X11 display #> Warning: 'rgl.init' failed, running with 'rgl.useNULL = TRUE'."},{"path":"https://psychbruce.github.io/PsychWordVec/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. bruceR cc","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/sum_wordvec.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the sum vector of multiple words. — sum_wordvec","title":"Calculate the sum vector of multiple words. — sum_wordvec","text":"Calculate sum vector multiple words.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/sum_wordvec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the sum vector of multiple words. — sum_wordvec","text":"","code":"sum_wordvec(data, x = NULL, verbose = TRUE)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/sum_wordvec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the sum vector of multiple words. — sum_wordvec","text":"data wordvec (data.table) embed (matrix), see data_wordvec_load. x Can : NULL: use sum word vectors data single word:  \"China\" list words:  c(\"king\", \"queen\") cc(\" king , queen ; man | woman\") R formula (~ xxx) specifying   words positively negatively   contribute similarity (word analogy):  ~ boy - +  ~ king - man + woman  ~ Beijing - China + Japan verbose Print information console? Defaults TRUE.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/sum_wordvec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the sum vector of multiple words. — sum_wordvec","text":"Normalized sum vector.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/sum_wordvec.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Calculate the sum vector of multiple words. — sum_wordvec","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/sum_wordvec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the sum vector of multiple words. — sum_wordvec","text":"","code":"sum_wordvec(normalize(demodata), ~ king - man + woman) #>          dim1          dim2          dim3          dim4          dim5  #> -0.0055029992 -0.0670502053 -0.0451917763  0.0387523404 -0.0028627068  #>          dim6          dim7          dim8          dim9         dim10  #> -0.0311541918  0.0722744639 -0.0547962213 -0.0020958381  0.1267937027  #>         dim11         dim12         dim13         dim14         dim15  #> -0.1177978229 -0.1116193754 -0.0432157999  0.0087648568 -0.0212594939  #>         dim16         dim17         dim18         dim19         dim20  #> -0.0361886454 -0.0145757764 -0.0391255380  0.0236719225  0.0679695260  #>         dim21         dim22         dim23         dim24         dim25  #>  0.0825006128  0.0179503178 -0.0102744498  0.0096805959  0.0097418516  #>         dim26         dim27         dim28         dim29         dim30  #> -0.0583066547 -0.1043284916  0.0066451406  0.0774529645 -0.0630043398  #>         dim31         dim32         dim33         dim34         dim35  #>  0.0176932143 -0.0357660002 -0.0127675640  0.0952755743 -0.1004202304  #>         dim36         dim37         dim38         dim39         dim40  #> -0.0307725420  0.0191817489  0.0342936599 -0.0381161430 -0.0036010425  #>         dim41         dim42         dim43         dim44         dim45  #> -0.0222736867 -0.0654523468  0.0581761410  0.0386649344  0.0625108822  #>         dim46         dim47         dim48         dim49         dim50  #>  0.0144455335 -0.0298451219  0.0179820393 -0.0070950816  0.0410382461  #>         dim51         dim52         dim53         dim54         dim55  #>  0.0193902936  0.0009054039 -0.0824461123  0.0896711960 -0.1071550823  #>         dim56         dim57         dim58         dim59         dim60  #>  0.0090518215 -0.0452947722 -0.0242425101  0.0499748883  0.0017788248  #>         dim61         dim62         dim63         dim64         dim65  #>  0.0896162255  0.0778694536  0.0043599153  0.0777055479  0.0090822422  #>         dim66         dim67         dim68         dim69         dim70  #> -0.0272763870  0.0142941177  0.0595841030 -0.0188500428  0.0720441406  #>         dim71         dim72         dim73         dim74         dim75  #>  0.0662750676  0.0333189751 -0.0275026411  0.0473774977  0.0302065682  #>         dim76         dim77         dim78         dim79         dim80  #>  0.0363555415 -0.0305552844  0.0234168307  0.1017575023  0.0411732703  #>         dim81         dim82         dim83         dim84         dim85  #>  0.0094623327  0.0278133126 -0.0067087575  0.0304040180 -0.0629132792  #>         dim86         dim87         dim88         dim89         dim90  #>  0.0630248067 -0.0340541402  0.0388550926  0.0546983130  0.0218934638  #>         dim91         dim92         dim93         dim94         dim95  #>  0.0433205797 -0.1001378189 -0.0860689768 -0.0450885649  0.0072867091  #>         dim96         dim97         dim98         dim99        dim100  #>  0.0317569448  0.0645638429 -0.0160440819  0.0639956819 -0.0188932674  #>        dim101        dim102        dim103        dim104        dim105  #>  0.0053826412 -0.0191800667 -0.0261438897 -0.1247332173 -0.1010494892  #>        dim106        dim107        dim108        dim109        dim110  #>  0.0587875995 -0.1849463247 -0.0674423763  0.0657176846 -0.0205700926  #>        dim111        dim112        dim113        dim114        dim115  #>  0.0666514408  0.0815598320  0.0569012270 -0.0101953971  0.0484093475  #>        dim116        dim117        dim118        dim119        dim120  #> -0.1185521372  0.0030313936 -0.0933326263  0.0544090501  0.0927743854  #>        dim121        dim122        dim123        dim124        dim125  #>  0.0572259111  0.0394707134 -0.0271925329 -0.0265718034  0.0412028096  #>        dim126        dim127        dim128        dim129        dim130  #>  0.0438836936  0.0418592306  0.0034390046  0.0029262269  0.0134619940  #>        dim131        dim132        dim133        dim134        dim135  #>  0.0060545735  0.0513732397  0.0327279879  0.1054238844  0.0668562286  #>        dim136        dim137        dim138        dim139        dim140  #>  0.0769686804 -0.0227940919 -0.0422079731  0.0703340490 -0.0155774872  #>        dim141        dim142        dim143        dim144        dim145  #>  0.0372660027 -0.1356710009  0.0350093190  0.0236513594  0.0484942080  #>        dim146        dim147        dim148        dim149        dim150  #> -0.0400414708  0.0188626798  0.0274173715 -0.0486294492  0.0038126260  #>        dim151        dim152        dim153        dim154        dim155  #>  0.0133522101  0.0559058596  0.0022117502 -0.0837040985 -0.0849908637  #>        dim156        dim157        dim158        dim159        dim160  #> -0.0533659570 -0.0608346184  0.0319297479  0.0282798544  0.0078445029  #>        dim161        dim162        dim163        dim164        dim165  #> -0.0496067834 -0.0011413436  0.1585030627 -0.0257539005 -0.0556869160  #>        dim166        dim167        dim168        dim169        dim170  #> -0.0376501897  0.0119025195 -0.0722575073 -0.0134277815  0.0713891951  #>        dim171        dim172        dim173        dim174        dim175  #> -0.1004959954  0.0377733481  0.1079291741  0.0487607486  0.0176890786  #>        dim176        dim177        dim178        dim179        dim180  #> -0.0642323537  0.0939240962  0.0323515907  0.0062917470  0.0628913182  #>        dim181        dim182        dim183        dim184        dim185  #> -0.1168540632  0.0042852646 -0.1256198725 -0.0767676420  0.0159300785  #>        dim186        dim187        dim188        dim189        dim190  #>  0.0294246554  0.0809427655  0.0357663118 -0.0218166578  0.0638628914  #>        dim191        dim192        dim193        dim194        dim195  #>  0.0283086395 -0.0002039863 -0.0330815284  0.0067946163  0.0198031884  #>        dim196        dim197        dim198        dim199        dim200  #>  0.0572634491  0.0025753907 -0.0179017809  0.0275942861 -0.0879453025  #>        dim201        dim202        dim203        dim204        dim205  #> -0.0565598458  0.0206358787  0.0228473515 -0.0461282294 -0.0145729556  #>        dim206        dim207        dim208        dim209        dim210  #> -0.0143933569  0.0095678299  0.0631869552  0.0659021305  0.0042216349  #>        dim211        dim212        dim213        dim214        dim215  #>  0.0293649478  0.1033812547 -0.0516775577 -0.1253424596  0.0270994610  #>        dim216        dim217        dim218        dim219        dim220  #>  0.0385621533 -0.0036571112 -0.0278117809  0.0295294855  0.0024058853  #>        dim221        dim222        dim223        dim224        dim225  #> -0.0144733111  0.0180931805  0.0579378103 -0.0116669835  0.0087559466  #>        dim226        dim227        dim228        dim229        dim230  #> -0.0283575865 -0.0278965844 -0.0272467575  0.0342410954  0.0878225024  #>        dim231        dim232        dim233        dim234        dim235  #> -0.0741101358 -0.0684089597 -0.1740935244 -0.0044601583  0.0376029740  #>        dim236        dim237        dim238        dim239        dim240  #>  0.0221138776  0.0207988624 -0.0113099903 -0.0275133395 -0.0000199454  #>        dim241        dim242        dim243        dim244        dim245  #>  0.0563714279 -0.1243700668  0.0584321684  0.0294712221  0.0939427080  #>        dim246        dim247        dim248        dim249        dim250  #> -0.0011363452 -0.0734356129  0.0425710531  0.0715560074 -0.0782001568  #>        dim251        dim252        dim253        dim254        dim255  #>  0.0274794287  0.0326764105  0.0565690520 -0.0124150119  0.0545865281  #>        dim256        dim257        dim258        dim259        dim260  #>  0.0131866584 -0.1333997208 -0.0673208573 -0.0237475804 -0.0439831482  #>        dim261        dim262        dim263        dim264        dim265  #>  0.0048958304  0.0900008028 -0.0692655794  0.0177480897 -0.0185615675  #>        dim266        dim267        dim268        dim269        dim270  #> -0.0438003657  0.0037071559  0.0152207093  0.0955843067  0.1068395259  #>        dim271        dim272        dim273        dim274        dim275  #>  0.0044241142  0.1174178922 -0.0404572738  0.0672502162 -0.0424039690  #>        dim276        dim277        dim278        dim279        dim280  #> -0.1056443456 -0.0766069551  0.0035567801 -0.0748190562  0.0503630702  #>        dim281        dim282        dim283        dim284        dim285  #>  0.0909204843 -0.0668049051  0.0951348101 -0.0171847044  0.0625944510  #>        dim286        dim287        dim288        dim289        dim290  #> -0.0035357800 -0.0205028927 -0.0161002229 -0.0182009416  0.0601725192  #>        dim291        dim292        dim293        dim294        dim295  #> -0.0265177060 -0.0849906153 -0.0761131339 -0.1206830347  0.0004351351  #>        dim296        dim297        dim298        dim299        dim300  #>  0.0062921084 -0.0137075363  0.0623639418  0.0806824072  0.0592501645  #> attr(,\"formula\") #> [1] \"king - man + woman\" #> attr(,\"x.words\") #> [1] \"king\"  \"woman\" \"man\""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tab_similarity.html","id":null,"dir":"Reference","previous_headings":"","what":"Tabulate cosine similarity/distance of word pairs. — tab_similarity","title":"Tabulate cosine similarity/distance of word pairs. — tab_similarity","text":"Tabulate cosine similarity/distance word pairs.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tab_similarity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tabulate cosine similarity/distance of word pairs. — tab_similarity","text":"","code":"tab_similarity(   data,   words = NULL,   pattern = NULL,   words1 = NULL,   words2 = NULL,   unique = FALSE,   distance = FALSE )"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tab_similarity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tabulate cosine similarity/distance of word pairs. — tab_similarity","text":"data wordvec (data.table) embed (matrix), see data_wordvec_load. words [Option 1] Character string(s). pattern [Option 2] Regular expression (see str_subset). neither words pattern specified (.e., NULL), words data extracted. words1, words2 [Option 3] Two sets words n1 * n2 word pairs. See examples. unique Return unique word pairs (TRUE) pairs duplicates (FALSE; default). distance Compute cosine distance instead? Defaults FALSE (cosine similarity).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tab_similarity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tabulate cosine similarity/distance of word pairs. — tab_similarity","text":"data.table words, word pairs, cosine similarity (cos_sim) cosine distance (cos_dist).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tab_similarity.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Tabulate cosine similarity/distance of word pairs. — tab_similarity","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tab_similarity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tabulate cosine similarity/distance of word pairs. — tab_similarity","text":"","code":"tab_similarity(demodata, cc(\"king, queen, man, woman\")) #>     word1 word2    wordpair   cos_sim #>  1:  king  king   king-king 1.0000000 #>  2:  king queen  king-queen 0.6510958 #>  3:  king   man    king-man 0.2294268 #>  4:  king woman  king-woman 0.1284797 #>  5: queen  king  queen-king 0.6510958 #>  6: queen queen queen-queen 1.0000000 #>  7: queen   man   queen-man 0.1665822 #>  8: queen woman queen-woman 0.3161813 #>  9:   man  king    man-king 0.2294268 #> 10:   man queen   man-queen 0.1665822 #> 11:   man   man     man-man 1.0000000 #> 12:   man woman   man-woman 0.7664012 #> 13: woman  king  woman-king 0.1284797 #> 14: woman queen woman-queen 0.3161813 #> 15: woman   man   woman-man 0.7664012 #> 16: woman woman woman-woman 1.0000000 tab_similarity(demodata, cc(\"king, queen, man, woman\"),                unique=TRUE) #>    word1 word2    wordpair   cos_sim #> 1:  king queen  king-queen 0.6510958 #> 2:  king   man    king-man 0.2294268 #> 3:  king woman  king-woman 0.1284797 #> 4: queen   man   queen-man 0.1665822 #> 5: queen woman queen-woman 0.3161813 #> 6:   man woman   man-woman 0.7664012  tab_similarity(demodata, cc(\"Beijing, China, Tokyo, Japan\")) #>       word1   word2        wordpair   cos_sim #>  1: Beijing Beijing Beijing-Beijing 1.0000000 #>  2: Beijing   China   Beijing-China 0.7648461 #>  3: Beijing   Tokyo   Beijing-Tokyo 0.5229628 #>  4: Beijing   Japan   Beijing-Japan 0.3995245 #>  5:   China Beijing   China-Beijing 0.7648461 #>  6:   China   China     China-China 1.0000000 #>  7:   China   Tokyo     China-Tokyo 0.3814305 #>  8:   China   Japan     China-Japan 0.5967756 #>  9:   Tokyo Beijing   Tokyo-Beijing 0.5229628 #> 10:   Tokyo   China     Tokyo-China 0.3814305 #> 11:   Tokyo   Tokyo     Tokyo-Tokyo 1.0000000 #> 12:   Tokyo   Japan     Tokyo-Japan 0.7002254 #> 13:   Japan Beijing   Japan-Beijing 0.3995245 #> 14:   Japan   China     Japan-China 0.5967756 #> 15:   Japan   Tokyo     Japan-Tokyo 0.7002254 #> 16:   Japan   Japan     Japan-Japan 1.0000000 tab_similarity(demodata, cc(\"Beijing, China, Tokyo, Japan\"),                unique=TRUE) #>      word1 word2      wordpair   cos_sim #> 1: Beijing China Beijing-China 0.7648461 #> 2: Beijing Tokyo Beijing-Tokyo 0.5229628 #> 3: Beijing Japan Beijing-Japan 0.3995245 #> 4:   China Tokyo   China-Tokyo 0.3814305 #> 5:   China Japan   China-Japan 0.5967756 #> 6:   Tokyo Japan   Tokyo-Japan 0.7002254  ## only n1 * n2 word pairs across two sets of words tab_similarity(demodata,                words1=cc(\"king, queen, King, Queen\"),                words2=cc(\"man, woman\")) #>    word1 word2    wordpair    cos_sim #> 1:  king   man    king-man 0.22942676 #> 2:  king woman  king-woman 0.12847968 #> 3: queen   man   queen-man 0.16658216 #> 4: queen woman queen-woman 0.31618132 #> 5:  King   man    King-man 0.15777646 #> 6:  King woman  King-woman 0.06369529 #> 7: Queen   man   Queen-man 0.09365463 #> 8: Queen woman Queen-woman 0.20171619"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/test_RND.html","id":null,"dir":"Reference","previous_headings":"","what":"Relative Norm Distance (RND) analysis. — test_RND","title":"Relative Norm Distance (RND) analysis. — test_RND","text":"Tabulate data conduct permutation test significance Relative Norm Distance (RND; also known Relative Euclidean Distance). alternative method Single-Category WEAT.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/test_RND.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Relative Norm Distance (RND) analysis. — test_RND","text":"","code":"test_RND(   data,   T1,   A1,   A2,   use.pattern = FALSE,   labels = list(),   p.perm = TRUE,   p.nsim = 10000,   p.side = 2,   seed = NULL )"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/test_RND.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Relative Norm Distance (RND) analysis. — test_RND","text":"data wordvec (data.table) embed (matrix), see data_wordvec_load. T1 Target words single category (vector words pattern regular expression). A1, A2 Attribute words (vector words pattern regular expression). must specified. use.pattern Defaults FALSE (using vector words). use regular expression T1, T2, A1, A2, please specify argument TRUE. labels Labels target attribute concepts (named list), (default) list(T1=\"Target\", A1=\"Attrib1\", A2=\"Attrib2\"). p.perm Permutation test get exact approximate p value overall effect. Defaults TRUE. See also sweater package. p.nsim Number samples resampling permutation test. Defaults 10000. p.nsim larger number possible permutations (rearrangements data), ignored exact permutation test conducted. Otherwise (cases real data always SC-WEAT), resampling test performed, takes much less computation time produces approximate p value (comparable exact one). p.side One-sided (1) two-sided (2) p value. Defaults 2. Caliskan et al.'s (2017) article, reported one-sided p value WEAT. , suggest reporting two-sided p value conservative estimate. users take full responsibility choice. one-sided p value calculated proportion sampled permutations         difference means greater test statistic. two-sided p value calculated proportion sampled permutations         absolute difference greater test statistic. seed Random seed reproducible results permutation test. Defaults NULL.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/test_RND.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Relative Norm Distance (RND) analysis. — test_RND","text":"list object new class rnd: words.valid Valid (actually matched) words words..found Words found data.raw data.table (absolute relative) norm distances eff.label Description difference two attribute concepts eff.type Effect type: RND eff Raw effect p value (p.perm=TRUE) eff.interpretation Interpretation RND score","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/test_RND.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Relative Norm Distance (RND) analysis. — test_RND","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/test_RND.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Relative Norm Distance (RND) analysis. — test_RND","text":"Garg, N., Schiebinger, L., Jurafsky, D., & Zou, J. (2018). Word embeddings quantify 100 years gender ethnic stereotypes. Proceedings National Academy Sciences, 115(16), E3635--E3644. Bhatia, N., & Bhatia, S. (2021). Changes gender stereotypes time: computational analysis. Psychology Women Quarterly, 45(1), 106--125.","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/test_RND.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Relative Norm Distance (RND) analysis. — test_RND","text":"","code":"rnd = test_RND(   demodata,   labels=list(T1=\"Occupation\", A1=\"Male\", A2=\"Female\"),   T1=cc(\"     architect, boss, leader, engineer, CEO, officer, manager,     lawyer, scientist, doctor, psychologist, investigator,     consultant, programmer, teacher, clerk, counselor,     salesperson, therapist, psychotherapist, nurse\"),   A1=cc(\"male, man, boy, brother, he, him, his, son\"),   A2=cc(\"female, woman, girl, sister, she, her, hers, daughter\"),   seed=1) rnd #>  #> ── Relative Norm Distance (RND) ──────────────────────────────────────────────── #>  #> 21 Occupation (T1) valid words #> 8 Male (A1) valid words #> 8 Female (A2) valid words #>   #> Relative norm distances (differences): #> ───────────────────────────────────────────────────────────── #>                       rnd closer_to norm_dist_A1 norm_dist_A2 #> ───────────────────────────────────────────────────────────── #>  \"architect\"       -0.105      Male        1.138        1.243 #>  \"boss\"            -0.110      Male        1.028        1.138 #>  \"leader\"          -0.102      Male        1.105        1.206 #>  \"engineer\"        -0.093      Male        1.123        1.216 #>  \"CEO\"             -0.065      Male        1.235        1.300 #>  \"officer\"         -0.063      Male        1.098        1.161 #>  \"manager\"         -0.051      Male        1.171        1.222 #>  \"lawyer\"          -0.054      Male        1.048        1.102 #>  \"scientist\"       -0.040      Male        1.135        1.175 #>  \"doctor\"          -0.052      Male        0.965        1.017 #>  \"psychologist\"    -0.036      Male        1.080        1.115 #>  \"investigator\"    -0.035      Male        1.095        1.130 #>  \"consultant\"      -0.029      Male        1.172        1.202 #>  \"programmer\"      -0.027      Male        1.172        1.199 #>  \"teacher\"          0.029    Female        1.028        0.999 #>  \"clerk\"            0.039    Female        1.046        1.007 #>  \"counselor\"        0.025    Female        1.089        1.065 #>  \"salesperson\"      0.040    Female        1.123        1.083 #>  \"therapist\"        0.030    Female        1.059        1.029 #>  \"psychotherapist\"  0.050    Female        1.115        1.066 #>  \"nurse\"            0.125    Female        1.053        0.927 #> ───────────────────────────────────────────────────────────── #> If RND < 0: Occupation is more associated with Male than Female #> If RND > 0: Occupation is more associated with Female than Male #>  #> Overall effect (raw): #> ────────────────────────────────────────── #>      Target       Attrib rnd_sum     p     #> ────────────────────────────────────────── #>  Occupation  Male/Female  -0.523  .076 .   #> ────────────────────────────────────────── #> Permutation test: approximate p value = 7.57e-02 (two-sided) #>"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/test_WEAT.html","id":null,"dir":"Reference","previous_headings":"","what":"Word Embedding Association Test (WEAT) and Single-Category WEAT. — test_WEAT","title":"Word Embedding Association Test (WEAT) and Single-Category WEAT. — test_WEAT","text":"Tabulate data (cosine similarity standardized effect size) conduct permutation test significance Word Embedding Association Test (WEAT) Single-Category Word Embedding Association Test (SC-WEAT). WEAT, two-samples permutation test conducted (.e., rearrangements data). SC-WEAT, one-sample permutation test conducted (.e., rearrangements +/- signs data).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/test_WEAT.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Word Embedding Association Test (WEAT) and Single-Category WEAT. — test_WEAT","text":"","code":"test_WEAT(   data,   T1,   T2,   A1,   A2,   use.pattern = FALSE,   labels = list(),   p.perm = TRUE,   p.nsim = 10000,   p.side = 2,   seed = NULL,   pooled.sd = \"Caliskan\" )"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/test_WEAT.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Word Embedding Association Test (WEAT) and Single-Category WEAT. — test_WEAT","text":"data wordvec (data.table) embed (matrix), see data_wordvec_load. T1, T2 Target words (vector words pattern regular expression). T1 specified, tabulate data single-category WEAT (SC-WEAT). A1, A2 Attribute words (vector words pattern regular expression). must specified. use.pattern Defaults FALSE (using vector words). use regular expression T1, T2, A1, A2, please specify argument TRUE. labels Labels target attribute concepts (named list), (default) list(T1=\"Target1\", T2=\"Target2\", A1=\"Attrib1\", A2=\"Attrib2\"). p.perm Permutation test get exact approximate p value overall effect. Defaults TRUE. See also sweater package. p.nsim Number samples resampling permutation test. Defaults 10000. p.nsim larger number possible permutations (rearrangements data), ignored exact permutation test conducted. Otherwise (cases real data always SC-WEAT), resampling test performed, takes much less computation time produces approximate p value (comparable exact one). p.side One-sided (1) two-sided (2) p value. Defaults 2. Caliskan et al.'s (2017) article, reported one-sided p value WEAT. , suggest reporting two-sided p value conservative estimate. users take full responsibility choice. one-sided p value calculated proportion sampled permutations         difference means greater test statistic. two-sided p value calculated proportion sampled permutations         absolute difference greater test statistic. seed Random seed reproducible results permutation test. Defaults NULL. pooled.sd Method used calculate pooled SD effect size estimate WEAT. Defaults \"Caliskan\": sd(data.diff$cos_sim_diff), highly suggested         identical Caliskan et al.'s (2017) original approach. Otherwise specified, calculate pooled SD :         \\(\\sqrt{[(n_1 - 1) * \\sigma_1^2 + (n_2 - 1) * \\sigma_2^2] / (n_1 + n_2 - 2)}\\).        suggested may overestimate effect size,         especially T1 T2 words small variances.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/test_WEAT.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Word Embedding Association Test (WEAT) and Single-Category WEAT. — test_WEAT","text":"list object new class weat: words.valid Valid (actually matched) words words..found Words found data.raw data.table cosine similarities word pairs data.mean data.table mean cosine similarities     across attribute words data.diff data.table differential mean cosine similarities     two attribute concepts eff.label Description difference two attribute concepts eff.type Effect type: WEAT SC-WEAT eff Raw effect, standardized effect size, p value (p.perm=TRUE)","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/test_WEAT.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Word Embedding Association Test (WEAT) and Single-Category WEAT. — test_WEAT","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/test_WEAT.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Word Embedding Association Test (WEAT) and Single-Category WEAT. — test_WEAT","text":"Caliskan, ., Bryson, J. J., & Narayanan, . (2017). Semantics derived automatically language corpora contain human-like biases. Science, 356(6334), 183--186.","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/test_WEAT.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Word Embedding Association Test (WEAT) and Single-Category WEAT. — test_WEAT","text":"","code":"## Remember: cc() is more convenient than c()!  weat = test_WEAT(   demodata,   labels=list(T1=\"King\", T2=\"Queen\", A1=\"Male\", A2=\"Female\"),   T1=cc(\"king, King\"),   T2=cc(\"queen, Queen\"),   A1=cc(\"male, man, boy, brother, he, him, his, son\"),   A2=cc(\"female, woman, girl, sister, she, her, hers, daughter\"),   seed=1) weat #>  #> ── WEAT (Word Embedding Association Test) ────────────────────────────────────── #>  #> 2 King (T1) valid words #> 2 Queen (T2) valid words #> 8 Male (A1) valid words #> 8 Female (A2) valid words #>   #> Relative semantic similarities (differences): #> ─────────────────────────────────────────────── #>          Target cos_sim_diff std_diff closer_to #> ─────────────────────────────────────────────── #>  \"king\"    King        0.104    1.587      Male #>  \"King\"    King        0.090    1.598      Male #>  \"queen\"  Queen       -0.176   -1.751    Female #>  \"Queen\"  Queen       -0.129   -1.781    Female #> ─────────────────────────────────────────────── #>  #> Mean differences for single target category: #> ───────────────────────────────────────────── #>  Target mean_raw_diff mean_std_diff     p     #> ───────────────────────────────────────────── #>    King         0.097         1.592 <.001 *** #>   Queen        -0.152        -1.766 <.001 *** #> ───────────────────────────────────────────── #> Permutation test: approximate p values (forced to two-sided) #>  #> Overall effect (raw and standardized mean differences): #> ───────────────────────────────────────────────────────── #>      Target       Attrib mean_diff_raw eff_size     p     #> ───────────────────────────────────────────────────────── #>  King/Queen  Male/Female         0.249    1.716 <.001 *** #> ───────────────────────────────────────────────────────── #> Permutation test: exact p value = 0.00e+00 (two-sided) #>   sc_weat = test_WEAT(   demodata,   labels=list(T1=\"Occupation\", A1=\"Male\", A2=\"Female\"),   T1=cc(\"     architect, boss, leader, engineer, CEO, officer, manager,     lawyer, scientist, doctor, psychologist, investigator,     consultant, programmer, teacher, clerk, counselor,     salesperson, therapist, psychotherapist, nurse\"),   A1=cc(\"male, man, boy, brother, he, him, his, son\"),   A2=cc(\"female, woman, girl, sister, she, her, hers, daughter\"),   seed=1) sc_weat #>  #> ── SC-WEAT (Single-Category Word Embedding Association Test) ─────────────────── #>  #> 21 Occupation (T1) valid words #> 8 Male (A1) valid words #> 8 Female (A2) valid words #>   #> Relative semantic similarities (differences): #> ────────────────────────────────────────────────── #>                    cos_sim_diff std_diff closer_to #> ────────────────────────────────────────────────── #>  \"architect\"              0.087    1.048      Male #>  \"boss\"                   0.081    1.027      Male #>  \"leader\"                 0.079    0.985      Male #>  \"engineer\"               0.071    0.775      Male #>  \"CEO\"                    0.045    0.637      Male #>  \"officer\"                0.033    0.448      Male #>  \"manager\"                0.022    0.342      Male #>  \"lawyer\"                 0.020    0.238      Male #>  \"scientist\"              0.008    0.170      Male #>  \"doctor\"                 0.013    0.169      Male #>  \"psychologist\"           0.001    0.030      Male #>  \"investigator\"           0.001    0.012      Male #>  \"consultant\"            -0.003   -0.055    Female #>  \"programmer\"            -0.006   -0.144    Female #>  \"teacher\"               -0.067   -0.800    Female #>  \"clerk\"                 -0.078   -1.035    Female #>  \"counselor\"             -0.064   -1.196    Female #>  \"salesperson\"           -0.083   -1.207    Female #>  \"therapist\"             -0.069   -1.245    Female #>  \"psychotherapist\"       -0.092   -1.323    Female #>  \"nurse\"                 -0.162   -1.491    Female #> ────────────────────────────────────────────────── #>  #> Overall effect (raw and standardized mean differences): #> ───────────────────────────────────────────────────────── #>      Target       Attrib mean_diff_raw eff_size     p     #> ───────────────────────────────────────────────────────── #>  Occupation  Male/Female        -0.008   -0.124  .599     #> ───────────────────────────────────────────────────────── #> Permutation test: approximate p value = 5.99e-01 (two-sided) #>   if (FALSE) {  ## the same as the first example, but using regular expression weat = test_WEAT(   demodata,   labels=list(T1=\"King\", T2=\"Queen\", A1=\"Male\", A2=\"Female\"),   use.pattern=TRUE,  # use regular expression below   T1=\"^[kK]ing$\",   T2=\"^[qQ]ueen$\",   A1=\"^male$|^man$|^boy$|^brother$|^he$|^him$|^his$|^son$\",   A2=\"^female$|^woman$|^girl$|^sister$|^she$|^her$|^hers$|^daughter$\",   seed=1) weat  ## replicating Caliskan et al.'s (2017) results ## WEAT7 (Table 1): d = 1.06, p = .018 ## (requiring installation of the `sweater` package) Caliskan.WEAT7 = test_WEAT(   as_wordvec(sweater::glove_math),   labels=list(T1=\"Math\", T2=\"Arts\", A1=\"Male\", A2=\"Female\"),   T1=cc(\"math, algebra, geometry, calculus, equations, computation, numbers, addition\"),   T2=cc(\"poetry, art, dance, literature, novel, symphony, drama, sculpture\"),   A1=cc(\"male, man, boy, brother, he, him, his, son\"),   A2=cc(\"female, woman, girl, sister, she, her, hers, daughter\"),   p.side=1, seed=1234) Caliskan.WEAT7 # d = 1.055, p = .0173 (= 173 counts / 10000 permutation samples)  ## replicating Caliskan et al.'s (2017) supplemental results ## WEAT7 (Table S1): d = 0.97, p = .027 Caliskan.WEAT7.supp = test_WEAT(   demodata,   labels=list(T1=\"Math\", T2=\"Arts\", A1=\"Male\", A2=\"Female\"),   T1=cc(\"math, algebra, geometry, calculus, equations, computation, numbers, addition\"),   T2=cc(\"poetry, art, dance, literature, novel, symphony, drama, sculpture\"),   A1=cc(\"male, man, boy, brother, he, him, his, son\"),   A2=cc(\"female, woman, girl, sister, she, her, hers, daughter\"),   p.side=1, seed=1234) Caliskan.WEAT7.supp # d = 0.966, p = .0221 (= 221 counts / 10000 permutation samples) }"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/text_init.html","id":null,"dir":"Reference","previous_headings":"","what":"Install required Python modules\nin a new conda environment\nand initialize the environment,\nnecessary for all text_* functions\ndesigned for contextualized word embeddings. — text_init","title":"Install required Python modules\nin a new conda environment\nand initialize the environment,\nnecessary for all text_* functions\ndesigned for contextualized word embeddings. — text_init","text":"Install required Python modules new conda environment initialize environment, necessary text_* functions designed contextualized word embeddings.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/text_init.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Install required Python modules\nin a new conda environment\nand initialize the environment,\nnecessary for all text_* functions\ndesigned for contextualized word embeddings. — text_init","text":"","code":"text_init()"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/text_init.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Install required Python modules\nin a new conda environment\nand initialize the environment,\nnecessary for all text_* functions\ndesigned for contextualized word embeddings. — text_init","text":"Users may first need manually install Anaconda Miniconda. R package text (https://www.r-text.org/) enables users access HuggingFace Transformers models R, R package reticulate interface Python Python modules torch transformers. advanced usage, see text::textrpp_install() text::textrpp_install_virtualenv() text::textrpp_uninstall() text::textrpp_initialize()","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/text_init.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Install required Python modules\nin a new conda environment\nand initialize the environment,\nnecessary for all text_* functions\ndesigned for contextualized word embeddings. — text_init","text":"","code":"if (FALSE) { text_init()  # You may need to specify the version of Python: # RStudio -> Tools -> Global/Project Options # -> Python -> Select -> Conda Environments # -> Choose \".../textrpp_condaenv/python.exe\" }"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/text_model_download.html","id":null,"dir":"Reference","previous_headings":"","what":"Download pre-trained language models from HuggingFace. — text_model_download","title":"Download pre-trained language models from HuggingFace. — text_model_download","text":"Download pre-trained language models (Transformers Models, GPT, BERT, RoBERTa, DeBERTa, DistilBERT, etc.) HuggingFace local \".cache\" folder (\"C:/Users/[YourUserName]/.cache/\"). models never removed unless run text_model_remove.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/text_model_download.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download pre-trained language models from HuggingFace. — text_model_download","text":"","code":"text_model_download(model = NULL, save.config = TRUE)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/text_model_download.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download pre-trained language models from HuggingFace. — text_model_download","text":"model Character string(s) specifying pre-trained language model(s) downloaded. full list options, see HuggingFace. Defaults download nothing check currently downloaded models. Example choices: \"gpt2\" (50257 vocab, 768 dims, 12 layers) \"openai-gpt\" (40478 vocab, 768 dims, 12 layers) \"bert-base-uncased\" (30522 vocab, 768 dims, 12 layers) \"bert-large-uncased\" (30522 vocab, 1024 dims, 24 layers) \"bert-base-cased\" (28996 vocab, 768 dims, 12 layers) \"bert-large-cased\" (28996 vocab, 1024 dims, 24 layers) \"bert-base-chinese\" (21128 vocab, 768 dims, 12 layers) \"bert-base-multilingual-cased\" (119547 vocab, 768 dims, 12 layers) \"distilbert-base-uncased\" (30522 vocab, 768 dims, 6 layers) \"distilbert-base-cased\" (28996 vocab, 768 dims, 6 layers) \"distilbert-base-multilingual-cased\" (119547 vocab, 768 dims, 6 layers) \"albert-base-v2\" (30000 vocab, 768 dims, 12 layers) \"albert-large-v2\" (30000 vocab, 1024 dims, 24 layers) \"roberta-base\" (50265 vocab, 768 dims, 12 layers) \"roberta-large\" (50265 vocab, 1024 dims, 24 layers) \"xlm-roberta-base\" (250002 vocab, 768 dims, 12 layers) \"xlm-roberta-large\" (250002 vocab, 1024 dims, 24 layers) \"xlnet-base-cased\" (32000 vocab, 768 dims, 12 layers) \"xlnet-large-cased\" (32000 vocab, 1024 dims, 24 layers) \"microsoft/deberta-v3-base\" (128100 vocab, 768 dims, 12 layers) \"microsoft/deberta-v3-large\" (128100 vocab, 1024 dims, 24 layers) ... (see https://huggingface.co/models) save.config Save configuration file model current path. Defaults TRUE.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/text_model_download.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download pre-trained language models from HuggingFace. — text_model_download","text":"Invisibly return names downloaded models.","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/text_model_download.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download pre-trained language models from HuggingFace. — text_model_download","text":"","code":"if (FALSE) { # text_init()  # initialize the environment  text_model_download()  # check downloaded models text_model_download(c(   \"bert-base-uncased\",   \"bert-base-cased\",   \"bert-base-multilingual-cased\" )) }"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/text_model_remove.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove downloaded models from the local .cache folder. — text_model_remove","title":"Remove downloaded models from the local .cache folder. — text_model_remove","text":"Remove downloaded models local .cache folder.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/text_model_remove.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove downloaded models from the local .cache folder. — text_model_remove","text":"","code":"text_model_remove(model = NULL)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/text_model_remove.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove downloaded models from the local .cache folder. — text_model_remove","text":"model Model name. See text_model_download. Defaults automatically find downloaded models .cache folder.","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/text_model_remove.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remove downloaded models from the local .cache folder. — text_model_remove","text":"","code":"if (FALSE) { # text_init()  # initialize the environment  text_model_remove() }"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/text_to_vec.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract contextualized word embeddings from transformers (pre-trained language models). — text_to_vec","title":"Extract contextualized word embeddings from transformers (pre-trained language models). — text_to_vec","text":"Extract hidden layers language model aggregate get token (roughly word) embeddings text embeddings (reshaped embed matrix). wrapper function text::textEmbed().","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/text_to_vec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract contextualized word embeddings from transformers (pre-trained language models). — text_to_vec","text":"","code":"text_to_vec(   text,   model,   layers = \"all\",   layer.to.token = \"concatenate\",   token.to.word = TRUE,   token.to.text = TRUE,   encoding = \"UTF-8\",   ... )"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/text_to_vec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract contextualized word embeddings from transformers (pre-trained language models). — text_to_vec","text":"text Can : character string vector text (usually sentences) data frame least one character variable   (text character variables given data frame) file path disk containing text model Model name HuggingFace. See text_model_download. model downloaded, automatically download model. layers Layers extracted model, aggregated function text::textEmbedLayerAggregation(). Defaults \"\" extracts layers. may extract layers need (e.g., 11:12). Note layer 0 decontextualized input layer (.e., comprising hidden states). layer..token Method aggregate hidden layers token. Defaults \"concatenate\", links together word embedding layer one long row. Options include \"mean\", \"min\", \"max\", \"concatenate\". token..word Aggregate subword token embeddings (whole word vocabulary) whole word embeddings. Defaults TRUE, sums subword token embeddings. token..text Aggregate token embeddings text. Defaults TRUE, averages token embeddings. FALSE, text embedding token embedding [CLS] (special token used represent beginning text sequence). encoding Text encoding (used text file). Defaults \"UTF-8\". ... parameters passed text::textEmbed().","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/text_to_vec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract contextualized word embeddings from transformers (pre-trained language models). — text_to_vec","text":"list : token.embed Token (roughly word) embeddings text.embed Text embeddings, aggregated token embeddings","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/text_to_vec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract contextualized word embeddings from transformers (pre-trained language models). — text_to_vec","text":"","code":"if (FALSE) { # text_init()  # initialize the environment  text = c(\"Download models from HuggingFace\",          \"Chinese are East Asian\",          \"Beijing is the capital of China\") embed = text_to_vec(text, model=\"bert-base-cased\", layers=c(0, 12)) embed  embed1 = embed$token.embed[[1]] embed2 = embed$token.embed[[2]] embed3 = embed$token.embed[[3]]  View(embed1) View(embed2) View(embed3) View(embed$text.embed)  plot_similarity(embed1, value.color=\"grey\") plot_similarity(embed2, value.color=\"grey\") plot_similarity(embed3, value.color=\"grey\") plot_similarity(rbind(embed1, embed2, embed3)) }"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/text_unmask.html","id":null,"dir":"Reference","previous_headings":"","what":"Fill in the blank mask(s) in a query (sentence). — text_unmask","title":"Fill in the blank mask(s) in a query (sentence). — text_unmask","text":"Predict probably correct masked token(s) sequence, based Python module transformers.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/text_unmask.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fill in the blank mask(s) in a query (sentence). — text_unmask","text":"","code":"text_unmask(query, model, topn = 5)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/text_unmask.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fill in the blank mask(s) in a query (sentence). — text_unmask","text":"query query (sentence/prompt) masked token(s) [MASK]. See examples. model Model name HuggingFace. See text_model_download. model downloaded, automatically download model. topn Number predictions return. Defaults 5.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/text_unmask.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fill in the blank mask(s) in a query (sentence). — text_unmask","text":"data.table query results: mask_id [MASK] ID (position sequence, indicating multiple masks) prob Probability predicted token sequence token_id Predicted token ID (replace [MASK]) token Predicted token (replace [MASK]) sequence Complete sentence predicted token","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/text_unmask.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fill in the blank mask(s) in a query (sentence). — text_unmask","text":"Masked language modeling task masking words sentence predicting words replace masks. models useful want get statistical understanding language model trained . See https://huggingface.co/tasks/fill-mask details.","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/text_unmask.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fill in the blank mask(s) in a query (sentence). — text_unmask","text":"","code":"if (FALSE) { # text_init()  # initialize the environment  model = \"distilbert-base-cased\" text_unmask(\"Beijing is the [MASK] of China.\", model) text_unmask(\"Beijing is the [MASK] [MASK] of China.\", model) text_unmask(\"The man worked as a [MASK].\", model) text_unmask(\"The woman worked as a [MASK].\", model) }"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tokenize.html","id":null,"dir":"Reference","previous_headings":"","what":"Tokenize raw text for training word embeddings. — tokenize","title":"Tokenize raw text for training word embeddings. — tokenize","text":"Tokenize raw text training word embeddings.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tokenize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tokenize raw text for training word embeddings. — tokenize","text":"","code":"tokenize(   text,   tokenizer = text2vec::word_tokenizer,   split = \" \",   remove = \"_|'|<br/>|<br />|e\\\\.g\\\\.|i\\\\.e\\\\.\",   encoding = \"UTF-8\",   simplify = TRUE,   verbose = TRUE )"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tokenize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tokenize raw text for training word embeddings. — tokenize","text":"text character vector text, file path disk containing text. tokenizer Function used tokenize text. Defaults text2vec::word_tokenizer. split Separator tokens, used simplify=TRUE. Defaults \" \". remove Strings (regular expression) removed text. Defaults \"_|'|<br/>|<br />|e\\\\.g\\\\.|\\\\.e\\\\.\". may turn specifying remove=NULL. encoding Text encoding (used text file). Defaults \"UTF-8\". simplify Return character vector (TRUE) list character vectors (FALSE). Defaults TRUE. verbose Print information console? Defaults TRUE.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tokenize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tokenize raw text for training word embeddings. — tokenize","text":"simplify=TRUE: tokenized character vector,   element sentence. simplify=FALSE: list tokenized character vectors,   element vector tokens sentence.","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tokenize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tokenize raw text for training word embeddings. — tokenize","text":"","code":"txt1 = c(   \"I love natural language processing (NLP)!\",   \"I've been in this city for 10 years. I really like here!\",   \"However, my computer is not among the \\\"Top 10\\\" list.\" ) tokenize(txt1, simplify=FALSE) #> ✔ Tokenized: 4 sentences (time cost = 0.4 secs) #> [[1]] #> [1] \"I\"          \"love\"       \"natural\"    \"language\"   \"processing\" #> [6] \"NLP\"        #>  #> [[2]] #> [1] \"Ive\"   \"been\"  \"in\"    \"this\"  \"city\"  \"for\"   \"10\"    \"years\" #>  #> [[3]] #> [1] \"I\"      \"really\" \"like\"   \"here\"   #>  #> [[4]] #>  [1] \"However\"  \"my\"       \"computer\" \"is\"       \"not\"      \"among\"    #>  [7] \"the\"      \"Top\"      \"10\"       \"list\"     #>  tokenize(txt1) %>% cat(sep=\"\\n----\\n\") #> ✔ Tokenized: 4 sentences (time cost = 0.3 secs) #> I love natural language processing NLP #> ---- #> Ive been in this city for 10 years #> ---- #> I really like here #> ---- #> However my computer is not among the Top 10 list  txt2 = text2vec::movie_review$review[1:5] texts = tokenize(txt2) #> ✔ Tokenized: 81 sentences (time cost = 0.3 secs)  txt2[1] #> [1] \"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\" texts[1:20]  # all sentences in txt2[1] #>  [1] \"With all this stuff going down at the moment with MJ ive started listening to his music watching the odd documentary here and there watched The Wiz and watched Moonwalker again\"                                       #>  [2] \"Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent\"                                                  #>  [3] \"Moonwalker is part biography part feature film which i remember going to see at the cinema when it was originally released\"                                                                                             #>  [4] \"Some of it has subtle messages about MJs feeling towards the press and also the obvious message of drugs are bad mkay\"                                                                                                  #>  [5] \"Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring\"                                                        #>  [6] \"Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him\"                                          #>  [7] \"The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord\"                         #>  [8] \"Why he wants MJ dead so bad is beyond me\"                                                                                                                                                                               #>  [9] \"Because MJ overheard his plans\"                                                                                                                                                                                         #> [10] \"Nah Joe Pescis character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno maybe he just hates MJs music\"                                                                             #> [11] \"Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence\"                                                                                                                  #> [12] \"Also the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene\" #> [13] \"Bottom line this movie is for people who like MJ on one level or another which i think is most people\"                                                                                                                  #> [14] \"If not then stay away\"                                                                                                                                                                                                  #> [15] \"It does try and give off a wholesome message and ironically MJs bestest buddy in this movie is a girl\"                                                                                                                  #> [16] \"Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty\"                                                                                                                    #> [17] \"Well with all the attention ive gave this subject\"                                                                                                                                                                      #> [18] \"hmmm well i dont know because people can be different behind closed doors i know this for a fact\"                                                                                                                       #> [19] \"He is either an extremely nice but stupid guy or one of the most sickest liars\"                                                                                                                                         #> [20] \"I hope he is not the latter\""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/train_wordvec.html","id":null,"dir":"Reference","previous_headings":"","what":"Train static word embeddings using the Word2Vec, GloVe, or FastText algorithm. — train_wordvec","title":"Train static word embeddings using the Word2Vec, GloVe, or FastText algorithm. — train_wordvec","text":"Train static word embeddings using Word2Vec, GloVe, FastText algorithm multi-threading.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/train_wordvec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Train static word embeddings using the Word2Vec, GloVe, or FastText algorithm. — train_wordvec","text":"","code":"train_wordvec(   text,   method = c(\"word2vec\", \"glove\", \"fasttext\"),   dims = 300,   window = 5,   min.freq = 5,   threads = 8,   model = c(\"skip-gram\", \"cbow\"),   loss = c(\"ns\", \"hs\"),   negative = 5,   subsample = 1e-04,   learning = 0.05,   ngrams = c(3, 6),   x.max = 10,   convergence = -1,   stopwords = character(0),   encoding = \"UTF-8\",   tolower = FALSE,   normalize = FALSE,   iteration,   tokenizer,   remove,   file.save,   compress = \"bzip2\",   verbose = TRUE )"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/train_wordvec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Train static word embeddings using the Word2Vec, GloVe, or FastText algorithm. — train_wordvec","text":"text character vector text, file path disk containing text. method Training algorithm: \"word2vec\" (default):   using word2vec package \"glove\":   using rsparse   text2vec packages \"fasttext\":   using fastTextR package dims Number dimensions word vectors trained. Common choices include 50, 100, 200, 300, 500. Defaults 300. window Window size (number nearby words behind/ahead current word). defines many surrounding words included training: [window] words behind [window] words ahead ([window]*2 total). Defaults 5. min.freq Minimum frequency words included training. Words appear less value times excluded vocabulary. Defaults 5 (take words appear least five times). threads Number CPU threads used training. modest value produces fastest training. many threads always helpful. Defaults 8. model <Word2Vec / FastText> Learning model architecture: \"skip-gram\" (default): Skip-Gram,   predicts surrounding words given current word \"cbow\": Continuous Bag--Words,   predicts current word based context loss <Word2Vec / FastText> Loss function (computationally efficient approximation): \"ns\" (default): Negative Sampling \"hs\": Hierarchical Softmax negative <Negative Sampling Word2Vec / FastText> Number negative examples. Values range 5~20 useful small training datasets, large datasets value can small 2~5. Defaults 5. subsample <Word2Vec / FastText> Subsampling frequent words (threshold occurrence words). appear higher frequency training data randomly -sampled. Defaults 0.0001 (1e-04). learning <Word2Vec / FastText> Initial (starting) learning rate, also known alpha. Defaults 0.05. ngrams <FastText> Minimal maximal ngram length. Defaults c(3, 6). x.max <GloVe> Maximum number co-occurrences use weighting function. Defaults 10. convergence <GloVe> Convergence tolerance SGD iterations. Defaults -1. stopwords <Word2Vec / GloVe> character vector stopwords excluded training. encoding Text encoding. Defaults \"UTF-8\". tolower Convert upper-case characters lower-case? Defaults FALSE. normalize Normalize word vectors unit length? Defaults FALSE. See normalize. iteration Number training iterations. iterations makes precise model, computational cost linearly proportional iterations. Defaults 5 Word2Vec FastText 10 GloVe. tokenizer Function used tokenize text. Defaults text2vec::word_tokenizer. remove Strings (regular expression) removed text. Defaults \"_|'|<br/>|<br />|e\\\\.g\\\\.|\\\\.e\\\\.\". may turn specifying remove=NULL. file.save File name --saved R data (must .RData). compress Compression method saved file. Defaults \"bzip2\". Options include: 1 \"gzip\": modest file size (fastest) 2 \"bzip2\": small file size (fast) 3 \"xz\": minimized file size (slow) verbose Print information console? Defaults TRUE.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/train_wordvec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Train static word embeddings using the Word2Vec, GloVe, or FastText algorithm. — train_wordvec","text":"wordvec (data.table) three variables: word, vec, freq.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/train_wordvec.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Train static word embeddings using the Word2Vec, GloVe, or FastText algorithm. — train_wordvec","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/train_wordvec.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Train static word embeddings using the Word2Vec, GloVe, or FastText algorithm. — train_wordvec","text":"--one package: https://CRAN.R-project.org/package=wordsalad Word2Vec: https://code.google.com/archive/p/word2vec/ https://CRAN.R-project.org/package=word2vec https://github.com/maxoodf/word2vec GloVe: https://nlp.stanford.edu/projects/glove/ https://text2vec.org/glove.html https://CRAN.R-project.org/package=text2vec https://CRAN.R-project.org/package=rsparse FastText: https://fasttext.cc/ https://CRAN.R-project.org/package=fastTextR","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/train_wordvec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Train static word embeddings using the Word2Vec, GloVe, or FastText algorithm. — train_wordvec","text":"","code":"review = text2vec::movie_review  # a data.frame' text = review$review  ## Note: All the examples train 50 dims for faster code check.  ## Word2Vec (SGNS) dt1 = train_wordvec(   text,   method=\"word2vec\",   model=\"skip-gram\",   dims=50, window=5,   normalize=TRUE) #> ✔ Tokenized: 70105 sentences (time cost = 3 secs) #> ✔ Text corpus: 5242249 characters, 1185427 tokens (roughly words) #>  #> ── Training model information ────────────────────────────────────────────────── #> - Method:      Word2Vec (Skip-Gram with Negative Sampling) #> - Dimensions:  50 #> - Window size: 5 (5 words behind and 5 words ahead the current word) #> - Subsampling: 1e-04 #> - Min. freq.:  5 occurrences in text #> - Iterations:  5 training iterations #> - CPU threads: 8 #>  #> ── Training...  #> ✔ Word vectors trained: 14205 unique tokens (time cost = 11 secs)  dt1 #> # wordvec (data.table): 14205 × 3 (normalized) #>                 word                     vec  freq #>     1:           the [ 0.0962, ...<50 dims>] 58797 #>     2:           and [ 0.0713, ...<50 dims>] 32193 #>     3:             a [-0.0879, ...<50 dims>] 31783 #>     4:            of [ 0.0412, ...<50 dims>] 29142 #>     5:            to [ 0.0362, ...<50 dims>] 27218 #> ------                                             #> 14201:        parrot [ 0.2309, ...<50 dims>]     5 #> 14202:          Lori [ 0.0081, ...<50 dims>]     5 #> 14203:      shambles [ 0.1283, ...<50 dims>]     5 #> 14204: comprehension [ 0.1932, ...<50 dims>]     5 #> 14205:        drunks [ 0.1852, ...<50 dims>]     5 most_similar(dt1, \"Ive\")  # evaluate performance #> [Word Vector] =~ Ive #> (normalized to unit length) #>           word   cos_sim row_id #>  1:        ive 0.8504476    110 #>  2:       seen 0.8212163    124 #>  3:     lately 0.8008251   1542 #>  4:   lifetime 0.7818848   3473 #>  5:      WORST 0.7794276   3487 #>  6:   funniest 0.7707824   5137 #>  7:      Hands 0.7630291   5989 #>  8:      Youve 0.7618677   6904 #>  9: criticized 0.7599927   7224 #> 10:       ever 0.7587857   8457 most_similar(dt1, ~ man - he + she, topn=5)  # evaluate performance #> [Word Vector] =~ man - he + she #> (normalized to unit length) #>       word   cos_sim row_id #> 1:   woman 0.8375592    260 #> 2:    lady 0.7960672    299 #> 3:      ex 0.7522800   1140 #> 4:    girl 0.7513236   1354 #> 5: widowed 0.7404475   6752 most_similar(dt1, ~ boy - he + she, topn=5)  # evaluate performance #> [Word Vector] =~ boy - he + she #> (normalized to unit length) #>     word   cos_sim row_id #> 1:  girl 0.8314146    260 #> 2: woman 0.7838329    299 #> 3:  lady 0.7637334   1140 #> 4:  aged 0.7287399   1839 #> 5:   mom 0.7076608   1903  ## GloVe dt2 = train_wordvec(   text,   method=\"glove\",   dims=50, window=5,   normalize=TRUE) #> ✔ Tokenized: 70105 sentences (time cost = 3 secs) #> ✔ Text corpus: 5242249 characters, 1185427 tokens (roughly words) #>  #> ── Training model information ────────────────────────────────────────────────── #> - Method:      GloVe #> - Dimensions:  50 #> - Window size: 5 (5 words behind and 5 words ahead the current word) #> - Subsampling: N/A #> - Min. freq.:  5 occurrences in text #> - Iterations:  10 training iterations #> - CPU threads: 8 #>  #> ── Training...  #> ✔ Word vectors trained: 14207 unique tokens (time cost = 13 secs)  dt2 #> # wordvec (data.table): 14207 × 3 (normalized) #>            word                     vec  freq #>     1:      the [-0.0108, ...<50 dims>] 58797 #>     2:      and [ 0.0208, ...<50 dims>] 32193 #>     3:        a [-0.0603, ...<50 dims>] 31783 #>     4:       of [-0.0312, ...<50 dims>] 29142 #>     5:       to [-0.0585, ...<50 dims>] 27218 #> ------                                        #> 14203:      yea [ 0.1901, ...<50 dims>]     5 #> 14204:   yearly [-0.0160, ...<50 dims>]     5 #> 14205: yearning [ 0.0093, ...<50 dims>]     5 #> 14206:   yelled [ 0.3532, ...<50 dims>]     5 #> 14207:      yer [-0.0874, ...<50 dims>]     5 most_similar(dt2, \"Ive\")  # evaluate performance #> [Word Vector] =~ Ive #> (normalized to unit length) #>        word   cos_sim row_id #>  1:    seen 0.9436693     74 #>  2:    ever 0.8941794     91 #>  3:   heard 0.7567965    110 #>  4:   worst 0.7558227    124 #>  5:   since 0.7396439    261 #>  6:   youve 0.7203209    262 #>  7:  havent 0.7026932    305 #>  8: watched 0.7006075    515 #>  9:  movies 0.6916209    767 #> 10:    been 0.6792962    950 most_similar(dt2, ~ man - he + she, topn=5)  # evaluate performance #> [Word Vector] =~ man - he + she #> (normalized to unit length) #>     word   cos_sim row_id #> 1: woman 0.8583957    198 #> 2: young 0.7673556    260 #> 3:  girl 0.7496135    299 #> 4:   hit 0.7388676    478 #> 5:   boy 0.7270141    594 most_similar(dt2, ~ boy - he + she, topn=5)  # evaluate performance #> [Word Vector] =~ boy - he + she #> (normalized to unit length) #>     word   cos_sim row_id #> 1:  girl 0.8003291    153 #> 2: young 0.7725992    198 #> 3: woman 0.7321471    260 #> 4: named 0.7148128    299 #> 5:   man 0.7127950    867  ## FastText dt3 = train_wordvec(   text,   method=\"fasttext\",   model=\"skip-gram\",   dims=50, window=5,   normalize=TRUE) #> ✔ Tokenized: 70105 sentences (time cost = 3 secs) #> ✔ Text corpus: 5242249 characters, 1185427 tokens (roughly words) #>  #> ── Training model information ────────────────────────────────────────────────── #> - Method:      FastText (Skip-Gram with Negative Sampling) #> - Dimensions:  50 #> - Window size: 5 (5 words behind and 5 words ahead the current word) #> - Subsampling: 1e-04 #> - Min. freq.:  5 occurrences in text #> - Iterations:  5 training iterations #> - CPU threads: 8 #>  #> ── Training...  #> ✔ Word vectors trained: 14207 unique tokens (time cost = 25 secs)  dt3 #> # wordvec (data.table): 14207 × 3 (normalized) #>                word                     vec  freq #>     1:          the [-0.0383, ...<50 dims>] 58797 #>     2:          and [ 0.0860, ...<50 dims>] 32193 #>     3:            a [ 0.0024, ...<50 dims>] 31783 #>     4:           of [ 0.0300, ...<50 dims>] 29142 #>     5:           to [-0.0522, ...<50 dims>] 27218 #> ------                                            #> 14203:        spray [ 0.0512, ...<50 dims>]     5 #> 14204: disabilities [ 0.0585, ...<50 dims>]     5 #> 14205:        crook [ 0.0957, ...<50 dims>]     5 #> 14206:     Syndrome [ 0.0684, ...<50 dims>]     5 #> 14207:      snipers [ 0.0362, ...<50 dims>]     5 most_similar(dt3, \"Ive\")  # evaluate performance #> [Word Vector] =~ Ive #> (normalized to unit length) #>           word   cos_sim row_id #>  1:      Youve 0.8403307    110 #>  2:       seen 0.8187520    765 #>  3:       Weve 0.8112237    945 #>  4:      youve 0.7953780   3105 #>  5:         ve 0.7853087   3494 #>  6:      WORST 0.7654500   5898 #>  7:        ive 0.7607501   6913 #>  8: beforehand 0.7461706   7108 #>  9:     havent 0.7440477   9171 #> 10:    funnier 0.7367521  12894 most_similar(dt3, ~ man - he + she, topn=5)  # evaluate performance #> [Word Vector] =~ man - he + she #> (normalized to unit length) #>        word   cos_sim row_id #> 1:    woman 0.8737580    261 #> 2: salesman 0.7749060    299 #> 3:   madman 0.7637398   5594 #> 4:     girl 0.7518185   6553 #> 5: henchman 0.7490453  12263 most_similar(dt3, ~ boy - he + she, topn=5)  # evaluate performance #> [Word Vector] =~ boy - he + she #> (normalized to unit length) #>     word   cos_sim row_id #> 1:  girl 0.7969428    261 #> 2: woman 0.7623809    299 #> 3:   kid 0.7081417    676 #> 4:  boys 0.6980761   1045 #> 5: widow 0.6750687   5504"},{"path":"https://psychbruce.github.io/PsychWordVec/news/index.html","id":"psychwordvec-021-ongoing","dir":"Changelog","previous_headings":"","what":"PsychWordVec 0.2.1 (ongoing…)","title":"PsychWordVec 0.2.1 (ongoing…)","text":"⚠️ users update package version ≥ 0.2.1. Old versions (≤ 0.2.0) may slow running speed old functions deprecated.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/news/index.html","id":"major-changes-0-2-1","dir":"Changelog","previous_headings":"","what":"Major Changes","title":"PsychWordVec 0.2.1 (ongoing…)","text":"functions now substantially enhanced faster speed, especially tab_similarity(), most_similar(), dict_expand(), dict_reliability(), test_WEAT(), test_RND(). pair_similarity() improved using matrix operation tcrossprod(embed, embed) compute cosine similarity, embed normalized. data_wordvec_load() got two wrapper functions load_wordvec() load_embed() faster use. data_wordvec_normalize() (deprecated) renamed normalize(). get_wordvecs() (deprecated) integrated get_wordvec(). tab_similarity_cross() (deprecated) integrated tab_similarity().","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/news/index.html","id":"bug-fixes-0-2-1","dir":"Changelog","previous_headings":"","what":"Bug Fixes","title":"PsychWordVec 0.2.1 (ongoing…)","text":"Fixed issue unexpected long loading processing time 0.2.0, related duplicate words .RData, many words embed wordvec, many words printed console. Now related functions substantially improved take unnecessarily long time.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/news/index.html","id":"psychwordvec-020-dec-2022","dir":"Changelog","previous_headings":"","what":"PsychWordVec 0.2.0 (Dec 2022)","title":"PsychWordVec 0.2.0 (Dec 2022)","text":"CRAN release: 2022-12-01","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/news/index.html","id":"breaking-news-0-2-0","dir":"Changelog","previous_headings":"","what":"Breaking News","title":"PsychWordVec 0.2.0 (Dec 2022)","text":"functions now internally use embed (extended class matrix) rather wordvec order enhance speed! text_init(): set Python environment PLM text_model_download(): download PLMs HuggingFace local “.cache” folder text_model_remove(): remove PLMs local “.cache” folder text_to_vec(): extract contextualized token text embeddings text_unmask(): fill blank mask(s) query New orth_procrustes() function: Orthogonal Procrustes matrix alignment. Users can input either two matrices word embeddings two wordvec objects loaded data_wordvec_load() transformed matrices as_wordvec(). New dict_expand() function: Expand dictionary similar words, based most_similar(). New dict_reliability() function: Reliability analysis (Cronbach’s α) Principal Component Analysis (PCA) dictionary. Note Cronbach’s α may misleading number items/words large.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/news/index.html","id":"new-features-0-2-0","dir":"Changelog","previous_headings":"","what":"New Features","title":"PsychWordVec 0.2.0 (Dec 2022)","text":"New sum_wordvec() function: Calculate sum vector multiple words. New plot_similarity() function: Visualize cosine similarities word pairs style correlation matrix plot. New tab_similarity_cross() function: wrapper tab_similarity() tabulate cosine similarities n1 * n2 word pairs two sets words (arguments: words1, words2). New S3 methods: print.wordvec(), print.embed(), rbind.wordvec(), rbind.embed(), subset.wordvec(), subset.embed()","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/news/index.html","id":"major-changes-0-2-0","dir":"Changelog","previous_headings":"","what":"Major Changes","title":"PsychWordVec 0.2.0 (Dec 2022)","text":"as_matrix() renamed as_embed(): Now PsychWordVec supports two classes data objects – wordvec (data.table) embed (matrix). functions now use embed (transform wordvec embed) internally enhance speed. Matrix much faster! Deprecated data_wordvec_reshape(): Now use as_wordvec() as_embed().","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/news/index.html","id":"minor-changes-0-2-0","dir":"Changelog","previous_headings":"","what":"Minor Changes","title":"PsychWordVec 0.2.0 (Dec 2022)","text":"Defaults changed data_wordvec_subset(), get_wordvecs(), tab_similarity(), plot_similarity(): neither words pattern specified (NULL), words data extracted. Improved S3 methods print.weat() print.rnd().","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/news/index.html","id":"psychwordvec-012-nov-2022","dir":"Changelog","previous_headings":"","what":"PsychWordVec 0.1.2 (Nov 2022)","title":"PsychWordVec 0.1.2 (Nov 2022)","text":"CRAN release: 2022-11-03","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/news/index.html","id":"new-features-0-1-2","dir":"Changelog","previous_headings":"","what":"New Features","title":"PsychWordVec 0.1.2 (Nov 2022)","text":"Added permutation test significance test_WEAT() test_RND(): Users can specify number permutation samples choose calculate either one-sided two-sided p value. can well reproduce results Caliskan et al.’s (2017) article. Added pooled.sd argument test_WEAT(): Users can choose method used calculate pooled SD effect size estimate WEAT. However, original approach proposed Caliskan et al. (2017) default highly suggested. Wrapper functions as_matrix() as_wordvec() data_wordvec_reshape(), can make easier reshape word embeddings data matrix “wordvec” data.table vice versa.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/news/index.html","id":"major-changes-0-1-2","dir":"Changelog","previous_headings":"","what":"Major Changes","title":"PsychWordVec 0.1.2 (Nov 2022)","text":"test_WEAT() test_RND() now changed element names S3 print method returned objects (new class weat rnd, respectively): elements $eff.raw, $eff.size, $eff.sum now deprecated replaced $eff, data.table containing overall raw/standardized effects permutation p value. new S3 print methods print.weat() print.rnd() can make tidy report test results directly type print returned object (see code examples). Improved command line interfaces using cli package. Improved welcome messages library(PsychWordVec).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/news/index.html","id":"psychwordvec-010-aug-2022","dir":"Changelog","previous_headings":"","what":"PsychWordVec 0.1.0 (Aug 2022)","title":"PsychWordVec 0.1.0 (Aug 2022)","text":"CRAN release: 2022-08-22 CRAN initial release. Fixed issues CRAN manual inspection.","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/news/index.html","id":"new-features-0-0-8","dir":"Changelog","previous_headings":"","what":"New Features","title":"PsychWordVec 0.0.8 (Aug 2022)","text":"Added wordvec primary class word vectors data: Now data classes contain wordvec, data.table, data.frame, actually perform data.table. New train_wordvec() function: Train word vectors using Word2Vec, GloVe, FastText algorithm multi-threading. New tokenize() function: Tokenize raw texts training word vectors. New data_wordvec_reshape() function: Reshape word vectors data dense (data.table new classs wordvec two variables word vec) plain (matrix word vectors) vice versa. New test_RND() function, tab_WEAT() renamed test_WEAT(): two functions serve convenient tools word semantic similarity analysis conceptual association test. New plot_wordvec_tSNE() function: Visualize 2-D 3-D word vectors dimensionality reduced using t-Distributed Stochastic Neighbor Embedding (t-SNE) method.","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/news/index.html","id":"new-features-0-0-6","dir":"Changelog","previous_headings":"","what":"New Features","title":"PsychWordVec 0.0.6 (Jul 2022)","text":"Enhanced functions. New data_wordvec_subset() function. Added unique argument tab_similarity(). Added support use regular expression pattern test_WEAT().","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/news/index.html","id":"psychwordvec-004-apr-2022","dir":"Changelog","previous_headings":"","what":"PsychWordVec 0.0.4 (Apr 2022)","title":"PsychWordVec 0.0.4 (Apr 2022)","text":"Initial public release GitHub functions.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/news/index.html","id":"psychwordvec-001-mar-2022","dir":"Changelog","previous_headings":"","what":"PsychWordVec 0.0.1 (Mar 2022)","title":"PsychWordVec 0.0.1 (Mar 2022)","text":"Basic functions WordVector_RData.pdf file.","code":""}]
