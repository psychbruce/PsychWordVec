[{"path":"https://psychbruce.github.io/PsychWordVec/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Han-Wu-Shuang Bao. Author, maintainer.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Bao H (2022). PsychWordVec: Word Embedding Research Framework Psychological Science. R package version 0.1.4, https://psychbruce.github.io/PsychWordVec/.","code":"@Manual{,   title = {PsychWordVec: Word Embedding Research Framework for Psychological Science},   author = {Han-Wu-Shuang Bao},   year = {2022},   note = {R package version 0.1.4},   url = {https://psychbruce.github.io/PsychWordVec/}, }"},{"path":"https://psychbruce.github.io/PsychWordVec/index.html","id":"psychwordvec-","dir":"","previous_headings":"","what":"Word Embedding Research Framework for Psychological Science","title":"Word Embedding Research Framework for Psychological Science","text":"Word Embedding Research Framework Psychological Science. integrated toolbox word embedding research provides: collection pre-trained word vectors .RData compressed format; variety functions process, analyze, visualize word vectors; range tests examine conceptual associations, including Word Embedding Association Test (Caliskan et al., 2017) Relative Norm Distance (Garg et al., 2018), permutation test significance; set training methods locally train word vectors text corpora, including Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/index.html","id":"author","dir":"","previous_headings":"","what":"Author","title":"Word Embedding Research Framework for Psychological Science","text":"Han-Wu-Shuang (Bruce) Bao 包寒吴霜 Email: baohws@foxmail.com Homepage: psychbruce.github.io","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Word Embedding Research Framework for Psychological Science","text":"Bao, H.-W.-S. (2022). PsychWordVec: Word embedding research framework psychological science. R package version 0.1.x. https://CRAN.R-project.org/package=PsychWordVec","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Word Embedding Research Framework for Psychological Science","text":"","code":"## Method 1: Install from CRAN install.packages(\"PsychWordVec\")  ## Method 2: Install from GitHub install.packages(\"devtools\") devtools::install_github(\"psychbruce/PsychWordVec\", force=TRUE)"},{"path":"https://psychbruce.github.io/PsychWordVec/index.html","id":"functions","dir":"","previous_headings":"","what":"Functions","title":"Word Embedding Research Framework for Psychological Science","text":"data_transform() data_wordvec_load() data_wordvec_normalize() as_matrix(): wordvec matrix as_wordvec(): matrix wordvec data_wordvec_subset() orth_procrustes() (Orthogonal Procrustes matrix alignment) get_wordvec() get_wordvecs() sum_wordvec() plot_wordvec() plot_wordvec_tSNE() cosine_similarity() pair_similarity() plot_similarity() tab_similarity() tab_similarity_cross() most_similar() (find Top-N similar words) dict_expand() (expand dictionary similar words) dict_reliability() (reliability analysis PCA dictionary) test_WEAT() (permutation test significance) test_RND() (permutation test significance) tokenize() train_wordvec() See documentation (help pages) usage details.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/cosine_similarity.html","id":null,"dir":"Reference","previous_headings":"","what":"Cosine similarity/distance between two vectors. — cosine_similarity","title":"Cosine similarity/distance between two vectors. — cosine_similarity","text":"Cosine similarity/distance two vectors.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/cosine_similarity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cosine similarity/distance between two vectors. — cosine_similarity","text":"","code":"cosine_similarity(v1, v2, distance = FALSE)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/cosine_similarity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cosine similarity/distance between two vectors. — cosine_similarity","text":"v1, v2 Numeric vector (length). distance Compute cosine distance instead? Defaults FALSE (cosine similarity).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/cosine_similarity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cosine similarity/distance between two vectors. — cosine_similarity","text":"value cosine similarity/distance.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/cosine_similarity.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cosine similarity/distance between two vectors. — cosine_similarity","text":"Cosine similarity = sum(v1 * v2) / ( sqrt(sum(v1^2)) * sqrt(sum(v2^2)) ) Cosine distance = 1 - cosine_similarity(v1, v2)","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/cosine_similarity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cosine similarity/distance between two vectors. — cosine_similarity","text":"","code":"cosine_similarity(v1=c(1,1,1), v2=c(2,2,2))  # 1 #> [1] 1 cosine_similarity(v1=c(1,4,1), v2=c(4,1,1))  # 0.5 #> [1] 0.5 cosine_similarity(v1=c(1,1,0), v2=c(0,0,1))  # 0 #> [1] 0  cosine_similarity(v1=c(1,1,1), v2=c(2,2,2), distance=TRUE)  # 0 #> [1] -2.220446e-16 cosine_similarity(v1=c(1,4,1), v2=c(4,1,1), distance=TRUE)  # 0.5 #> [1] 0.5 cosine_similarity(v1=c(1,1,0), v2=c(0,0,1), distance=TRUE)  # 1 #> [1] 1"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_transform.html","id":null,"dir":"Reference","previous_headings":"","what":"Transform plain text data of word vectors into a compressed ","title":"Transform plain text data of word vectors into a compressed ","text":"Transform plain text data word vectors compressed \".RData\" file. Speed: total (preprocess + compress + save), can process 30000 words/min slowest settings (compress=\"xz\", compress.level=9) modern computer (HP ProBook 450, Windows 11, Intel i7-1165G7 CPU, 32GB RAM).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_transform.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Transform plain text data of word vectors into a compressed ","text":"","code":"data_transform(   file.load,   file.save,   sep = \" \",   header = \"auto\",   encoding = \"auto\",   compress = \"bzip2\",   compress.level = 9,   verbose = TRUE )"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_transform.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Transform plain text data of word vectors into a compressed ","text":"file.load File name raw data (must plain text). Data must format (values separated sep): cat 0.001 0.002 0.003 0.004 0.005 ... 0.300 dog 0.301 0.302 0.303 0.304 0.305 ... 0.600 file.save File name --saved R data (must .RData). sep Column separator. Defaults \" \". header 1st row header (e.g., meta-information \"2000000 300\")? Defaults \"auto\", automatically determines whether header. TRUE, 1st row dropped. encoding File encoding. Defaults \"auto\" (using vroom::vroom_lines() fast read file). specified value (e.g., \"UTF-8\"), uses readLines() read file, much slower vroom. compress Compression method saved file. Defaults \"bzip2\". Options include: 1 \"gzip\": modest file size (fastest) 2 \"bzip2\": small file size (fast) 3 \"xz\": minimized file size (slow) compress.level Compression level 0 (none) 9 (maximal compression minimal file size). Defaults 9. verbose Print information console? Defaults TRUE.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_transform.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Transform plain text data of word vectors into a compressed ","text":"data.table (new class wordvec) two variables: word vec.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_transform.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Transform plain text data of word vectors into a compressed ","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_transform.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Transform plain text data of word vectors into a compressed ","text":"","code":"if (FALSE) { # please first manually download plain text data of word vectors # e.g., from: https://fasttext.cc/docs/en/crawl-vectors.html  # the text file must be on your disk # the following code cannot run unless you have the file library(bruceR) set.wd() data_transform(file.load=\"cc.zh.300.vec\",   # plain text file                file.save=\"cc.zh.300.vec.RData\",  # RData file                header=TRUE, compress=\"xz\")  # of minimal size }"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_load.html","id":null,"dir":"Reference","previous_headings":"","what":"Load word vectors data from an ","title":"Load word vectors data from an ","text":"Load word vectors data \".RData\" file.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_load.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load word vectors data from an ","text":"","code":"data_wordvec_load(file.load, normalize = FALSE, verbose = TRUE)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_load.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load word vectors data from an ","text":"file.load File name (must .RData transformed data_transform). normalize Normalize word vectors unit length? Defaults FALSE. See data_wordvec_normalize. verbose Print information console? Defaults TRUE.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_load.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load word vectors data from an ","text":"data.table (new class wordvec) two variables: word words (tokens) vec raw normalized word vectors","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_load.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Load word vectors data from an ","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_load.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Load word vectors data from an ","text":"","code":"if (FALSE) { # please first manually download the .RData file # (see https://psychbruce.github.io/WordVector_RData.pdf) # or transform plain text data by using `data_transform()`  # the RData file must be on your disk # the following code cannot run unless you have the file library(bruceR) set.wd() d = data_wordvec_load(\"GloVe/glove_wiki_50d.RData\") }"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_normalize.html","id":null,"dir":"Reference","previous_headings":"","what":"Normalize all word vectors to unit length. — data_wordvec_normalize","title":"Normalize all word vectors to unit length. — data_wordvec_normalize","text":"L2-normalization (scaling unit euclidean length): norm vector vector space normalized 1. R formula: normalized_vec = vec / sqrt(sum(vec^2)) Note: Normalization change results cosine similarity can make computation faster.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_normalize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Normalize all word vectors to unit length. — data_wordvec_normalize","text":"","code":"data_wordvec_normalize(data, verbose = TRUE)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_normalize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Normalize all word vectors to unit length. — data_wordvec_normalize","text":"data data.table (new class wordvec) loaded data_wordvec_load. verbose Print information console? Defaults TRUE.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_normalize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Normalize all word vectors to unit length. — data_wordvec_normalize","text":"data.table (new class wordvec) normalized word vectors.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_normalize.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Normalize all word vectors to unit length. — data_wordvec_normalize","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_normalize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Normalize all word vectors to unit length. — data_wordvec_normalize","text":"","code":"d = data_wordvec_normalize(demodata) #> ✔ All word vectors have now been normalized.  data_wordvec_normalize(d)  # already normalized #> ! Word vectors have already been normalized."},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_reshape.html","id":null,"dir":"Reference","previous_headings":"","what":"Reshape word vectors data. — data_wordvec_reshape","title":"Reshape word vectors data. — data_wordvec_reshape","text":"Reshape word vectors data dense (data.table) plain (matrix) vice versa. easier use, two wrappers as_matrix() as_wordvec() also provided.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_reshape.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reshape word vectors data. — data_wordvec_reshape","text":"","code":"data_wordvec_reshape(   x,   to = c(\"plain\", \"dense\"),   normalize = FALSE,   verbose = TRUE )  as_wordvec(x, normalize = FALSE)  as_matrix(x, normalize = FALSE)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_reshape.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reshape word vectors data. — data_wordvec_reshape","text":"x Data reshaped, matrix, data.frame, data.table. See examples. Options include: \"plain\" (default) reshapes data   data.table (two variables word vec,   loaded data_wordvec_load)   matrix (dimensions columns words row names). \"dense\" just reverse. normalize Normalize word vectors unit length? Defaults FALSE. See data_wordvec_normalize. verbose Print information console? Defaults TRUE.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_reshape.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reshape word vectors data. — data_wordvec_reshape","text":"data.table (dense) matrix (plain) word vectors.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_reshape.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Reshape word vectors data. — data_wordvec_reshape","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_reshape.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Reshape word vectors data. — data_wordvec_reshape","text":"","code":"d = head(demodata, 10) d #>     word #>  1:   in #>  2:  for #>  3: that #>  4:   is #>  5:   on #>  6: with #>  7: said #>  8:  was #>  9:  the #> 10:   at #>                                                                                         vec #>  1:              0.05295587, 0.06545975, 0.06619558, 0.04707222, 0.05222079,-0.08200884,... #>  2:       -0.008512173,-0.034224283, 0.032284114, 0.045868184,-0.013142564,-0.046220811,... #>  3:             -0.01236052,-0.02222963, 0.06553974, 0.03947722,-0.08662021, 0.02491257,... #>  4:        0.003746262,-0.038919677, 0.091331742, 0.012000260,-0.070574262, 0.105342762,... #>  5:              0.01668099,-0.05667031, 0.01736675, 0.12735656, 0.00388493,-0.05636580,... #>  6:             -0.01596967, 0.01409130,-0.02270204, 0.08767797, 0.01048975,-0.01792692,... #>  7:       -0.004531971,-0.022021472, 0.049639883,-0.037960116,-0.028226893, 0.030659825,... #>  8:  0.0126005706,-0.0009168986, 0.0899195443,-0.0250828942, 0.0024846400,-0.0532416633,... #>  9:              0.07468524, 0.09791024, 0.04645094, 0.04986632,-0.06284517,-0.11248299,... #> 10:             -0.03536007,-0.02268949, 0.04390528, 0.06571068, 0.04007442, 0.02931927,...  d.plain = as_matrix(d) d.plain #>              [,1]          [,2]        [,3]        [,4]        [,5]        [,6] #> in    0.052955865  0.0654597520  0.06619558  0.04707222  0.05222079 -0.08200884 #> for  -0.008512173 -0.0342242830  0.03228411  0.04586818 -0.01314256 -0.04622081 #> that -0.012360524 -0.0222296329  0.06553974  0.03947722 -0.08662021  0.02491257 #> is    0.003746262 -0.0389196767  0.09133174  0.01200026 -0.07057426  0.10534276 #> on    0.016680988 -0.0566703074  0.01736675  0.12735656  0.00388493 -0.05636580 #> with -0.015969667  0.0140912978 -0.02270204  0.08767797  0.01048975 -0.01792692 #> said -0.004531971 -0.0220214718  0.04963988 -0.03796012 -0.02822689  0.03065983 #> was   0.012600571 -0.0009168986  0.08991954 -0.02508289  0.00248464 -0.05324166 #> the   0.074685235  0.0979102375  0.04645094  0.04986632 -0.06284517 -0.11248299 #> at   -0.035360065 -0.0226894859  0.04390528  0.06571068  0.04007442  0.02931927 #>               [,7]        [,8]        [,9]       [,10]        [,11]       [,12] #> in   -0.0614145542 -0.11620963 0.015629482  0.09929300 -0.085686495 -0.02813337 #> for  -0.0009480451 -0.05221907 0.046573437  0.06245102 -0.122784845 -0.02875568 #> that -0.0111626981 -0.07052257 0.092369148  0.09275220 -0.056340973 -0.06055691 #> is    0.0599364722 -0.05734172 0.038141198  0.01109212 -0.065385291 -0.03139527 #> on    0.0140914507 -0.10054456 0.082872681  0.03808492 -0.009825890  0.05514716 #> with -0.0155784738 -0.08955634 0.036793338  0.03992481 -0.075465688 -0.06231390 #> said  0.1275066591 -0.07884005 0.008273563 -0.04818022 -0.031633596 -0.07543335 #> was  -0.0039636963 -0.04283012 0.046852935  0.02342647  0.005353583 -0.17605276 #> the   0.0327884579 -0.11066151 0.040985572  0.02812051 -0.053054142 -0.07104228 #> at    0.0766130724 -0.03506497 0.041548107  0.05451318 -0.120223860 -0.02460431 #>             [,13]        [,14]       [,15]       [,16]        [,17]       [,18] #> in    0.052220786  0.058840269 -0.07759535 -0.07355015  0.033281186  0.07722781 #> for   0.051513099 -0.018700045  0.01314256  0.09879251  0.104437427 -0.02434496 #> that -0.054041870  0.060556911 -0.10884985  0.00510214  0.008623402 -0.01188171 #> is    0.053709175  0.005708134  0.07628293  0.13803567 -0.019330180  0.09859683 #> on    0.008607248  0.028944787 -0.03488575 -0.04174147  0.007655047  0.08531059 #> with -0.112102549 -0.013230031 -0.07640519  0.06356636  0.038672349  0.09018289 #> said  0.037230037  0.069106826 -0.11485362  0.03102511  0.088086884  0.10804021 #> was   0.039753756 -0.010293301  0.07666813  0.02141482 -0.005708808  0.10317096 #> the   0.012010745  0.046450937 -0.07923940 -0.05920221  0.005863609 -0.04030287 #> at    0.026962095 -0.052450502 -0.03285504  0.02165815 -0.033444633  0.06836235 #>              [,19]       [,20]         [,21]       [,22]        [,23] #> in   -0.0457850724 -0.02721377 -0.0342007886  0.03567170 -0.090834312 #> for  -0.0705657703 -0.08679598 -0.0575113629  0.04516221 -0.048338015 #> that -0.0007543318 -0.02376263 -0.0001240213  0.03008693 -0.018972113 #> is   -0.0417737438 -0.01206509 -0.0640876493  0.09444512  0.026335956 #> on    0.0962789960 -0.02879253 -0.0245269739 -0.09627900 -0.102981842 #> with -0.0403942412 -0.03131342 -0.0291214596  0.02191965  0.007241566 #> said -0.1245868418 -0.02092660 -0.1002535366  0.02409011  0.059616797 #> was  -0.0278040282  0.02744929 -0.0520587091  0.08991954  0.037387363 #> the   0.0188993432  0.01240992 -0.0182157063  0.08652530 -0.160300267 #> at    0.0444942761 -0.02872968 -0.0842741986 -0.06335290  0.020037209 #>              [,24]        [,25]       [,26]        [,27]        [,28] #> in   -4.817559e-02  0.001700625  0.02794885 -0.002194695  0.088628320 #> for   5.363030e-02  0.016406527  0.02416865 -0.130546964  0.037576403 #> that -3.602739e-02  0.074355453 -0.04331011 -0.050975092  0.004791302 #> is    9.146193e-03  0.042292907 -0.02425984 -0.100153790  0.100672422 #> on    6.733421e-02  0.020718170 -0.03183883  0.023155455  0.063373153 #> with -2.896498e-02  0.125253700 -0.02458105 -0.084546504  0.040707837 #> said  1.387000e-02  0.055966901 -0.06716028 -0.054506743 -0.029443359 #> was   7.026971e-05  0.073828750 -0.03170860 -0.073828750  0.109323192 #> the  -1.231106e-03  0.061023689  0.05442048 -0.076962781  0.080150600 #> at    5.561634e-03 -0.028729676 -0.01804815 -0.053629090  0.044789376 #>              [,29]       [,30]        [,31]         [,32]        [,33] #> in    0.0465209053 0.048175588  0.061047014 -0.0518532458 -0.016088907 #> for   0.0100115584 0.067037337  0.002536310 -0.0065712820 -0.070213144 #> that  0.0006703427 0.048675989 -0.042734742  0.0110669348  0.017439116 #> is   -0.0123244049 0.033470858  0.051892902 -0.0101191585 -0.042033591 #> on    0.0688579802 0.012796682  0.004227498  0.0007381741 -0.007807299 #> with -0.0120558097 0.085172413  0.036636861  0.0142477751 -0.030060965 #> said -0.1031733538 0.041123625 -0.061320149  0.0231168393  0.028104798 #> was   0.0392807680 0.043066609  0.035494442 -0.0501657885 -0.032418567 #> the  -0.0029742403 0.054648052 -0.032560890 -0.0114987171 -0.044857028 #> at    0.0324132926 0.006740222  0.049209233 -0.0556917709 -0.134957120 #>             [,34]        [,35]        [,36]       [,37]        [,38] #> in    0.041556105 -0.064356379  0.051853246 -0.09635118 -0.025007020 #> for   0.049043268 -0.006350890  0.031930765 -0.09667531 -0.071976999 #> that -0.035261284  0.087386320 -0.030278456  0.04024411  0.019738219 #> is    0.080953268  0.092369536  0.053968491 -0.08666140  0.060974266 #> on   -0.077998111  0.009254944 -0.016757114 -0.01340569  0.009407196 #> with  0.058242919 -0.112728458  0.015813190 -0.03115695  0.003542544 #> said  0.016425031  0.036986843 -0.015816549  0.02141349  0.025671866 #> was   0.085660231  0.103170958  0.068149503 -0.01656427  0.008636874 #> the  -0.002817554  0.052598073  0.013946939 -0.06739933 -0.048727551 #> at   -0.100186650 -0.025341159 -0.006188042 -0.04478938 -0.077202668 #>             [,39]         [,40]       [,41]        [,42]       [,43] #> in    0.074285981  0.1323904167  0.08347975 -0.026110393 -0.03548793 #> for   0.023992333  0.0201994301  0.11219955 -0.012789937  0.01032011 #> that  0.013318941  0.0494420946  0.10808296  0.106549959  0.05135893 #> is    0.053449859 -0.0492981509  0.05812020  0.031265612 -0.01148110 #> on    0.086529232  0.0303156817 -0.04783468 -0.072818412  0.06672520 #> with  0.035227924  0.0077109982  0.07734406  0.019101138 -0.10208223 #> said -0.077379891 -0.0008058277 -0.01429609 -0.046963259  0.03966347 #> was   0.028868735  0.0001386009  0.02851399  0.004495808  0.08092744 #> the   0.090169192  0.0400753000 -0.03301603 -0.068309598  0.03051184 #> at    0.002725901  0.0539235866 -0.01804815 -0.104312012 -0.04302119 #>             [,44]        [,45]        [,46]        [,47]        [,48] #> in   -0.006389771  0.027029996  0.077595346  0.020318628 -0.021605770 #> for  -0.079386492 -0.061745049 -0.052924328 -0.017817756  0.124902049 #> that -0.050592039 -0.018876349 -0.010492355 -0.029128512  0.003377617 #> is    0.033730705  0.022313905 -0.004702754  0.017124934  0.056563242 #> on    0.026050121  0.007997614 -0.005902897 -0.018052507 -0.024069593 #> with  0.060748481 -0.034914328  0.006849732  0.072647172 -0.025989989 #> said -0.098793379 -0.103173354 -0.038690195 -0.057670252 -0.047450145 #> was  -0.001308955  0.056081529  0.050638776  0.026029355  0.008992100 #> the  -0.057608302  0.007343734  0.003315592 -0.008197114  0.036431413 #> at   -0.045967964 -0.006482538  0.052450502 -0.033297385  0.136135708 #>             [,49]       [,50]        [,51]       [,52]        [,53]       [,54] #> in   -0.003861428  0.08016963  0.045049240  0.07097586  0.025374560 -0.02041051 #> for   0.044633269  0.06456751 -0.017553286  0.10232094 -0.023816020  0.01984680 #> that -0.012360524  0.01494770  0.085086432  0.03506976 -0.035261284 -0.07473851 #> is    0.034249337 -0.06304986  0.016216798  0.03554698  0.064866128 -0.04410918 #> on    0.151730661  0.00594096  0.013786946  0.13893398  0.005712582 -0.02833515 #> with  0.154688755  0.08204158 -0.009629128  0.02880850 -0.082041584 -0.05855652 #> said -0.007056599  0.07543335  0.086626726 -0.01155817  0.075920231 -0.05304659 #> was   0.051585237  0.02449117 -0.007986031 -0.06199727  0.105063878  0.02496465 #> the   0.035748708  0.04144164  0.065121780  0.01178318 -0.004155916 -0.03096698 #> at    0.027698336  0.10254383 -0.010166155  0.08898916  0.007035322 -0.01009253 #>            [,55]        [,56]        [,57]        [,58]        [,59] #> in   -0.07097586 0.0007757501 -0.036407534  0.025926623  0.061047014 #> for  -0.11219955 0.0056897157 -0.051160472  0.031578139  0.004344243 #> that  0.06822268 0.0643897959  0.005365882 -0.103483965  0.002143684 #> is    0.09133174 0.0417737438  0.050595792 -0.004135236  0.012324405 #> on   -0.02208906 0.0877478746 -0.115168890  0.048444315  0.025897869 #> with  0.08955634 0.0989507564 -0.052293574  0.010567992  0.024424574 #> said  0.02737522 0.0715402563 -0.054506743  0.006935002  0.054993629 #> was   0.02318997 0.0738287499  0.083293838  0.038097329 -0.028513995 #> the  -0.03984773 0.0910794642 -0.020150968 -0.035293572  0.011100473 #> at   -0.06718376 0.0386013342 -0.044789376  0.067773358 -0.004751769 #>            [,60]        [,61]       [,62]       [,63]        [,64]       [,65] #> in   -0.08531820 -0.066930665 0.027029996 -0.10959014 -0.183876123 -0.04633638 #> for  -0.04039886 -0.106554631 0.020552057 -0.09596933 -0.127724507 -0.06597946 #> that -0.05940775  0.017630642 0.134912364 -0.03813575  0.030086929 -0.06937262 #> is    0.01245459 -0.008951706 0.082509693 -0.05837952 -0.093926493 -0.06175275 #> on   -0.05301438 -0.061850006 0.036409518 -0.06032686 -0.126137297 -0.00875950 #> with -0.09080945  0.003718261 0.043838667 -0.10897108 -0.066697826 -0.06200094 #> said -0.04939669 -0.091979974 0.022508357 -0.04720695  0.007756278 -0.01423479 #> was  -0.02082358 -0.035257948 0.088026624 -0.03904379 -0.074775210 -0.05655452 #> the  -0.01297884 -0.105652216 0.086981373 -0.03665991 -0.108384898  0.02174488 #> at   -0.06070122 -0.169727589 0.009871055 -0.05952264 -0.168549001 -0.05097742 #>             [,66]        [,67]         [,68]        [,69]      [,70] #> in    0.039901422  0.042843247  0.1353329950  0.045968842 0.06545975 #> for  -0.036694114 -0.018788201 -0.1072606067 -0.058216616 0.10867184 #> that -0.013510468  0.017151826  0.1050169619  0.008383994 0.03909417 #> is    0.012454594 -0.005643305  0.0280220397 -0.071093425 0.04229291 #> on   -0.001475724  0.092013435  0.1249186543  0.037170780 0.09627900 #> with  0.067950286 -0.016361502  0.0389853039 -0.090809445 0.11460747 #> said  0.066673396  0.021048199  0.0001210984 -0.067160281 0.09003343 #> was   0.040937195 -0.029933927 -0.0218882878  0.004466246 0.06483665 #> the  -0.014914104  0.024591343  0.1001877837 -0.004354571 0.08971312 #> at   -0.033002889  0.061585316  0.1160984988  0.047146553 0.12729599 #>             [,71]         [,72]       [,73]       [,74]       [,75] #> in    0.093408597 -3.033936e-02  0.01700851  0.13386133 -0.02234085 #> for  -0.031401826  5.715874e-02  0.02399233  0.06527348  0.01640653 #> that  0.029895403 -4.120175e-03  0.04810062 -0.03928570 -0.08317038 #> is    0.039178992  2.335170e-02  0.06123358 -0.01096247  0.03969816 #> on    0.084091947  3.294640e-03  0.07799811  0.05331888 -0.01729062 #> with  0.060121930  8.705142e-02 -0.01471721  0.03788932  0.03272236 #> said -0.042826976 -2.993024e-02  0.06521324  0.05523682 -0.07932693 #> was   0.009346841  3.123513e-02  0.05276868  0.07666813 -0.01141762 #> the   0.026071469 -5.032146e-02  0.07969453 -0.03438237 -0.01889934 #> at    0.068067854  8.810748e-05  0.07955985  0.01023978  0.06423699 #>             [,76]       [,77]       [,78]        [,79]      [,80]        [,81] #> in   -0.022340850  0.08826078 0.023444223 -0.072446775 0.05001404  0.003539831 #> for  -0.045162209  0.11855044 0.062098398 -0.008952956 0.14183752 -0.044985895 #> that  0.043693160  0.12111461 0.134145473  0.037752699 0.09965108  0.064006743 #> is   -0.006130054  0.04281154 0.022184248  0.042552222 0.18681466  0.051374271 #> on   -0.036256642  0.11455988 0.004913257 -0.095669363 0.07738848 -0.049967462 #> with -0.100203216  0.09018289 0.156567766  0.034288418 0.12462779  0.027085972 #> said  0.019223251  0.02798320 0.139186925  0.071053371 0.02360323  0.083706909 #> was   0.059867370  0.08234738 0.016919008  0.062943730 0.12872684  0.093705386 #> the  -0.079694531  0.11658201 0.134797719  0.024932696 0.14026308  0.049182687 #> at   -0.013922792 -0.04419978 0.076613072 -0.057754451 0.03005612  0.038306838 #>             [,82]        [,83]        [,84]        [,85]        [,86] #> in   -0.060311181  0.047439755 -0.015537597 -0.041188565 -0.102234826 #> for   0.016230214 -0.021786972  0.015347925  0.002403352 -0.040046234 #> that -0.079720550  0.034494393 -0.010636000 -0.105016962 -0.123414495 #> is   -0.113127016  0.087699196 -0.041514428  0.037103404 -0.000741813 #> on   -0.089576150  0.093232078  0.009102068  0.067334209 -0.125528288 #> with -0.079849618 -0.004951482 -0.024737529  0.078283562 -0.115234020 #> said  0.041123625  0.069593213  0.065213238 -0.027740008 -0.072999916 #> was  -0.042830115  0.041173689 -0.014375244  0.027922275  0.012186222 #> the  -0.173961814  0.076051576 -0.009904808 -0.034838436 -0.068309598 #> at   -0.004622625  0.082506618  0.055102778  0.002596757 -0.027551087 #>            [,87]       [,88]       [,89]        [,90]        [,91]        [,92] #> in   -0.04780805  0.06288547 -0.04817559  0.016180792  0.058104436 -0.027948845 #> for  -0.05292433  0.02152250  0.03598886  0.012613624  0.075505432  0.028049706 #> that  0.01906788  0.16404088 -0.08010439 -0.073588562  0.038135752  0.059023914 #> is   -0.04852020  0.06901784  0.13388396 -0.008562201  0.049557998 -0.077839353 #> on   -0.09384171  0.03275297  0.04813919  0.057279941 -0.023765088  0.092623069 #> with -0.16408253 -0.01268172 -0.04728309  0.096445836 -0.008063072  0.044465217 #> said -0.07932693  0.15476028 -0.01143657 -0.022265164  0.167413818 -0.014113197 #> was  -0.04921884  0.09654525  0.05063878 -0.011772357  0.009761190 -0.017037255 #> the  -0.07013108  0.03369966 -0.12295765  0.005749825  0.047361209  0.042124345 #> at    0.08073843 -0.01038703 -0.07366630  0.024899414 -0.059522636 -0.009723807 #>              [,93]        [,94]        [,95]        [,96]        [,97] #> in   -0.0253745600 -0.138274820 -0.054795071  0.011951824  0.070240783 #> for   0.0613924225 -0.179237606  0.050101870 -0.107965860  0.042163438 #> that  0.0027669299 -0.096968139 -0.018972113 -0.001036127  0.030086929 #> is   -0.0008029223 -0.080434105 -0.014270336 -0.083548019  0.014011020 #> on    0.0341244855 -0.094450720  0.008911752 -0.065505933  0.011882544 #> with -0.0090808163 -0.029121460 -0.051354068 -0.062626850  0.035853833 #> said  0.0462331803  0.009794021 -0.027618411 -0.072999916  0.008516757 #> was   0.0321815888 -0.030052174  0.014079627 -0.047562901 -0.088026624 #> the   0.0093927796 -0.140263084 -0.056014393  0.053736847 -0.006460509 #> at   -0.0097974308 -0.060701224 -0.106079592 -0.030350612  0.088399560 #>             [,98]       [,99]      [,100]        [,101]       [,102] #> in   -0.046336382 -0.01071062 -0.00259688  0.0083660506 -0.119151450 #> for   0.069154542 -0.02416865  0.04551484  0.0154360816 -0.105143402 #> that  0.005964795  0.01389352  0.03430287 -0.0770383942 -0.045609210 #> is    0.045666137  0.03814120  0.04099527 -0.0207569484  0.028930176 #> on   -0.039608689  0.03336260  0.02178456  0.0871382414 -0.083482314 #> with  0.011742855 -0.06012193  0.10583961  0.0416467013 -0.133395652 #> said  0.103660239 -0.03211998 -0.07348680 -0.0564532881 -0.037230037 #> was   0.104117418 -0.02792228 -0.02188829  0.0217700409 -0.009465088 #> the   0.014800320 -0.01992340  0.09654483 -0.0002723356 -0.043718255 #> at    0.023572975 -0.07101463 -0.02872968  0.0580489476  0.002209929 #>           [,103]       [,104]       [,105]        [,106]       [,107] #> in   -0.01287143  0.004665798 -0.006573541 -0.0606787213 -0.011492399 #> for   0.03881132 -0.065626109 -0.018347418  0.0328130543  0.003836981 #> that  0.01106693  0.032195290 -0.027787041 -0.0180136951 -0.102717074 #> is   -0.06798004  0.048520204 -0.098077670 -0.0199790012 -0.014854327 #> on    0.13527805 -0.121262727 -0.036409518 -0.0840919472 -0.165745985 #> with -0.09018289  0.015187280  0.012995315  0.0385158718  0.047909643 #> said -0.01058490  0.061076955  0.072026643  0.0345534131 -0.023846918 #> was  -0.10127755  0.057974449 -0.043776576  0.0246094218  0.004377561 #> the  -0.01525546 -0.073318894 -0.064666643  0.0152554558 -0.029373071 #> at    0.08722097 -0.072782208 -0.016427820  0.0005847681  0.043021191 #>           [,108]       [,109]       [,110]       [,111]       [,112] #> in   -0.06619558  0.002620228 -0.012135594 -0.009285653  0.073550148 #> for  -0.08362090 -0.014113010  0.087501960  0.023287080  0.068449289 #> that -0.11344884  0.022709234 -0.096202033 -0.055957920 -0.005605290 #> is   -0.04774173 -0.062012061 -0.052412065  0.025557478 -0.086142239 #> on   -0.06489692  0.022089065 -0.134668418  0.051491233  0.028182901 #> with -0.02364155  0.016987411 -0.001594915  0.002387562  0.049162103 #> said -0.08906016 -0.072999916  0.024576499  0.017763592 -0.059616797 #> was  -0.04732592 -0.038097329 -0.066256583 -0.055135069 -0.002736157 #> the  -0.01280817 -0.034154799 -0.082882350 -0.044857028 -0.012296138 #> at   -0.05009333 -0.035212817 -0.050387821  0.083685206  0.016354196 #>           [,113]        [,114]       [,115]       [,116]       [,117] #> in   -0.10517665 -0.0647246723 -0.020225989  0.040636502  0.100028080 #> for  -0.04657344  0.0164065271  0.087148611  0.043574667  0.087148611 #> that -0.07588845  0.0459930477  0.081637385  0.020696636  0.005940461 #> is   -0.05786088  0.0451469738 -0.024259836  0.084066650 -0.020238317 #> on   -0.04356912 -0.0269642589  0.016833864 -0.056365803  0.003427549 #> with -0.02990449 -0.0519806189 -0.022859159  0.127759261  0.098324206 #> said  0.00158923  0.0102201078 -0.068133554 -0.120206867 -0.004775663 #> was  -0.01975839 -0.0009983145 -0.027449287  0.001767404  0.040227228 #> the  -0.06694326  0.0054933444 -0.043035550  0.037115050  0.093812147 #> at   -0.07130912 -0.0515664098 -0.009650183  0.005267137  0.091346332 #>            [,118]      [,119]     [,120]       [,121]      [,122]      [,123] #> in    0.084950662  0.09120261 0.06435638 -0.005355688  0.03364948 -0.10959014 #> for   0.035282885  0.06739069 0.04833802  0.021169876 -0.02469831 -0.08044509 #> that  0.028362406  0.03161993 0.04139327 -0.160208776 -0.02625404 -0.02280500 #> is   -0.043590017  0.10793751 0.04592545  0.036844088  0.01712493 -0.08510444 #> on    0.031077567 -0.02224194 0.03732303 -0.093232078 -0.01378695 -0.02071817 #> with  0.047596688  0.02552056 0.04070784 -0.068263241 -0.02505048 -0.07139471 #> said  0.015816549  0.02920017 0.04234009 -0.107066942  0.11582689 -0.02956496 #> was  -0.034311488  0.12778038 0.06034084 -0.007808661  0.04425005 -0.11594890 #> the  -0.045994868  0.07058714 0.03620384 -0.155746108 -0.08971312 -0.09426728 #> at   -0.003775345  0.15322614 0.03418148 -0.001169536  0.01569098  0.04449428 #>            [,124]      [,125]       [,126]       [,127]      [,128] #> in   -0.002413110 -0.08862832 -0.049278961  0.053691698 -0.07097586 #> for   0.038635005 -0.01852373  0.012878094  0.044985895 -0.01870004 #> that  0.024912574 -0.09696814 -0.052891926  0.012456287 -0.06783963 #> is    0.050336477 -0.01310288  0.028800518  0.014854327  0.02387086 #> on    0.109684687 -0.04143634 -0.011425787  0.007046037 -0.02635463 #> with -0.013855940 -0.03554088 -0.057929964 -0.002524800 -0.07296077 #> said  0.063753579 -0.05742656 -0.100253537  0.043800248 -0.02749681 #> was  -0.005087527 -0.03289156  0.068149503  0.113582506 -0.03099863 #> the   0.027096458 -0.05396441 -0.017988138 -0.040075300 -0.07832819 #> at   -0.035212817 -0.02843518 -0.001187641 -0.045378368 -0.03359188 #>            [,129]      [,130]       [,131]       [,132]       [,133] #> in   -0.022800275  0.09046677  0.060311181 -0.071344156 -0.122093275 #> for   0.105143402  0.04516221  0.077975263 -0.117845183 -0.070565770 #> that  0.009821228 -0.04944209 -0.094668251  0.018397533 -0.103483965 #> is    0.090293948  0.03866036 -0.019330180 -0.005448819 -0.009081364 #> on   -0.048139186  0.01089228 -0.065505933 -0.065201429 -0.029553796 #> with  0.079536022  0.07859716  0.035071446 -0.046657183 -0.114607469 #> said  0.015025671  0.01320072  0.006722208 -0.066186511 -0.016060241 #> was   0.094651846  0.02437293 -0.060577336  0.009997684 -0.057737956 #> the  -0.018557058  0.04804485  0.007912654 -0.033699663 -0.139351879 #> at    0.017090437  0.06040673 -0.010018906 -0.152047556 -0.015322856 #>           [,134]        [,135]      [,136]       [,137]      [,138] #> in   -0.05847273  1.599702e-02 -0.06141455  0.002965173 -0.11841637 #> for  -0.07656403 -6.174505e-02 -0.06421488  0.073035601 -0.05751136 #> that -0.09275220 -9.533938e-03 -0.08623638  0.074738506 -0.03296140 #> is   -0.05708241 -3.730320e-04 -0.03917899  0.134921759  0.03009816 #> on   -0.01843314 -3.838942e-02 -0.03168658 -0.015995853 -0.05941272 #> with  0.04571768 -4.289980e-02  0.01964945 -0.004970721  0.02505048 #> said -0.03990666 -7.361089e-03 -0.06107695  0.026888332  0.07543335 #> was  -0.05679150 -4.361568e-05  0.01786547  0.070988884  0.01200885 #> the  -0.01730543 -3.392723e-02 -0.07149742 -0.036887481 -0.05737980 #> at   -0.08545339 -1.675852e-03 -0.04685206  0.050387821 -0.02976102 #>            [,139]       [,140]      [,141]       [,142]       [,143] #> in   -0.073918441  0.029971821  0.02960428 -0.006849196  0.077595346 #> for   0.006086421  0.017376973  0.09455810  0.037047463  0.058922592 #> that  0.014373122  0.040627166  0.01173806 -0.124947492 -0.017056063 #> is    0.018681360 -0.004573097  0.09859683  0.011416268  0.140111261 #> on   -0.050577095 -0.063373153  0.12613730  0.074037055 -0.001761509 #> with  0.027712522 -0.033348913 -0.01589143  0.130890732 -0.043525712 #> said  0.045016714  0.033580141 -0.01727671 -0.014478486  0.002661673 #> was  -0.072408817  0.001471302 -0.01502609  0.051822215  0.123994054 #> the  -0.018671775  0.038708959  0.03438237 -0.067399326  0.005521324 #> at   -0.085453390  0.120223860  0.03830684  0.014880508  0.001390408 #>            [,144]       [,145]       [,146]       [,147]      [,148] #> in    0.051117413 -0.032178566  0.047808048 -0.036958844  0.01572137 #> for   0.067743313 -0.042339751 -0.069860517 -0.020463900 -0.10514340 #> that -0.004024411  0.028170879 -0.002383092 -0.061323802 -0.04024411 #> is   -0.012649081 -0.052671381 -0.021924932 -0.036844088 -0.06045564 #> on   -0.037628161  0.014015325  0.081654038  0.050271966 -0.09627900 #> with  0.023328591  0.042273252  0.019884166  0.005440794 -0.12525370 #> said -0.036013571  0.004379975  0.063753579 -0.011436574 -0.11971998 #> was  -0.029105714 -0.100331092  0.076668131 -0.104590406 -0.08944607 #> the  -0.058746143  0.006887665 -0.014800320  0.015027888 -0.01354776 #> at   -0.052156006 -0.027846187  0.008177098  0.042726695 -0.09841847 #>            [,149]       [,150]       [,151]      [,152]        [,153] #> in   -0.125770933  0.070240783  0.070608323 0.005171165  0.0404527319 #> for  -0.106554631  0.105143402 -0.012701781 0.023816020 -0.0617450490 #> that -0.005821150  0.068605734 -0.018780586 0.034685920 -0.0893031546 #> is    0.026595272 -0.031265612  0.024519683 0.046444084  0.0560446108 #> on   -0.051491233  0.099934923  0.036104390 0.060935868 -0.0131011867 #> with -0.001614795  0.118364849  0.092061905 0.051667023 -0.0078674756 #> said  0.014539284  0.026279850  0.033580141 0.133346792 -0.0596167972 #> was  -0.083767310  0.003875011 -0.014730470 0.047562901  0.0022626847 #> the   0.007201038  0.094267283 -0.005208884 0.013320194 -0.0710422805 #> at   -0.076024080  0.034475973 -0.020774053 0.128474582 -0.0009667683 #>            [,154]       [,155]       [,156]      [,157]        [,158] #> in    0.039533129 -0.018387537 -0.024455710 -0.04633638 -0.0041830253 #> for  -0.007938432 -0.026814790 -0.009879323  0.02593322 -0.0059541856 #> that  0.016863751 -0.003006338 -0.034111340 -0.08163739 -0.1456441279 #> is    0.056563242  0.014854327  0.050336477  0.06175275 -0.0918509047 #> on   -0.028182901 -0.045701901  0.002723070 -0.05667031  0.0119586703 #> with  0.045404723 -0.019962405 -0.013621224 -0.08141503 -0.1183648490 #> said  0.114853619  0.032363176  0.011801862 -0.02056181  0.0006309078 #> was   0.124940514  0.051822215  0.061523797  0.03076214 -0.0634167173 #> the   0.052598073 -0.012067637  0.028576583 -0.02322500 -0.0919906692 #> at    0.044494276 -0.016943189 -0.053334594 -0.07602408  0.0086924665 #>            [,159]      [,160]       [,161]       [,162]        [,163] #> in    0.072446775  0.02850091  0.009193768 -0.033097416 -0.0056313432 #> for   0.036341487 -0.06844929  0.034577632  0.014995298  0.0221403211 #> that -0.035261284  0.03526128 -0.034878231  0.014947701 -0.0164806982 #> is   -0.018551702 -0.11053226  0.031654585  0.055266132 -0.0009527727 #> on   -0.010359397 -0.03138207  0.009178194 -0.002589537  0.0216323080 #> with  0.015813190  0.05573800 -0.003092351  0.070142251 -0.0358538331 #> said -0.016789821  0.02177828  0.020683409  0.041123625  0.0367431514 #> was   0.031708601 -0.04803589  0.028632242 -0.017983715  0.0489823496 #> the   0.030284276 -0.02618525 -0.075596440  0.019240695  0.0150278878 #> at   -0.009429311  0.04567347  0.041548107 -0.046852056 -0.0166486924 #>           [,164]      [,165]       [,166]       [,167]       [,168] #> in    0.07943455  0.01535383  0.109590142  0.061782094  0.004344201 #> for   0.09385285  0.03810534  0.013584070 -0.012701781  0.025227248 #> that  0.01058812  0.01197747 -0.023859178  0.036602755  0.080487441 #> is    0.03113595 -0.01582729 -0.019979001  0.025816793 -0.033990021 #> on    0.03579988  0.04996746  0.003884930  0.039913193  0.015310093 #> with  0.12588025  0.03663686  0.067950286  0.050414563 -0.060434885 #> said  0.02336003  0.02907857 -0.005018856  0.001452184 -0.025184981 #> was   0.07430174 -0.07430174 -0.036677881  0.028395748 -0.024491175 #> the  -0.03916410 -0.03256089 -0.035066004  0.051232665  0.012808166 #> at    0.06246941  0.02489941  0.070720131 -0.007661126  0.025046663 #>            [,169]       [,170]      [,171]       [,172]       [,173] #> in    0.003447946 -0.069872490 -0.10444157 -0.043210787 -0.038798049 #> for   0.013231443 -0.007145023 -0.13336942 -0.064920856 -0.020993562 #> that -0.010875408  0.006467944 -0.04139327  0.015426518 -0.059790805 #> is    0.042292907  0.036844088 -0.05578476 -0.076801559  0.023092384 #> on    0.019804032 -0.077998111 -0.04874882 -0.015310093 -0.045092892 #> with -0.056990459 -0.050101608 -0.12149632  0.004031857  0.007437163 #> said  0.148920147 -0.028104798  0.01654663  0.047206951  0.041610012 #> was   0.100804565  0.023544715 -0.04566950 -0.047089429 -0.059630876 #> the   0.025046480 -0.054648052 -0.06694326 -0.112027853 -0.021289741 #> at   -0.018416877 -0.040074419 -0.04154811 -0.040074419  0.057165459 #>            [,174]       [,175]       [,176]        [,177]       [,178] #> in   -0.098557168 -0.105176651 -0.015445712 -0.0204105125  0.024639480 #> for  -0.043927293 -0.037047463 -0.001708938  0.0479846660 -0.059628567 #> that -0.070522569  0.034494393  0.006228536  0.0099169909 -0.085086432 #> is   -0.036584772 -0.018941207 -0.006227297  0.0072650913 -0.035027815 #> on   -0.053928518 -0.048139186  0.027116511 -0.0001173092 -0.007122163 #> with -0.037419889 -0.076718148 -0.062000941 -0.0040510958  0.047909643 #> said -0.058886718 -0.011618969 -0.010828590  0.1060936694  0.039906660 #> was  -0.095598791 -0.085660231 -0.053951630  0.0539516297 -0.028513995 #> the  -0.153013425 -0.003372484 -0.055785892  0.0660329846 -0.071952553 #> at   -0.001574469 -0.104312012 -0.121991441 -0.0083611582  0.062174309 #>           [,179]       [,180]      [,181]        [,182]       [,183] #> in    0.07906626 -0.001758618 -0.01700851  0.0003795903 -0.083479749 #> for  -0.02893199  0.069507168 -0.11149429 -0.1107883171  0.020463900 #> that -0.01456465 -0.082020438 -0.11958161 -0.0620906930 -0.022613471 #> is    0.06330970  0.016605771 -0.02464934 -0.0010457650  0.003908334 #> on   -0.06185001  0.016376483  0.03336260  0.0283351537 -0.044483259 #> with -0.05072816  0.067637331  0.01996240 -0.0095117703  0.044152263 #> said -0.02482019  0.099280264  0.02774001 -0.0260366566  0.019710137 #> was   0.11026965  0.019403648  0.06034084  0.0714623569  0.008814245 #> the   0.04827241 -0.040075300 -0.04508460  0.0280067301 -0.061478825 #> at    0.05186151  0.034475973 -0.02195264  0.0633528971 -0.056281367 #>            [,184]      [,185]       [,186]        [,187]       [,188] #> in    0.063988839 -0.09782209 -0.013147081 -0.0002703828  0.081273004 #> for   0.009482619  0.02161066 -0.008776643 -0.0698605172  0.017905912 #> that -0.014660412  0.07627150 -0.006563707 -0.0277870412  0.005916913 #> is   -0.030098160  0.01478950  0.043849864 -0.0071678479  0.038141198 #> on    0.086529232  0.02559336  0.006969287 -0.0095594485  0.020565918 #> with  0.021136626 -0.07891011 -0.034444896 -0.0058711069 -0.036636861 #> said  0.030173438  0.06229342  0.006752607 -0.0345534131  0.073486801 #> was   0.019640142  0.08234738  0.055135069 -0.0108850204  0.032655061 #> the  -0.029600639 -0.04553973 -0.032560890  0.0548756197 -0.013661547 #> at    0.039779923 -0.02313123 -0.038011738 -0.0014362726  0.067183762 #>          [,189]       [,190]        [,191]      [,192]       [,193]      [,194] #> in   0.06693066  0.033649479  0.0189388467  0.01792811  0.061047014  0.01783623 #> for  0.13972031  0.009393739  0.0174651293 -0.02593322  0.071271746 -0.06986052 #> that 0.04560921  0.064389796  0.0226134709  0.04005180  0.002491414 -0.01456465 #> is   0.07680156  0.067980042  0.0224435633  0.07524513 -0.009599996  0.01148110 #> on   0.11395025  0.010892280 -0.0199562846  0.06733421  0.020565918 -0.02193681 #> with 0.04477817 -0.041333747 -0.0007778976 -0.03209645 -0.039454736 -0.03914178 #> said 0.01873686  0.074460074 -0.0335801407 -0.07835366 -0.092953246  0.06083326 #> was  0.04164716  0.032655061  0.0099976836  0.02318997  0.008932976  0.09938463 #> the  0.16849738  0.053054142  0.0489551188  0.05396441  0.108840034  0.04849998 #> at   0.06895195 -0.045673468 -0.0218053937  0.00163904 -0.064236989  0.04243160 #>            [,195]       [,196]       [,197]        [,198]       [,199] #> in   -0.082743916  0.004045198 -0.013330851 -0.0255590831 -0.024823250 #> for  -0.144659974 -0.009967480  0.062098398 -0.0578639894 -0.127724507 #> that  0.011738063 -0.057107864 -0.026829409  0.0344943933 -0.038327278 #> is   -0.048779519  0.071093425 -0.103785805 -0.0267249297 -0.020108659 #> on   -0.135887061  0.063678282 -0.018280885 -0.0005902897 -0.004455876 #> with -0.003914499 -0.008219550 -0.085798964  0.0049514818 -0.091435355 #> said -0.052803392 -0.003285106  0.090033429  0.0552368222 -0.085653454 #> was  -0.022716501  0.097018239 -0.007572167 -0.0681495035  0.005324021 #> the  -0.049866324  0.017419218 -0.014572752  0.0053795604 -0.068309598 #> at   -0.163834044  0.006224854 -0.007734750  0.0424315955 -0.006703410 #>            [,200]      [,201]      [,202]        [,203]        [,204] #> in   -0.123564188 0.072079235 -0.01379103  0.0039992554 -0.0259266229 #> for  -0.126313278 0.003704746 -0.02522725 -0.0395172938  0.0677433129 #> that -0.126480489 0.020122057  0.02874546 -0.0001208815 -0.0009882453 #> is   -0.051114424 0.054746969 -0.05682309 -0.0783585163  0.0529306966 #> on   -0.016452609 0.038542298 -0.01058778 -0.0135585676 -0.0743421833 #> with -0.116486480 0.022389086  0.04070784 -0.0205100753 -0.0601219298 #> said -0.005018856 0.044286635 -0.08565345  0.0321199829 -0.0256718663 #> was  -0.084240298 0.023544715 -0.07666813 -0.0506387763  0.0176289741 #> the  -0.108384898 0.037797754  0.05829101 -0.0403028681  0.0098479158 #> at   -0.044494276 0.047441049 -0.05569177 -0.0130386997 -0.0294665199 #>            [,205]       [,206]        [,207]      [,208]       [,209] #> in   -0.033281186 -0.050014040 -0.0135146212 -0.02234085 -0.005723228 #> for  -0.067390686 -0.008644408 -0.0004082664  0.07056577  0.017905912 #> that -0.031236873 -0.025296412 -0.0123605237  0.04771757  0.076271503 #> is   -0.122467696  0.120910739 -0.0399574710  0.03450865  0.048779519 #> on    0.005674519  0.064287291 -0.0018756986  0.09323208  0.066115567 #> with  0.019805927 -0.043212757 -0.0302174423 -0.03100047  0.074839137 #> said -0.033093255  0.041610012  0.0299302445  0.01995333 -0.024333305 #> was   0.014611738  0.061997269 -0.0553715625  0.06862298  0.113582506 #> the   0.020265684  0.039619231  0.0305118444  0.04121314  0.053736847 #> at    0.018858621 -0.079559845 -0.0005340761  0.12022386 -0.067183762 #>            [,210]        [,211]       [,212]       [,213]      [,214] #> in   -0.038614279 -0.0408202718  0.067298204 -0.054059238 0.011492399 #> for  -0.028755681  0.0070568661  0.085384755  0.018611888 0.088913188 #> that -0.011785944 -0.0264455707 -0.012025352  0.003664907 0.025870991 #> is    0.024908657  0.0334708582  0.037103404  0.024519683 0.051633587 #> on   -0.025135983 -0.0115019134  0.022394194 -0.023765088 0.035495380 #> with  0.008728742 -0.0186317059 -0.076718148  0.024424574 0.059808975 #> said  0.001794549 -0.0131399250 -0.009186037  0.007056599 0.005748935 #> was  -0.043540082 -0.0004991573 -0.072882290  0.087080163 0.065783110 #> the   0.024363775 -0.0170778659 -0.025161196 -0.006289833 0.004752815 #> at    0.065415578  0.0269620950 -0.049209233 -0.053923587 0.018858621 #>            [,215]       [,216]     [,217]       [,218]       [,219] #> in   -0.062150387 -0.023903647 0.02684623 -0.015997022 -0.044681700 #> for   0.046573437  0.051160472 0.02116988 -0.035812548 -0.056452761 #> that -0.064389796  0.083553435 0.12111461  0.006898879 -0.094285198 #> is   -0.107418350  0.105861925 0.09911546 -0.063569018 -0.075763765 #> on    0.009559448  0.012339301 0.11273161  0.005103572 -0.094450720 #> with -0.003444425  0.001653914 0.02818195  0.127759261 -0.092687815 #> said  0.022629954 -0.031389904 0.05012677  0.037230037 -0.006174524 #> was  -0.100804565 -0.006181796 0.08518676 -0.067676031 -0.098438172 #> the  -0.108384898  0.003401397 0.05373685 -0.055558324 -0.082427214 #> at   -0.081917022  0.010902395 0.01370192 -0.028287932 -0.057754451 #>            [,220]       [,221]       [,222]        [,223]       [,224] #> in   -0.009837716  0.035304161  0.017376049  0.0158132517 -0.059207809 #> for   0.020905406  0.032989368 -0.031049199  0.0187000447 -0.037400089 #> that  0.048100624  0.045993048  0.030469982 -0.0125520502 -0.034494393 #> is    0.079915473 -0.017902881 -0.077320722 -0.0003666554 -0.038919677 #> on    0.020109161  0.098107272  0.031686577 -0.0180525067  0.027421016 #> with  0.007358924  0.001546176  0.064818815  0.0541725843  0.006105823 #> said  0.014600083  0.101713694 -0.061806536 -0.0642399657 -0.030295035 #> was  -0.014552615 -0.013487907 -0.003150021  0.0615237967 -0.072408817 #> the   0.012637490  0.042351914 -0.043263119 -0.0165080130 -0.058291006 #> at    0.066594769  0.037422746  0.109615961  0.0545131825 -0.095471693 #>            [,225]       [,226]       [,227]      [,228]       [,229] #> in   -0.006068173  0.014709879 -0.004183025  0.03125896  0.020961822 #> for   0.101614969  0.003086927 -0.027344453  0.01984680  0.043398353 #> that  0.094285198 -0.059023914  0.098118083  0.02759551  0.057107864 #> is    0.071093425  0.018941207 -0.012194747  0.01472467 -0.042033591 #> on   -0.036561770  0.001933105 -0.007883425  0.10054456  0.066115567 #> with -0.034288418 -0.012525242 -0.025207602 -0.03569736  0.118991400 #> said  0.094900289 -0.077866776 -0.002403031  0.11534001  0.009246835 #> was   0.070988884 -0.004081943  0.054425102  0.08045446 -0.007631291 #> the   0.032105754 -0.022542298  0.028804151  0.08925799  0.074230099 #> at    0.061879812 -0.064532089  0.015985472  0.01222884  0.006261666 #>           [,230]       [,231]      [,232]        [,233]        [,234] #> in    0.01025120  0.026110393 -0.13753899  0.0904667722  5.589844e-02 #> for   0.02046390  0.020287587 -0.02646216  0.0945581033 -7.009175e-05 #> that  0.06860573  0.016576461 -0.05787397  0.0270209352 -7.397162e-02 #> is    0.11001310 -0.044368495 -0.02633596  0.0167354291  7.887768e-02 #> on   -0.02117555  0.113341239 -0.02787840  0.0213278034 -2.361284e-02 #> with  0.01088159  0.059808975 -0.03100047  0.0162832632  1.503080e-02 #> said  0.03333695  0.001383911  0.02360323 -0.0540198579 -7.397369e-02 #> was   0.05963088  0.035021454 -0.02117832 -0.0366778810 -4.377658e-02 #> the   0.03665991  0.026071469 -0.08015060  0.0755964403  6.193396e-02 #> at   -0.05745996  0.026225251 -0.12670640 -0.0005226101 -6.445726e-03 #>             [,235]       [,236]       [,237]       [,238]      [,239] #> in   -0.0308914236 -0.007493143  0.032362336 -0.005493516  0.09267352 #> for  -0.0501018703 -0.015965744  0.049043268 -0.016847310 -0.01107016 #> that  0.0091030033 -0.044843104 -0.061706855  0.012552050  0.05940775 #> is    0.0295789968 -0.023870863 -0.042292907  0.002529922 -0.01102729 #> on   -0.0055222662 -0.023003203  0.049053324  0.016909990  0.02879253 #> with -0.0191793766 -0.025989989  0.005675510  0.059182424  0.01557847 #> said  0.0281047981 -0.035039800  0.009307634 -0.047206951  0.02031812 #> was   0.0492188435 -0.070042424 -0.019403648 -0.006152234  0.01289619 #> the  -0.0003917156 -0.064666643  0.033472095 -0.031878186  0.04189678 #> at   -0.0034072248 -0.044494276  0.044789376  0.061879812  0.07484489 #>            [,240]       [,241]       [,242]       [,243]      [,244]     [,245] #> in    0.043394557 -0.040268962 -0.024271940 -0.006849196 -0.03512039 0.03309742 #> for  -0.042163438  0.044103606  0.000466074  0.002888936 -0.05151310 0.06633208 #> that  0.023954941  0.003617025 -0.114215728 -0.019450929 -0.08470338 0.05404187 #> is    0.031914432  0.002529922  0.005935035  0.091850905 -0.07161259 0.01634646 #> on    0.042960111  0.031534324 -0.001090102 -0.085310590 -0.09566936 0.06154550 #> with  0.015500235 -0.103334687  0.101455676  0.021136626 -0.03961185 0.04884915 #> said -0.021169796  0.029808149  0.018980058 -0.068619941  0.01691192 0.07981332 #> was  -0.037860835  0.017274233  0.016919008  0.087080163 -0.06720304 0.01360615 #> the  -0.007201038 -0.006916578 -0.044400959  0.013035734 -0.09290094 0.02299743 #> at    0.093114517  0.002689088  0.067183762 -0.071898719 -0.13967147 0.01480688 #>            [,246]       [,247]       [,248]       [,249]      [,250] #> in   -0.038245986  0.051853246  0.002252688 -0.003148943 -0.03328119 #> for   0.018964515  0.014466359  0.025579875 -0.041810089 -0.02143435 #> that  0.045609210  0.098118083 -0.051358929  0.004144508  0.00929453 #> is   -0.042292907  0.048001041  0.032433064  0.040995265 -0.02672493 #> on   -0.100544557  0.004132028 -0.053624013 -0.010968406  0.02544049 #> with  0.056990459  0.129011721  0.045404723 -0.050728159  0.03585383 #> said  0.049153496 -0.019710137 -0.064239966  0.094900289 -0.01399160 #> was  -0.013902256  0.032891555  0.034074509  0.046379463  0.02425468 #> the  -0.092900942  0.107018557  0.029600639  0.020607037  0.06739933 #> at    0.003591285  0.011786487 -0.019595465 -0.090757340 -0.06954094 #>            [,251]       [,252]      [,253]       [,254]       [,255] #> in    0.055530151 -0.009607251  0.05074987  0.004735088  0.056633523 #> for   0.019758647  0.018170382  0.04357467  0.095264079 -0.003153406 #> that  0.054807976  0.099651080  0.05135893 -0.013606231  0.093519092 #> is   -0.049038835 -0.142186850  0.05734172  0.045666137  0.033470858 #> on    0.018661516  0.007122163 -0.01264443 -0.040217698  0.010892280 #> with  0.002798636  0.009550890  0.11335501 -0.004912362 -0.018475229 #> said -0.052560198  0.005231651  0.04136682 -0.015816549  0.090033429 #> was  -0.010707650 -0.145764095  0.05537156  0.036440903  0.006122673 #> the   0.034382367  0.023908639  0.01275127 -0.025502548  0.005521324 #> at   -0.088399560 -0.005451198  0.01723768 -0.080148838  0.091935928 #>            [,256]        [,257]       [,258]       [,259]       [,260] #> in   -0.028500908  0.0036776580  0.033649479 -0.050749873  0.007309374 #> for   0.001973408  0.0432213175  0.071271746 -0.066332084 -0.033165681 #> that -0.025487938  0.1134488375  0.060173858 -0.025296412 -0.051741982 #> is    0.057341721 -0.0142055066  0.054228338 -0.064087649  0.028152229 #> on   -0.080435396 -0.0002165228  0.026354626  0.002056654  0.076474964 #> with -0.042899803  0.0522935736 -0.006888851 -0.053546034  0.067950286 #> said -0.057183367  0.0307819202  0.001079919 -0.017641497 -0.099766651 #> was   0.064363178  0.0122458297  0.017628974 -0.013665278 -0.066256583 #> the  -0.062845166  0.0471336414 -0.026412821 -0.042124345 -0.016166661 #> at   -0.035801810  0.0307923564  0.068362350 -0.069836038  0.033886377 #>            [,261]      [,262]      [,263]      [,264]      [,265]     [,266] #> in    0.003562425  0.01544571  0.05369170  0.12871276  0.13091950 0.04192364 #> for  -0.012613624  0.02769708 -0.01384854  0.03351903  0.03457763 0.07091912 #> that  0.049442095 -0.04905904 -0.07550461  0.08355343 -0.03123687 0.09121920 #> is    0.050336477 -0.08769920  0.02348136  0.03840105  0.02192493 0.22625350 #> on   -0.059717226  0.05758445  0.06550593 -0.07769361  0.02239419 0.09079479 #> with  0.024894006  0.02458105 -0.02019712  0.03162638  0.01526552 0.05010161 #> said -0.039663467 -0.04939669  0.01478248  0.02153508  0.05012677 0.15184046 #> was   0.087080163 -0.04495953  0.04117369  0.06436318  0.01928540 0.20823484 #> the   0.019695832  0.03278846 -0.04007530  0.06193396  0.11384933 0.11521567 #> at    0.048914737  0.01009253  0.07013053  0.01679594  0.08898916 0.01038703 #>            [,267]      [,268]       [,269]       [,270]      [,271] #> in    0.068769870 -0.02813337  0.037510907 -0.029604281  0.03328119 #> for  -0.029108308  0.06809594 -0.025050935 -0.030519537  0.05080712 #> that -0.007617888 -0.02778704 -0.051358929  0.046184574  0.12724659 #> is   -0.054746969 -0.08510444 -0.048001041 -0.033990021 -0.02555748 #> on   -0.065811062  0.01431983 -0.005217762  0.002894666  0.13710633 #> with  0.007945714 -0.02066719  0.041960297  0.037419889  0.08830388 #> said  0.134320064 -0.04525991 -0.044286635  0.096846834  0.11728705 #> was  -0.009110347 -0.07146236  0.029578702  0.043776576  0.04212015 #> the   0.003728759  0.04212435 -0.017305434  0.045084596  0.04212435 #> at    0.054513183  0.04596796  0.087220971 -0.014806884 -0.03138195 #>            [,272]      [,273]       [,274]      [,275]       [,276] #> in    0.047072215  0.03659130 -0.040085192  0.03677507 -0.098557168 #> for  -0.009879323  0.07691666  0.011907649  0.09526408 -0.001224077 #> that  0.040244113  0.12494749  0.074738506  0.05979081 -0.072055565 #> is    0.076801559  0.03476850  0.026205767  0.02880052  0.072131219 #> on   -0.030925315  0.14868374 -0.036409518  0.03016343  0.037780413 #> with -0.020510075 -0.03616679 -0.046030632  0.05479849  0.018083394 #> said -0.103660239  0.01606024  0.009368432 -0.06910683 -0.012957530 #> was  -0.033365028  0.05371514  0.047562901  0.07288229  0.078088064 #> the   0.081060872  0.02743781  0.035066004  0.03210575 -0.068764734 #> at    0.028435180  0.04361079  0.055397275  0.07248771 -0.085453390 #>            [,277]      [,278]      [,279]      [,280]       [,281]       [,282] #> in   -0.021789540 -0.02721377 -0.04578507 -0.04321079  0.092673518 -0.062150387 #> for  -0.006130499 -0.10302620 -0.03369534 -0.07938649  0.059275218 -0.029637970 #> that  0.019738219 -0.06170686  0.07013873 -0.04599305 -0.031428399  0.036027390 #> is   -0.010248816 -0.11468344 -0.03943884 -0.07783935  0.006097639 -0.087699196 #> on   -0.023308331 -0.11090333  0.02803065 -0.02635463  0.051491233  0.068857980 #> with  0.001291579 -0.04540472  0.02223261 -0.02567703 -0.020040643  0.057303414 #> said  0.007847476 -0.01429609  0.03917658 -0.05742656 -0.090520314  0.082246751 #> was  -0.039043789 -0.08471377 -0.01609079 -0.06247026  0.059394382 -0.007010009 #> the  -0.037570186 -0.13661920 -0.02276987 -0.01821571  0.006148069 -0.001693703 #> at   -0.013775544 -0.07513999 -0.04066401 -0.03565456 -0.022689486 -0.048619637 #>            [,283]       [,284]       [,285]       [,286]       [,287] #> in   -0.008964056  0.094144430  0.001022785  0.048175588 -0.080169631 #> for  -0.013672226  0.063509627 -0.002029048  0.172180740 -0.034047970 #> that  0.024337994  0.030661509  0.027979353 -0.083170382 -0.029128512 #> is   -0.055784764  0.001702557  0.071612588 -0.002108003 -0.055006817 #> on   -0.068248347  0.058803088 -0.045092892  0.030620186 -0.098716281 #> with -0.025833511  0.029434414  0.070455206  0.051667023 -0.056364550 #> said -0.057670252 -0.058400331  0.085166568  0.062780306 -0.006235322 #> was  -0.079034524 -0.017392480  0.013487907 -0.030762141 -0.038334308 #> the  -0.010189268  0.086981373  0.061023689  0.017191650 -0.086981373 #> at   -0.041842603  0.031676449  0.049503729  0.011712863 -0.048914737 #>           [,288]       [,289]        [,290]       [,291]       [,292] #> in   -0.10811923 -0.031810273  0.0181118821 -0.127241846 -0.066930665 #> for  -0.01658284  0.029460935  0.0211698756 -0.016318371  0.002690222 #> that -0.12648049  0.016767988  0.0009584174 -0.008863595 -0.012264760 #> is   -0.07420681  0.055525448 -0.0066810996 -0.124024121 -0.019330180 #> on    0.04874882  0.018433138 -0.0755608258  0.016757114 -0.017442874 #> with -0.02129310  0.031469902  0.0089634583 -0.013308270  0.001976488 #> said -0.01642503  0.026036657  0.0135660120 -0.096359949  0.014904075 #> was  -0.05560806  0.008932976  0.0141387503 -0.145764095 -0.022243029 #> the  -0.01468654 -0.066488121 -0.0833374859 -0.066488121 -0.028120514 #> at   -0.05628137  0.041253007 -0.0128178275 -0.019889961  0.031823697 #>           [,293]      [,294]       [,295]       [,296]       [,297] #> in   -0.06067872  0.04891067  0.046152612 -0.035671701 -0.044314160 #> for  -0.05962857  0.05892259  0.005733794  0.000344678  0.013319600 #> that -0.02625404 -0.01619341 -0.015234991  0.050208986  0.015810355 #> is   -0.04981731  0.09703988  0.014400525  0.067980042 -0.013167712 #> on    0.01927115  0.02528824 -0.081654038  0.051795737  0.009825890 #> with -0.08893043 -0.01565671 -0.059808975 -0.030530397 -0.048536193 #> said -0.04647687  0.02676674 -0.058643525  0.034796607  0.052803392 #> was  -0.08471377  0.11310903  0.010944144  0.053478642 -0.050165788 #> the  -0.01212453  0.01525546 -0.017077866  0.013833155  0.004667944 #> at   -0.06895195  0.11315173  0.001786289 -0.001657748 -0.028435180 #>            [,298]      [,299]      [,300] #> in   -0.035855471  0.01089439 -0.04707222 #> for   0.051513099 -0.02522725  0.01746513 #> that  0.005389430  0.04790910 -0.11651562 #> is    0.005967450  0.08718003  0.05682309 #> on   -0.072818412 -0.01835701 -0.04417875 #> with  0.006927970 -0.01205581 -0.04415226 #> said  0.072026643  0.09003343 -0.04331336 #> was  -0.058920910  0.10743027 -0.01064853 #> the   0.003415387  0.04440096 -0.06421151 #> at   -0.065121081 -0.05215601 -0.01930037  d.dense = as_wordvec(d.plain) d.dense  # identical to `d` #>     word #>  1:   in #>  2:  for #>  3: that #>  4:   is #>  5:   on #>  6: with #>  7: said #>  8:  was #>  9:  the #> 10:   at #>                                                                                         vec #>  1:              0.05295587, 0.06545975, 0.06619558, 0.04707222, 0.05222079,-0.08200884,... #>  2:       -0.008512173,-0.034224283, 0.032284114, 0.045868184,-0.013142564,-0.046220811,... #>  3:             -0.01236052,-0.02222963, 0.06553974, 0.03947722,-0.08662021, 0.02491257,... #>  4:        0.003746262,-0.038919677, 0.091331742, 0.012000260,-0.070574262, 0.105342762,... #>  5:              0.01668099,-0.05667031, 0.01736675, 0.12735656, 0.00388493,-0.05636580,... #>  6:             -0.01596967, 0.01409130,-0.02270204, 0.08767797, 0.01048975,-0.01792692,... #>  7:       -0.004531971,-0.022021472, 0.049639883,-0.037960116,-0.028226893, 0.030659825,... #>  8:  0.0126005706,-0.0009168986, 0.0899195443,-0.0250828942, 0.0024846400,-0.0532416633,... #>  9:              0.07468524, 0.09791024, 0.04645094, 0.04986632,-0.06284517,-0.11248299,... #> 10:             -0.03536007,-0.02268949, 0.04390528, 0.06571068, 0.04007442, 0.02931927,..."},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_subset.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract a subset of word vectors data. — data_wordvec_subset","title":"Extract a subset of word vectors data. — data_wordvec_subset","text":"Extract subset word vectors data. may specify either data.table loaded data_wordvec_load) .RData file transformed data_transform).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_subset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract a subset of word vectors data. — data_wordvec_subset","text":"","code":"data_wordvec_subset(   x,   words = NULL,   pattern = NULL,   file.save,   compress = \"bzip2\",   compress.level = 9,   verbose = TRUE )"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_subset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract a subset of word vectors data. — data_wordvec_subset","text":"x Can one following: data.table (new class wordvec) loaded data_wordvec_load .RData file transformed data_transform words [Option 1] Word strings (NULL; single word; vector words). pattern [Option 2] Pattern regular expression (see str_subset). file.save File name --saved R data (must .RData). compress Compression method saved file. Defaults \"bzip2\". Options include: 1 \"gzip\": modest file size (fastest) 2 \"bzip2\": small file size (fast) 3 \"xz\": minimized file size (slow) compress.level Compression level 0 (none) 9 (maximal compression minimal file size). Defaults 9. verbose Print information console? Defaults TRUE.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_subset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract a subset of word vectors data. — data_wordvec_subset","text":"subset word vectors data valid (available) words.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_subset.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Extract a subset of word vectors data. — data_wordvec_subset","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/data_wordvec_subset.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract a subset of word vectors data. — data_wordvec_subset","text":"","code":"## specify `x` as a data.table: d = data_wordvec_subset(demodata, c(\"China\", \"Japan\", \"Korea\")) d #>     word #> 1: China #> 2: Japan #> 3: Korea #>                                                                                  vec #> 1:       -0.02697318, 0.04999036, 0.04010034, 0.03056975,-0.04711339,-0.08379688,... #> 2:        0.01832947, 0.09023783, 0.06027598, 0.03049064,-0.09587769,-0.02784667,... #> 3:  0.033778005, 0.133294179,-0.021963288, 0.001675763,-0.014011015,-0.112693960,...  ## specify `x` and `pattern`, and save with `file.save`: data_wordvec_subset(demodata, pattern=\"Chin[ae]|Japan|Korea\",                     file.save=\"subset.RData\") #> 6 words matched... #>  #> Compressing and saving... #> ✔ Saved to \"subset.RData\" (time cost = 0.007 secs)  ## load the subset: d.subset = data_wordvec_load(\"subset.RData\") #> Loading... #> ✔ Word vector data: 6 words, 300 dims (loading time: 0.003 secs) d.subset #>        word #> 1:    China #> 2:    Japan #> 3:  Chinese #> 4: Japanese #> 5:   Korean #> 6:    Korea #>                                                                                  vec #> 1:       -0.02697318, 0.04999036, 0.04010034, 0.03056975,-0.04711339,-0.08379688,... #> 2:        0.01832947, 0.09023783, 0.06027598, 0.03049064,-0.09587769,-0.02784667,... #> 3:       -0.04985132, 0.04593451, 0.07940573, 0.02581589,-0.03204736,-0.09044442,... #> 4:  0.003658937, 0.091899336, 0.062968012, 0.029782102,-0.093941412,-0.062627841,... #> 5:        0.02635512, 0.11745195, 0.03810029, 0.02850357,-0.05127785,-0.08995103,... #> 6:  0.033778005, 0.133294179,-0.021963288, 0.001675763,-0.014011015,-0.112693960,...  ## specify `x` as an .RData file and save with `file.save`: data_wordvec_subset(\"subset.RData\",                     words=c(\"China\", \"Chinese\"),                     file.save=\"new.subset.RData\") #> Loading... #> ✔ Word vector data: 6 words, 300 dims (loading time: 0.003 secs) #>  #> Compressing and saving... #> ✔ Saved to \"new.subset.RData\" (time cost = 0.003 secs) d.new.subset = data_wordvec_load(\"new.subset.RData\") #> Loading... #> ✔ Word vector data: 2 words, 300 dims (loading time: 0.002 secs) d.new.subset #>       word #> 1:   China #> 2: Chinese #>                                                                            vec #> 1: -0.02697318, 0.04999036, 0.04010034, 0.03056975,-0.04711339,-0.08379688,... #> 2: -0.04985132, 0.04593451, 0.07940573, 0.02581589,-0.03204736,-0.09044442,...  unlink(\"subset.RData\")  # delete file for code check unlink(\"new.subset.RData\")  # delete file for code check"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/demodata.html","id":null,"dir":"Reference","previous_headings":"","what":"Demo data (corpus: Google News; algorithm: word2vec; vocabulary: 8000; dimensions: 300). — demodata","title":"Demo data (corpus: Google News; algorithm: word2vec; vocabulary: 8000; dimensions: 300). — demodata","text":"demo data contains sample 8000 English words 300-d word embeddings (word vectors) trained using \"word2vec\" algorithm based Google News corpus. words Top 8000 frequent wordlist, whereas selected less frequent words appended.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/demodata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Demo data (corpus: Google News; algorithm: word2vec; vocabulary: 8000; dimensions: 300). — demodata","text":"","code":"data(demodata)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/demodata.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Demo data (corpus: Google News; algorithm: word2vec; vocabulary: 8000; dimensions: 300). — demodata","text":"data.table (new class wordvec) two variables word vec, transformed raw data (see URL Source) .RData using data_transform function.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/demodata.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Demo data (corpus: Google News; algorithm: word2vec; vocabulary: 8000; dimensions: 300). — demodata","text":"Google Code - word2vec (https://code.google.com/archive/p/word2vec/)","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/demodata.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Demo data (corpus: Google News; algorithm: word2vec; vocabulary: 8000; dimensions: 300). — demodata","text":"","code":"class(demodata) #> [1] \"wordvec\"    \"data.table\" \"data.frame\" head(demodata, 10) #>     word #>  1:   in #>  2:  for #>  3: that #>  4:   is #>  5:   on #>  6: with #>  7: said #>  8:  was #>  9:  the #> 10:   at #>                                                                                         vec #>  1:              0.05295587, 0.06545975, 0.06619558, 0.04707222, 0.05222079,-0.08200884,... #>  2:       -0.008512173,-0.034224283, 0.032284114, 0.045868184,-0.013142564,-0.046220811,... #>  3:             -0.01236052,-0.02222963, 0.06553974, 0.03947722,-0.08662021, 0.02491257,... #>  4:        0.003746262,-0.038919677, 0.091331742, 0.012000260,-0.070574262, 0.105342762,... #>  5:              0.01668099,-0.05667031, 0.01736675, 0.12735656, 0.00388493,-0.05636580,... #>  6:             -0.01596967, 0.01409130,-0.02270204, 0.08767797, 0.01048975,-0.01792692,... #>  7:       -0.004531971,-0.022021472, 0.049639883,-0.037960116,-0.028226893, 0.030659825,... #>  8:  0.0126005706,-0.0009168986, 0.0899195443,-0.0250828942, 0.0024846400,-0.0532416633,... #>  9:              0.07468524, 0.09791024, 0.04645094, 0.04986632,-0.06284517,-0.11248299,... #> 10:             -0.03536007,-0.02268949, 0.04390528, 0.06571068, 0.04007442, 0.02931927,... data_wordvec_normalize(demodata) #> ✔ All word vectors have now been normalized."},{"path":"https://psychbruce.github.io/PsychWordVec/reference/dict_expand.html","id":null,"dir":"Reference","previous_headings":"","what":"Expand a dictionary from the most similar words. — dict_expand","title":"Expand a dictionary from the most similar words. — dict_expand","text":"Expand dictionary similar words.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/dict_expand.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Expand a dictionary from the most similar words. — dict_expand","text":"","code":"dict_expand(data, words, threshold = 0.5, iteration = 5, verbose = TRUE)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/dict_expand.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Expand a dictionary from the most similar words. — dict_expand","text":"data data.table (new class wordvec) loaded data_wordvec_load. words vector words. threshold Threshold cosine similarity, used find words similarities higher value. Defaults 0.5. low threshold may lead failure convergence. iteration Number maximum iterations. Defaults 5. verbose Print information console? Defaults TRUE.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/dict_expand.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Expand a dictionary from the most similar words. — dict_expand","text":"expanded list (character vector) words.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/dict_expand.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Expand a dictionary from the most similar words. — dict_expand","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/dict_expand.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Expand a dictionary from the most similar words. — dict_expand","text":"","code":"dict = dict_expand(demodata, \"king\") #> ! Results may be inaccurate if word vectors are not normalized. #> ✔ All word vectors have now been normalized. #>  #> ── Iteration 1 (threshold of cosine similarity = 0.5) ────────────────────────── #> ✔ 3 more words appended: \"queen\", \"royal\", and \"King\" #>  #> ── Iteration 2 (threshold of cosine similarity = 0.5) ────────────────────────── #> ✔ 2 more words appended: \"Queen\" and \"Prince\" #>  #> ── Iteration 3 (threshold of cosine similarity = 0.5) ────────────────────────── #> ✔ No more words appended. Successfully convergent. #>  #> ── Finish (convergent) ── #>  dict #> [1] \"king\"   \"queen\"  \"royal\"  \"King\"   \"Queen\"  \"Prince\"  dict = dict_expand(demodata, cc(\"king, queen\")) #> ! Results may be inaccurate if word vectors are not normalized. #> ✔ All word vectors have now been normalized. #>  #> ── Iteration 1 (threshold of cosine similarity = 0.5) ────────────────────────── #> ✔ 2 more words appended: \"royal\" and \"Queen\" #>  #> ── Iteration 2 (threshold of cosine similarity = 0.5) ────────────────────────── #> ✔ 1 more words appended: \"King\" #>  #> ── Iteration 3 (threshold of cosine similarity = 0.5) ────────────────────────── #> ✔ 1 more words appended: \"Prince\" #>  #> ── Iteration 4 (threshold of cosine similarity = 0.5) ────────────────────────── #> ✔ No more words appended. Successfully convergent. #>  #> ── Finish (convergent) ── #>  dict #> [1] \"king\"   \"queen\"  \"royal\"  \"Queen\"  \"King\"   \"Prince\"  most_similar(demodata, dict) #> ! Results may be inaccurate if word vectors are not normalized. #> ✔ All word vectors have now been normalized. #> [Word Vector] =~ king + queen + royal + Queen + King + Prince #> (normalized to unit length) #>          word   cos_sim row_id #>  1:   Jackson 0.3615837   1066 #>  2:    Albert 0.3607588   6472 #>  3:     Crown 0.3587386   6775 #>  4:     crown 0.3399563   5174 #>  5: superstar 0.3375027   7218 #>  6:   royalty 0.3353748   7893 #>  7:     Kings 0.3330198   4068 #>  8:    legend 0.3305279   5428 #>  9:       Sir 0.3121715   5054 #> 10:      pope 0.3093004   6388  dict.cn = dict_expand(demodata, \"China\") #> ! Results may be inaccurate if word vectors are not normalized. #> ✔ All word vectors have now been normalized. #>  #> ── Iteration 1 (threshold of cosine similarity = 0.5) ────────────────────────── #> ✔ 19 more words appended: \"Chinese\", \"Beijing\", \"Taiwan\", \"Shanghai\", \"Shenzhen\", \"Guangzhou\", \"yuan\", \"India\", \"Japan\", \"Li\", \"Asia\", \"Korea\", \"Taiwanese\", \"mainland\", \"Russia\", \"Tibet\", \"Wang\", \"Chen\", and \"Vietnam\" #>  #> ── Iteration 2 (threshold of cosine similarity = 0.5) ────────────────────────── #> ✔ 14 more words appended: \"Seoul\", \"HK\", \"Singapore\", \"Korean\", \"Thailand\", \"Japanese\", \"Pyongyang\", \"Vietnamese\", \"Asian\", \"Tokyo\", \"Malaysia\", \"Xinhua\", \"Thai\", and \"Malaysian\" #>  #> ── Iteration 3 (threshold of cosine similarity = 0.5) ────────────────────────── #> ✔ 5 more words appended: \"Indonesian\", \"Indonesia\", \"Bangkok\", \"Philippine\", and \"Philippines\" #>  #> ── Iteration 4 (threshold of cosine similarity = 0.5) ────────────────────────── #> ✔ 1 more words appended: \"Nepal\" #>  #> ── Iteration 5 (threshold of cosine similarity = 0.5) ────────────────────────── #> ✔ No more words appended. Successfully convergent. #>  #> ── Finish (convergent) ── #>  dict.cn  # too inclusive if setting threshold = 0.5 #>  [1] \"China\"       \"Chinese\"     \"Beijing\"     \"Taiwan\"      \"Shanghai\"    #>  [6] \"Shenzhen\"    \"Guangzhou\"   \"yuan\"        \"India\"       \"Japan\"       #> [11] \"Li\"          \"Asia\"        \"Korea\"       \"Taiwanese\"   \"mainland\"    #> [16] \"Russia\"      \"Tibet\"       \"Wang\"        \"Chen\"        \"Vietnam\"     #> [21] \"Seoul\"       \"HK\"          \"Singapore\"   \"Korean\"      \"Thailand\"    #> [26] \"Japanese\"    \"Pyongyang\"   \"Vietnamese\"  \"Asian\"       \"Tokyo\"       #> [31] \"Malaysia\"    \"Xinhua\"      \"Thai\"        \"Malaysian\"   \"Indonesian\"  #> [36] \"Indonesia\"   \"Bangkok\"     \"Philippine\"  \"Philippines\" \"Nepal\"        dict.cn = dict_expand(demodata,                       cc(\"China, Chinese\"),                       threshold=0.6) #> ! Results may be inaccurate if word vectors are not normalized. #> ✔ All word vectors have now been normalized. #>  #> ── Iteration 1 (threshold of cosine similarity = 0.6) ────────────────────────── #> ✔ 8 more words appended: \"Beijing\", \"Taiwan\", \"Taiwanese\", \"Shanghai\", \"Li\", \"Guangzhou\", \"Shenzhen\", and \"yuan\" #>  #> ── Iteration 2 (threshold of cosine similarity = 0.6) ────────────────────────── #> ✔ 4 more words appended: \"Wang\", \"Chen\", \"mainland\", and \"HK\" #>  #> ── Iteration 3 (threshold of cosine similarity = 0.6) ────────────────────────── #> ✔ No more words appended. Successfully convergent. #>  #> ── Finish (convergent) ── #>  dict.cn  # adequate to represent \"China\" #>  [1] \"China\"     \"Chinese\"   \"Beijing\"   \"Taiwan\"    \"Taiwanese\" \"Shanghai\"  #>  [7] \"Li\"        \"Guangzhou\" \"Shenzhen\"  \"yuan\"      \"Wang\"      \"Chen\"      #> [13] \"mainland\"  \"HK\""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/dict_reliability.html","id":null,"dir":"Reference","previous_headings":"","what":"Reliability analysis and PCA of a dictionary. — dict_reliability","title":"Reliability analysis and PCA of a dictionary. — dict_reliability","text":"Reliability analysis (Cronbach's \\(\\alpha\\)) Principal Component Analysis (PCA) dictionary. Note Cronbach's \\(\\alpha\\) may misleading number items/words large.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/dict_reliability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reliability analysis and PCA of a dictionary. — dict_reliability","text":"","code":"dict_reliability(   data,   words = NULL,   pattern = NULL,   sort = TRUE,   plot = TRUE,   ... )"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/dict_reliability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reliability analysis and PCA of a dictionary. — dict_reliability","text":"data data.table (new class wordvec) loaded data_wordvec_load. words [Option 1] Word strings (NULL; single word; vector words). pattern [Option 2] Pattern regular expression (see str_subset). sort Sort items first principal component loading (PC1)? Defaults TRUE. plot Visualize cosine similarities words ordered PC1? Defaults TRUE. ... parameters passed plot_similarity.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/dict_reliability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reliability analysis and PCA of a dictionary. — dict_reliability","text":"list object new class reliability: alpha Cronbach's \\(\\alpha\\) eigen Eigen values PCA pca PCA (1 principal component) pca.rotation PCA varimax rotation (potential principal components > 1) items Item statistics cos.sim matrix cosine similarities word pairs","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/dict_reliability.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Reliability analysis and PCA of a dictionary. — dict_reliability","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/dict_reliability.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Reliability analysis and PCA of a dictionary. — dict_reliability","text":"Nicolas, G., Bai, X., & Fiske, S. T. (2021). Comprehensive stereotype content dictionaries using semi-automated method. European Journal Social Psychology, 51(1), 178--196.","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/dict_reliability.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Reliability analysis and PCA of a dictionary. — dict_reliability","text":"","code":"d = data_wordvec_normalize(demodata) #> ✔ All word vectors have now been normalized.  dict = dict_expand(d, \"king\") #>  #> ── Iteration 1 (threshold of cosine similarity = 0.5) ────────────────────────── #> ✔ 3 more words appended: \"queen\", \"royal\", and \"King\" #>  #> ── Iteration 2 (threshold of cosine similarity = 0.5) ────────────────────────── #> ✔ 2 more words appended: \"Queen\" and \"Prince\" #>  #> ── Iteration 3 (threshold of cosine similarity = 0.5) ────────────────────────── #> ✔ No more words appended. Successfully convergent. #>  #> ── Finish (convergent) ── #>  dict_reliability(d, dict)  #> ── Reliability Analysis and PCA of Dictionary ────────────────────────────────── #>  #> Number of items = 6 #> Cronbach’s α = 0.836 (misleading when N of items is large) #> Variance explained by PC1 = 55.2% #> Potential principal components = 1 (with eigen value > 1) #>  #> Item Statistics: #> ──────────────────────────────────────────────────── #>         PC1 Loading Item-SumVec Sim. Item-Rest Corr. #> ──────────────────────────────────────────────────── #> queen         0.785            0.769           0.650 #> king          0.781            0.772           0.653 #> Queen         0.760            0.755           0.631 #> royal         0.754            0.751           0.624 #> King          0.723            0.732           0.597 #> Prince        0.645            0.668           0.511 #> ──────────────────────────────────────────────────── #> PC1 Loading = the first principal component loading #> Item-SumVec Sim. = cosine similarity with the sum vector #> Item-Rest Corr. = corrected item-total correlation  dict.cn = dict_expand(d, \"China\", threshold=0.65) #>  #> ── Iteration 1 (threshold of cosine similarity = 0.65) ───────────────────────── #> ✔ 4 more words appended: \"Chinese\", \"Beijing\", \"Taiwan\", and \"Shanghai\" #>  #> ── Iteration 2 (threshold of cosine similarity = 0.65) ───────────────────────── #> ✔ 4 more words appended: \"Guangzhou\", \"Taiwanese\", \"Shenzhen\", and \"Li\" #>  #> ── Iteration 3 (threshold of cosine similarity = 0.65) ───────────────────────── #> ✔ 3 more words appended: \"Wang\", \"Chen\", and \"yuan\" #>  #> ── Iteration 4 (threshold of cosine similarity = 0.65) ───────────────────────── #> ✔ No more words appended. Successfully convergent. #>  #> ── Finish (convergent) ── #>  dict_reliability(d, dict.cn)  #> ── Reliability Analysis and PCA of Dictionary ────────────────────────────────── #>  #> Number of items = 12 #> Cronbach’s α = 0.946 (misleading when N of items is large) #> Variance explained by PC1 = 63.0% #> Potential principal components = 2 (with eigen value > 1) #>  #> Item Statistics: #> ─────────────────────────────────────────────────────── #>            PC1 Loading Item-SumVec Sim. Item-Rest Corr. #> ─────────────────────────────────────────────────────── #> China            0.839            0.837           0.801 #> Chinese          0.823            0.822           0.781 #> Beijing          0.822            0.817           0.780 #> Li               0.818            0.819           0.778 #> Shanghai         0.817            0.815           0.775 #> Wang             0.793            0.793           0.749 #> Guangzhou        0.791            0.792           0.745 #> Chen             0.786            0.787           0.741 #> Shenzhen         0.781            0.783           0.735 #> Taiwan           0.773            0.775           0.726 #> Taiwanese        0.770            0.773           0.723 #> yuan             0.704            0.711           0.651 #> ─────────────────────────────────────────────────────── #> PC1 Loading = the first principal component loading #> Item-SumVec Sim. = cosine similarity with the sum vector #> Item-Rest Corr. = corrected item-total correlation  dict_reliability(d, c(dict, dict.cn))  #>  #> ── Reliability Analysis and PCA of Dictionary ────────────────────────────────── #>  #> Number of items = 18 #> Cronbach’s α = 0.899 (misleading when N of items is large) #> Variance explained by PC1 = 42.4% #> Potential principal components = 4 (with eigen value > 1) #>  #> Item Statistics: #> ─────────────────────────────────────────────────────── #>            PC1 Loading Item-SumVec Sim. Item-Rest Corr. #> ─────────────────────────────────────────────────────── #> China            0.832            0.741           0.695 #> Chinese          0.821            0.751           0.705 #> Li               0.819            0.763           0.719 #> Beijing          0.818            0.743           0.699 #> Shanghai         0.812            0.741           0.695 #> Wang             0.794            0.742           0.695 #> Chen             0.789            0.747           0.702 #> Guangzhou        0.786            0.714           0.663 #> Taiwan           0.774            0.723           0.673 #> Taiwanese        0.772            0.726           0.676 #> Shenzhen         0.772            0.685           0.630 #> yuan             0.697            0.631           0.569 #> royal            0.178            0.406           0.329 #> Queen            0.154            0.392           0.308 #> king             0.141            0.382           0.302 #> King             0.127            0.361           0.277 #> queen            0.124            0.364           0.286 #> Prince           0.085            0.305           0.218 #> ─────────────────────────────────────────────────────── #> PC1 Loading = the first principal component loading #> Item-SumVec Sim. = cosine similarity with the sum vector #> Item-Rest Corr. = corrected item-total correlation # low-loading items should be removed"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/get_wordvec.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract the word vector of a single word. — get_wordvec","title":"Extract the word vector of a single word. — get_wordvec","text":"Extract word vector single word.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/get_wordvec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract the word vector of a single word. — get_wordvec","text":"","code":"get_wordvec(data, word)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/get_wordvec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract the word vector of a single word. — get_wordvec","text":"data data.table (new class wordvec) loaded data_wordvec_load. word Word string (single word).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/get_wordvec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract the word vector of a single word. — get_wordvec","text":"numeric vector word (NA word appear data).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/get_wordvec.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Extract the word vector of a single word. — get_wordvec","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/get_wordvec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract the word vector of a single word. — get_wordvec","text":"","code":"d = data_wordvec_normalize(demodata) #> ✔ All word vectors have now been normalized.  v1 = get_wordvec(demodata, \"China\")  # raw vector v2 = get_wordvec(d, \"China\")  # normalized vector cor(v1, v2) #> [1] 1 cosine_similarity(v1, v2) #> [1] 1"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/get_wordvecs.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract the word vectors of multiple words. — get_wordvecs","title":"Extract the word vectors of multiple words. — get_wordvecs","text":"Extract word vectors multiple words, using either wordlist (vector words; using words) regular expression (pattern words; using pattern). words pattern arguments specified, words wins.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/get_wordvecs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract the word vectors of multiple words. — get_wordvecs","text":"","code":"get_wordvecs(   data,   words = NULL,   pattern = NULL,   plot = FALSE,   plot.dims = NULL,   plot.step = 0.05,   plot.border = \"white\" )"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/get_wordvecs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract the word vectors of multiple words. — get_wordvecs","text":"data data.table (new class wordvec) loaded data_wordvec_load. words [Option 1] Word strings (NULL; single word; vector words). pattern [Option 2] Pattern regular expression (see str_subset). plot Generate plot illustrate word vectors? Defaults FALSE. plot.dims Dimensions plotted (e.g., 1:100). Defaults NULL (plot dimensions). plot.step Step value breaks. Defaults 0.05. plot.border Color tile border. Defaults \"white\". remove border color, set plot.border=NA.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/get_wordvecs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract the word vectors of multiple words. — get_wordvecs","text":"data.table words columns dimensions rows.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/get_wordvecs.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Extract the word vectors of multiple words. — get_wordvecs","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/get_wordvecs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract the word vectors of multiple words. — get_wordvecs","text":"","code":"d = data_wordvec_normalize(demodata) #> ✔ All word vectors have now been normalized.  get_wordvecs(d, c(\"China\", \"Japan\", \"Korea\")) #>            China        Japan        Korea #>   1: -0.02697318  0.018329469  0.033778005 #>   2:  0.04999036  0.090237829  0.133294179 #>   3:  0.04010034  0.060275982 -0.021963288 #>   4:  0.03056975  0.030490640  0.001675763 #>   5: -0.04711339 -0.095877693 -0.014011015 #>  ---                                       #> 296: -0.01393625 -0.087418077 -0.072705831 #> 297: -0.06761304 -0.105042608 -0.115117643 #> 298:  0.05070960  0.048291315  0.015374394 #> 299:  0.03452576  0.002103985 -0.043017655 #> 300: -0.02931099  0.008636121  0.077552886 get_wordvecs(d, cc(\" China, Japan; Korea \")) #>            China        Japan        Korea #>   1: -0.02697318  0.018329469  0.033778005 #>   2:  0.04999036  0.090237829  0.133294179 #>   3:  0.04010034  0.060275982 -0.021963288 #>   4:  0.03056975  0.030490640  0.001675763 #>   5: -0.04711339 -0.095877693 -0.014011015 #>  ---                                       #> 296: -0.01393625 -0.087418077 -0.072705831 #> 297: -0.06761304 -0.105042608 -0.115117643 #> 298:  0.05070960  0.048291315  0.015374394 #> 299:  0.03452576  0.002103985 -0.043017655 #> 300: -0.02931099  0.008636121  0.077552886  ## specify `pattern`: get_wordvecs(d, pattern=\"Chin[ae]|Japan|Korea\") #> 6 words matched... #>            China        Japan     Chinese     Japanese      Korean        Korea #>   1: -0.02697318  0.018329469 -0.04985132  0.003658937  0.02635512  0.033778005 #>   2:  0.04999036  0.090237829  0.04593451  0.091899336  0.11745195  0.133294179 #>   3:  0.04010034  0.060275982  0.07940573  0.062968012  0.03810029 -0.021963288 #>   4:  0.03056975  0.030490640  0.02581589  0.029782102  0.02850357  0.001675763 #>   5: -0.04711339 -0.095877693 -0.03204736 -0.093941412 -0.05127785 -0.014011015 #>  ---                                                                            #> 296: -0.01393625 -0.087418077  0.04593451 -0.055820224 -0.05213705 -0.072705831 #> 297: -0.06761304 -0.105042608 -0.06907951 -0.119128757 -0.08479435 -0.115117643 #> 298:  0.05070960  0.048291315  0.06694317  0.074540472  0.02363348  0.015374394 #> 299:  0.03452576  0.002103985  0.05448025  0.001999554 -0.05614764 -0.043017655 #> 300: -0.02931099  0.008636121 -0.03275947  0.009104821  0.03867319  0.077552886  ## plot word vectors: get_wordvecs(d, cc(\"China, Japan, Korea,                     Mac, Linux, Windows\"),              plot=TRUE, plot.dims=1:100)  #>            China        Japan        Korea          Mac        Linux #>   1: -0.02697318  0.018329469  0.033778005 -0.017270569  0.068922067 #>   2:  0.04999036  0.090237829  0.133294179 -0.012575617 -0.118698961 #>   3:  0.04010034  0.060275982 -0.021963288 -0.052985444 -0.071656892 #>   4:  0.03056975  0.030490640  0.001675763  0.078136677  0.030358376 #>   5: -0.04711339 -0.095877693 -0.014011015 -0.057009737 -0.045400893 #>  ---                                                                 #> 296: -0.01393625 -0.087418077 -0.072705831 -0.088532396 -0.001282133 #> 297: -0.06761304 -0.105042608 -0.115117643 -0.063716664  0.025572363 #> 298:  0.05070960  0.048291315  0.015374394 -0.005407505  0.072751102 #> 299:  0.03452576  0.002103985 -0.043017655 -0.016767489 -0.063451858 #> 300: -0.02931099  0.008636121  0.077552886  0.040912906  0.056341033 #>            Windows #>   1:  0.0596512714 #>   2: -0.0797136210 #>   3: -0.0818534443 #>   4: -0.0143777935 #>   5: -0.0738285592 #>  ---               #> 296:  0.0235396994 #> 297:  0.0005308471 #> 298:  0.0098973674 #> 299:  0.0572438332 #> 300:  0.1321425780  ## a more complex example:  words = cc(\" China Chinese Japan Japanese good bad great terrible morning evening king queen man woman he she cat dog \")  dt = get_wordvecs(   d, words,   plot=TRUE,   plot.dims=1:100,   plot.step=0.06)   # if you want to change something: attr(dt, \"ggplot\") +   scale_fill_viridis_b(n.breaks=10, show.limits=TRUE) +   theme(legend.key.height=unit(0.1, \"npc\")) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale.   # or to save the plot: ggsave(attr(dt, \"ggplot\"),        filename=\"wordvecs.png\",        width=8, height=5, dpi=500) unlink(\"wordvecs.png\")  # delete file for code check"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/most_similar.html","id":null,"dir":"Reference","previous_headings":"","what":"Find the Top-N most similar words. — most_similar","title":"Find the Top-N most similar words. — most_similar","text":"Find Top-N similar words, replicates results produced Python gensim module most_similar() function. (Exact replication gensim requires word vectors data, demodata used examples.)","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/most_similar.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find the Top-N most similar words. — most_similar","text":"","code":"most_similar(data, x, topn = 10, keep = FALSE, above = NULL, verbose = TRUE)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/most_similar.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find the Top-N most similar words. — most_similar","text":"data data.table (new class wordvec) loaded data_wordvec_load. x Can one following: single word:  \"China\" list words:  c(\"king\", \"queen\") cc(\" king , queen ; man | woman\") R formula (~ xxx) specifying   words positively negatively   contribute similarity (word analogy):  ~ boy - +  ~ king - man + woman  ~ Beijing - China + Japan topn Top-N similar words. Defaults 10. keep Keep words specified x results? Defaults FALSE. Defaults NULL. Can one following: threshold value find words cosine similarities   higher value critical word find words cosine similarities   higher critical word topn specified, wins. verbose Print information console? Defaults TRUE.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/most_similar.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find the Top-N most similar words. — most_similar","text":"data.table similar words cosine similarities. row number word raw data also returned, may help determine relative word frequency cases. Two attributes appended returned data.table (see examples): wordvec wordvec.formula. Users may extract use.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/most_similar.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Find the Top-N most similar words. — most_similar","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/most_similar.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find the Top-N most similar words. — most_similar","text":"","code":"d = data_wordvec_normalize(demodata) #> ✔ All word vectors have now been normalized.  most_similar(d, \"China\") #> [Word Vector] =~ China #> (normalized to unit length) #>          word   cos_sim row_id #>  1:   Chinese 0.7678081    924 #>  2:   Beijing 0.7648461   2086 #>  3:    Taiwan 0.7081156   3011 #>  4:  Shanghai 0.6727434   3932 #>  5:  Shenzhen 0.6239033   7984 #>  6: Guangzhou 0.6223897   7986 #>  7:      yuan 0.6005429   4619 #>  8:     India 0.6004212    485 #>  9:     Japan 0.5967756    821 #> 10:        Li 0.5882897   7558 most_similar(d, c(\"king\", \"queen\")) #> [Word Vector] =~ king + queen #> (normalized to unit length) #>          word   cos_sim row_id #>  1:     royal 0.5985594   6754 #>  2:     Queen 0.5487501   4267 #>  3:      King 0.4662653   1449 #>  4:    legend 0.3730853   5428 #>  5:     crown 0.3673733   5174 #>  6: superstar 0.3652350   7218 #>  7:  champion 0.3587134   1509 #>  8:      lady 0.3581903   4631 #>  9:     lover 0.3425792   7947 #> 10:      pope 0.3420032   6388 most_similar(d, cc(\" king , queen ; man | woman \")) #> [Word Vector] =~ king + queen + man + woman #> (normalized to unit length) #>            word   cos_sim row_id #>  1:        girl 0.6387542   1257 #>  2:         boy 0.5966774   1379 #>  3:    teenager 0.5941694   4345 #>  4:        lady 0.5923386   4631 #>  5: grandmother 0.5081522   5364 #>  6:      mother 0.5000261    754 #>  7:       lover 0.4826535   7947 #>  8:      victim 0.4806608   1537 #>  9:   boyfriend 0.4717627   4811 #> 10:  girlfriend 0.4709186   3922  # the same as above: most_similar(d, ~ China) #> [Word Vector] =~ China #> (normalized to unit length) #>          word   cos_sim row_id #>  1:   Chinese 0.7678081    924 #>  2:   Beijing 0.7648461   2086 #>  3:    Taiwan 0.7081156   3011 #>  4:  Shanghai 0.6727434   3932 #>  5:  Shenzhen 0.6239033   7984 #>  6: Guangzhou 0.6223897   7986 #>  7:      yuan 0.6005429   4619 #>  8:     India 0.6004212    485 #>  9:     Japan 0.5967756    821 #> 10:        Li 0.5882897   7558 most_similar(d, ~ king + queen) #> [Word Vector] =~ king + queen #> (normalized to unit length) #>          word   cos_sim row_id #>  1:     royal 0.5985594   6754 #>  2:     Queen 0.5487501   4267 #>  3:      King 0.4662653   1449 #>  4:    legend 0.3730853   5428 #>  5:     crown 0.3673733   5174 #>  6: superstar 0.3652350   7218 #>  7:  champion 0.3587134   1509 #>  8:      lady 0.3581903   4631 #>  9:     lover 0.3425792   7947 #> 10:      pope 0.3420032   6388 most_similar(d, ~ king + queen + man + woman) #> [Word Vector] =~ king + queen + man + woman #> (normalized to unit length) #>            word   cos_sim row_id #>  1:        girl 0.6387542   1257 #>  2:         boy 0.5966774   1379 #>  3:    teenager 0.5941694   4345 #>  4:        lady 0.5923386   4631 #>  5: grandmother 0.5081522   5364 #>  6:      mother 0.5000261    754 #>  7:       lover 0.4826535   7947 #>  8:      victim 0.4806608   1537 #>  9:   boyfriend 0.4717627   4811 #> 10:  girlfriend 0.4709186   3922  most_similar(d, ~ boy - he + she) #> [Word Vector] =~ boy - he + she #> (normalized to unit length) #>            word   cos_sim row_id #>  1:        girl 0.8635271   1257 #>  2:       woman 0.6822032    558 #>  3:      mother 0.6530156    754 #>  4:    daughter 0.6431009   1078 #>  5:    teenager 0.6310561   4345 #>  6:       child 0.5933921    702 #>  7: grandmother 0.5800967   5364 #>  8:        teen 0.5755065   3542 #>  9:        baby 0.5672891   1838 #> 10:       girls 0.5599151   1195 most_similar(d, ~ Jack - he + she) #> [Word Vector] =~ Jack - he + she #> (normalized to unit length) #>        word   cos_sim row_id #>  1:    Jane 0.6338590   5502 #>  2: Rebecca 0.6054167   6896 #>  3:   Julie 0.5988102   5721 #>  4:   Sarah 0.5867900   3717 #>  5:     Amy 0.5831094   4899 #>  6:   Susan 0.5812214   4141 #>  7:    Lisa 0.5766206   4521 #>  8:     Ann 0.5765654   4955 #>  9:   Alice 0.5649283   7340 #> 10:   Carol 0.5647291   5861 most_similar(d, ~ Rose - she + he) #> [Word Vector] =~ Rose - she + he #> (normalized to unit length) #>         word   cos_sim row_id #>  1:  Leonard 0.4443190   6245 #>  2:   Martin 0.4206247   1425 #>  3:   Thomas 0.4078561   1214 #>  4:  Wallace 0.3833057   4350 #>  5:  Francis 0.3793813   5220 #>  6:  Johnson 0.3757961    761 #>  7:    Allen 0.3736620   2257 #>  8: Robinson 0.3725750   2698 #>  9:    Evans 0.3639734   3379 #> 10:   Duncan 0.3635448   4738  most_similar(d, ~ king - man + woman) #> [Word Vector] =~ king - man + woman #> (normalized to unit length) #>         word   cos_sim row_id #>  1:    queen 0.7118192   7852 #>  2:    royal 0.4938203   6754 #>  3:    Queen 0.4346379   4267 #>  4:     King 0.3749903   1449 #>  5:      she 0.3341126     65 #>  6:     lady 0.3282869   4631 #>  7:   mother 0.3241257    754 #>  8:    crown 0.3164823   5174 #>  9:     hers 0.3073009   7981 #> 10: daughter 0.3021213   1078 most_similar(d, ~ Tokyo - Japan + China) #> [Word Vector] =~ Tokyo - Japan + China #> (normalized to unit length) #>          word   cos_sim row_id #>  1:   Beijing 0.8216199   2086 #>  2:  Shanghai 0.7951419   3932 #>  3: Guangzhou 0.6529652   7986 #>  4:   Chinese 0.6439487    924 #>  5:  Shenzhen 0.6439113   7984 #>  6:     Seoul 0.5868835   5180 #>  7:      yuan 0.5821307   4619 #>  8:        Li 0.5712056   7558 #>  9:      Wang 0.5192575   7083 #> 10:    Moscow 0.5082187   3163 most_similar(d, ~ Beijing - China + Japan) #> [Word Vector] =~ Beijing - China + Japan #> (normalized to unit length) #>          word   cos_sim row_id #>  1:     Tokyo 0.8115592   3017 #>  2:     Seoul 0.6568831   5180 #>  3:  Japanese 0.6475989   1562 #>  4: Pyongyang 0.5348969   6322 #>  5:   Bangkok 0.4677356   6510 #>  6:     Korea 0.4660699   3768 #>  7:       yen 0.4631333   2695 #>  8:    Taiwan 0.4330458   3011 #>  9:    Moscow 0.4217667   3163 #> 10: Guangzhou 0.4154183   7986  most_similar(d, \"China\", above=0.7) #> [Word Vector] =~ China #> (normalized to unit length) #>       word   cos_sim row_id #> 1: Chinese 0.7678081    924 #> 2: Beijing 0.7648461   2086 #> 3:  Taiwan 0.7081156   3011 most_similar(d, \"China\", above=\"Shanghai\") #> [Word Vector] =~ China #> (normalized to unit length) #>        word   cos_sim row_id #> 1:  Chinese 0.7678081    924 #> 2:  Beijing 0.7648461   2086 #> 3:   Taiwan 0.7081156   3011 #> 4: Shanghai 0.6727434   3932  # automatically normalized for more accurate results ms = most_similar(demodata, ~ king - man + woman) #> ! Results may be inaccurate if word vectors are not normalized. #> ✔ All word vectors have now been normalized. #> [Word Vector] =~ king - man + woman #> (normalized to unit length) ms #>         word   cos_sim row_id #>  1:    queen 0.7118192   7852 #>  2:    royal 0.4938203   6754 #>  3:    Queen 0.4346379   4267 #>  4:     King 0.3749903   1449 #>  5:      she 0.3341126     65 #>  6:     lady 0.3282869   4631 #>  7:   mother 0.3241257    754 #>  8:    crown 0.3164823   5174 #>  9:     hers 0.3073009   7981 #> 10: daughter 0.3021213   1078 str(ms) #> Classes ‘wordvec’, ‘data.table’ and 'data.frame':\t10 obs. of  3 variables: #>  $ word   : chr  \"queen\" \"royal\" \"Queen\" \"King\" ... #>  $ cos_sim: num  0.712 0.494 0.435 0.375 0.334 ... #>  $ row_id : int  7852 6754 4267 1449 65 4631 754 5174 7981 1078 #>  - attr(*, \".internal.selfref\")=<externalptr>  #>  - attr(*, \"dims\")= int 300 #>  - attr(*, \"normalized\")= logi TRUE #>  - attr(*, \"sum.vec\")= num [1:300] -0.0055 -0.06705 -0.04519 0.03875 -0.00286 ... #>   ..- attr(*, \"formula\")= chr \"king - man + woman\" #>   ..- attr(*, \"x.words\")= chr [1:3] \"king\" \"woman\" \"man\" #>  - attr(*, \"sum.vec.formula\")= chr \"king - man + woman\""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/orth_procrustes.html","id":null,"dir":"Reference","previous_headings":"","what":"Orthogonal Procrustes solution for matrix alignment. — orth_procrustes","title":"Orthogonal Procrustes solution for matrix alignment. — orth_procrustes","text":"order compare word embeddings different time periods, must ensure embedding matrices aligned semantic space (coordinate axes). Orthogonal Procrustes solution (Schönemann, 1966) commonly used align historical embeddings time (Hamilton et al., 2016; Li et al., 2020). function produces results cds::orthprocr(), psych::Procrustes(), pracma::procrustes().","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/orth_procrustes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Orthogonal Procrustes solution for matrix alignment. — orth_procrustes","text":"","code":"orth_procrustes(M, X)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/orth_procrustes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Orthogonal Procrustes solution for matrix alignment. — orth_procrustes","text":"M, X Two embedding matrices size (rows columns), two wordvec objects loaded data_wordvec_load transformed matrices as_wordvec. M reference (anchor/baseline/target) matrix,         e.g., embedding matrix learned         later year (\\(t + 1\\)). X matrix transformed/rotated. Note: function automatically extracts intersection (overlapped part) words M X sorts order (according M).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/orth_procrustes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Orthogonal Procrustes solution for matrix alignment. — orth_procrustes","text":"matrix wordvec object X rotation, depending class M X.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/orth_procrustes.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Orthogonal Procrustes solution for matrix alignment. — orth_procrustes","text":"Hamilton, W. L., Leskovec, J., & Jurafsky, D. (2016). Diachronic word embeddings reveal statistical laws semantic change. Proceedings 54th Annual Meeting Association Computational Linguistics (Vol. 1, pp. 1489--1501). Association Computational Linguistics. Li, Y., Hills, T., & Hertwig, R. (2020). brief history risk. Cognition, 203, 104344. Schönemann, P. H. (1966). generalized solution orthogonal Procrustes problem. Psychometrika, 31(1), 1--10.","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/orth_procrustes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Orthogonal Procrustes solution for matrix alignment. — orth_procrustes","text":"","code":"M = matrix(c(0,0,  1,2,  2,0,  3,2,  4,0), ncol=2, byrow=TRUE) X = matrix(c(0,0, -2,1,  0,2, -2,3,  0,4), ncol=2, byrow=TRUE) rownames(M) = rownames(X) = cc(\"A, B, C, D, E\")  # words colnames(M) = colnames(X) = cc(\"dim1, dim2\")  # dimensions  ggplot() +   geom_path(data=as.data.frame(M), aes(x=dim1, y=dim2),             color=\"red\") +   geom_path(data=as.data.frame(X), aes(x=dim1, y=dim2),             color=\"blue\") +   coord_equal()   # Usage 1: input two matrices XR = orth_procrustes(M, X) XR  # aligned with M #>   dim1          dim2 #> A    0  0.000000e+00 #> B    1  2.000000e+00 #> C    2 -6.661338e-16 #> D    3  2.000000e+00 #> E    4 -1.332268e-15  ggplot() +   geom_path(data=as.data.frame(XR), aes(x=dim1, y=dim2)) +   coord_equal()   # Usage 2: input two `wordvec` objects M.wv = as_wordvec(M) X.wv = as_wordvec(X) XR.wv = orth_procrustes(M.wv, X.wv) XR.wv  # aligned with M.wv #>    word                         vec #> 1:    A                         0,0 #> 2:    B                         1,2 #> 3:    C  2.000000e+00,-6.661338e-16 #> 4:    D                         3,2 #> 5:    E  4.000000e+00,-1.332268e-15  # M and X must have the same set and order of words # and the same number of word vector dimensions. # The function extracts only the intersection of words # and sorts them in the same order according to M.  Y = rbind(X, X[rev(rownames(X)),]) rownames(Y)[1:5] = cc(\"F, G, H, I, J\") M.wv = as_wordvec(M) Y.wv = as_wordvec(Y) M.wv  # words: A, B, C, D, E #>    word vec #> 1:    A 0,0 #> 2:    B 1,2 #> 3:    C 2,0 #> 4:    D 3,2 #> 5:    E 4,0 Y.wv  # words: F, G, H, I, J, E, D, C, B, A #>     word   vec #>  1:    F   0,0 #>  2:    G -2, 1 #>  3:    H   0,2 #>  4:    I -2, 3 #>  5:    J   0,4 #>  6:    E   0,4 #>  7:    D -2, 3 #>  8:    C   0,2 #>  9:    B -2, 1 #> 10:    A   0,0 YR.wv = orth_procrustes(M.wv, Y.wv) YR.wv  # aligned with M.wv, with the same order of words #>    word                         vec #> 1:    A                         0,0 #> 2:    B                         1,2 #> 3:    C  2.000000e+00,-6.661338e-16 #> 4:    D                         3,2 #> 5:    E  4.000000e+00,-1.332268e-15"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/pair_similarity.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute cosine similarity/distance for a pair of words. — pair_similarity","title":"Compute cosine similarity/distance for a pair of words. — pair_similarity","text":"Compute cosine similarity/distance pair words.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/pair_similarity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute cosine similarity/distance for a pair of words. — pair_similarity","text":"","code":"pair_similarity(data, word1, word2, distance = FALSE)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/pair_similarity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute cosine similarity/distance for a pair of words. — pair_similarity","text":"data data.table (new class wordvec) loaded data_wordvec_load. word1, word2 Word string (single word). distance Compute cosine distance instead? Defaults FALSE (cosine similarity).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/pair_similarity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute cosine similarity/distance for a pair of words. — pair_similarity","text":"value cosine similarity/distance.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/pair_similarity.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Compute cosine similarity/distance for a pair of words. — pair_similarity","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/pair_similarity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute cosine similarity/distance for a pair of words. — pair_similarity","text":"","code":"pair_similarity(demodata, \"China\", \"Chinese\") #> [1] 0.7678081"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_similarity.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize cosine similarity of word pairs. — plot_similarity","title":"Visualize cosine similarity of word pairs. — plot_similarity","text":"Visualize cosine similarity word pairs.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_similarity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize cosine similarity of word pairs. — plot_similarity","text":"","code":"plot_similarity(   data,   words = NULL,   pattern = NULL,   label = \"auto\",   value.color = NULL,   value.percent = FALSE,   order = c(\"original\", \"AOE\", \"FPC\", \"hclust\", \"alphabet\"),   hclust.method = c(\"complete\", \"ward\", \"ward.D\", \"ward.D2\", \"single\", \"average\",     \"mcquitty\", \"median\", \"centroid\"),   hclust.n = NULL,   hclust.color = \"black\",   hclust.line = 2,   file = NULL,   width = 8,   height = 6,   dpi = 500,   ... )"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_similarity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize cosine similarity of word pairs. — plot_similarity","text":"data data.table (new class wordvec) loaded data_wordvec_load. words [Option 1] Word strings (NULL; single word; vector words). pattern [Option 2] Pattern regular expression (see str_subset). label Position text labels. Defaults \"auto\" (add labels less 20 words). Can TRUE (left top), FALSE (add labels words), character string (see usage tl.pos corrplot. value.color Color values added plot. Defaults NULL (add values). value.percent Whether transform values percentage style space saving. Defaults FALSE. order Character, ordering method correlation matrix. 'original' original order (default). 'AOE' angular order eigenvectors. 'FPC' first principal component order. 'hclust' hierarchical clustering order. 'alphabet' alphabetical order. See function corrMatOrder details. hclust.method Character, agglomeration method used order hclust. one 'ward', 'ward.D', 'ward.D2', 'single', 'complete', 'average', 'mcquitty', 'median' 'centroid'. hclust.n Number rectangles drawn plot according hierarchical clusters, valid order=\"hclust\". Defaults NULL (add rectangles). hclust.color Color rectangle border, valid hclust.n >= 1. Defaults \"black\". hclust.line Line width rectangle border, valid hclust.n >= 1. Defaults 2. file File name saved, png pdf. width, height Width height (inches) saved file. Defaults 8 6. dpi Dots per inch. Defaults 500 (.e., file resolution: 4000 * 3000). ... parameters passed corrplot.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_similarity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize cosine similarity of word pairs. — plot_similarity","text":"Invisibly return matrix cosine similarity pair words.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_similarity.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Visualize cosine similarity of word pairs. — plot_similarity","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_similarity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize cosine similarity of word pairs. — plot_similarity","text":"","code":"w1 = cc(\"king, queen, man, woman\") plot_similarity(demodata, w1)  plot_similarity(demodata, w1,                 value.color=\"grey\",                 value.percent=TRUE)  plot_similarity(demodata, w1,                 value.color=\"grey\",                 order=\"hclust\",                 hclust.n=2)   w2 = cc(\"China, Chinese,          Japan, Japanese,          Korea, Korean,          man, woman, boy, girl,          good, bad, positive, negative\") plot_similarity(demodata, w2,                 order=\"hclust\",                 hclust.n=3)  plot_similarity(demodata, w2,                 order=\"hclust\",                 hclust.n=7,                 file=\"plot.png\") #> ✔ Saved to /tmp/Rtmpqlv4dr/file3d2d454ed8a6/reference/plot.png  unlink(\"plot.png\")  # delete file for code check"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvec.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize word vectors. — plot_wordvec","title":"Visualize word vectors. — plot_wordvec","text":"Visualize word vectors.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize word vectors. — plot_wordvec","text":"","code":"plot_wordvec(dt, dims = NULL, step = 0.05, border = \"white\")"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize word vectors. — plot_wordvec","text":"dt data.table returned get_wordvecs loaded data_wordvec_load. dims Dimensions plotted (e.g., 1:100). Defaults NULL (plot dimensions). step Step value breaks. Defaults 0.05. border Color tile border. Defaults \"white\". remove border color, set border=NA.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize word vectors. — plot_wordvec","text":"ggplot object.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvec.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Visualize word vectors. — plot_wordvec","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize word vectors. — plot_wordvec","text":"","code":"d = data_wordvec_normalize(demodata) #> ✔ All word vectors have now been normalized.  dt = get_wordvecs(d, cc(\"king, queen, man, woman\")) dt[, QUEEN := king - man + woman] #>              king        queen         man       woman        QUEEN #>   1:  0.043406531  0.001733313  0.14116230  0.09156568 -0.006190088 #>   2:  0.010262695 -0.047404412  0.05663379 -0.02905080 -0.075421897 #>   3:  0.002965276 -0.022895979  0.01500378 -0.03879578 -0.050834289 #>   4:  0.048116999  0.040793453 -0.03592460 -0.04045076  0.043590844 #>   5: -0.008832774  0.043534590  0.03888312  0.04449576 -0.003220136 #>  ---                                                                #> 296: -0.061239879 -0.013624785 -0.02768308  0.04063452  0.007077723 #> 297: -0.096234342  0.055466349 -0.03613580  0.04467952 -0.015419019 #> 298: -0.029610726 -0.015478958 -0.13101869 -0.03125744  0.070150521 #> 299:  0.031461353  0.053853896 -0.03465654  0.02463828  0.090756176 #> 300:  0.086812717  0.050951612  0.00908675 -0.01107800  0.066647966 dt[, QUEEN := QUEEN / sqrt(sum(QUEEN^2))]  # normalize #>              king        queen         man       woman        QUEEN #>   1:  0.043406531  0.001733313  0.14116230  0.09156568 -0.005502999 #>   2:  0.010262695 -0.047404412  0.05663379 -0.02905080 -0.067050205 #>   3:  0.002965276 -0.022895979  0.01500378 -0.03879578 -0.045191776 #>   4:  0.048116999  0.040793453 -0.03592460 -0.04045076  0.038752340 #>   5: -0.008832774  0.043534590  0.03888312  0.04449576 -0.002862707 #>  ---                                                                #> 296: -0.061239879 -0.013624785 -0.02768308  0.04063452  0.006292108 #> 297: -0.096234342  0.055466349 -0.03613580  0.04467952 -0.013707536 #> 298: -0.029610726 -0.015478958 -0.13101869 -0.03125744  0.062363942 #> 299:  0.031461353  0.053853896 -0.03465654  0.02463828  0.080682407 #> 300:  0.086812717  0.050951612  0.00908675 -0.01107800  0.059250164 names(dt)[5] = \"king - man + woman\" plot_wordvec(dt[, c(1,3,4,5,2)], dims=1:50)   dt = get_wordvecs(d, cc(\"boy, girl, he, she\")) dt[, GIRL := boy - he + she] #>               boy         girl           he         she         GIRL #>   1:  0.083967962  0.047750749  0.109257091  0.04049595  0.015206823 #>   2:  0.058881966  0.001287155  0.072653299 -0.01454260 -0.028313934 #>   3:  0.033273650  0.022193947 -0.010884081 -0.05615750 -0.011999766 #>   4: -0.045990576 -0.026229304 -0.016638191 -0.01778671 -0.047139098 #>   5:  0.005705206  0.008532822  0.017608756  0.05861853  0.046714981 #>  ---                                                                 #> 296: -0.019162800  0.023538951  0.061006509  0.03311285 -0.047056460 #> 297: -0.026479563  0.023034833  0.002339808  0.09754833  0.068728958 #> 298: -0.094768644 -0.029928238 -0.098165154  0.01073948  0.014135985 #> 299: -0.036757568 -0.003089446  0.069880173  0.11365708  0.007019343 #> 300:  0.048429498  0.012946613 -0.090954911 -0.05392007  0.085464341 dt[, GIRL := GIRL / sqrt(sum(GIRL^2))]  # normalize #>               boy         girl           he         she         GIRL #>   1:  0.083967962  0.047750749  0.109257091  0.04049595  0.011744750 #>   2:  0.058881966  0.001287155  0.072653299 -0.01454260 -0.021867820 #>   3:  0.033273650  0.022193947 -0.010884081 -0.05615750 -0.009267829 #>   4: -0.045990576 -0.026229304 -0.016638191 -0.01778671 -0.036407138 #>   5:  0.005705206  0.008532822  0.017608756  0.05861853  0.036079578 #>  ---                                                                 #> 296: -0.019162800  0.023538951  0.061006509  0.03311285 -0.036343314 #> 297: -0.026479563  0.023034833  0.002339808  0.09754833  0.053081725 #> 298: -0.094768644 -0.029928238 -0.098165154  0.01073948  0.010917705 #> 299: -0.036757568 -0.003089446  0.069880173  0.11365708  0.005421279 #> 300:  0.048429498  0.012946613 -0.090954911 -0.05392007  0.066007034 names(dt)[5] = \"boy - he + she\" plot_wordvec(dt[, c(1,3,4,5,2)], dims=1:50)   dt = get_wordvecs(d, cc(\"   male, man, boy, he, his,   female, woman, girl, she, her\"))  p = plot_wordvec(dt, dims=1:100)  # if you want to change something: p + theme(legend.key.height=unit(0.1, \"npc\"))   # or to save the plot: ggsave(p, filename=\"wordvecs.png\",        width=8, height=5, dpi=500) unlink(\"wordvecs.png\")  # delete file for code check"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvec_tSNE.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize word vectors with dimensionality reduced using t-SNE. — plot_wordvec_tSNE","title":"Visualize word vectors with dimensionality reduced using t-SNE. — plot_wordvec_tSNE","text":"Visualize word vectors dimensionality reduced using t-Distributed Stochastic Neighbor Embedding (t-SNE) method (.e., projecting high-dimensional vectors low-dimensional vector space), implemented Rtsne::Rtsne(). specify random seed expect reproducible results.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvec_tSNE.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize word vectors with dimensionality reduced using t-SNE. — plot_wordvec_tSNE","text":"","code":"plot_wordvec_tSNE(   dt,   dims = 2,   perplexity,   theta = 0.5,   colors = NULL,   seed = NULL,   custom.Rtsne = NULL )"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvec_tSNE.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize word vectors with dimensionality reduced using t-SNE. — plot_wordvec_tSNE","text":"dt data.table returned get_wordvecs loaded data_wordvec_load. dims Output dimensionality: 2 (default, common choice) 3. perplexity Perplexity parameter, larger (number words - 1) / 3. Defaults floor((length(dt)-1)/3) (columns dt words). See Rtsne package details. theta Speed/accuracy trade-(increase less accuracy), set 0 exact t-SNE. Defaults 0.5. colors character vector specifying (1) categories words (2-D plot ) (2) exact colors words (2-D 3-D plot). See examples usage. seed Random seed reproducible results. Defaults NULL. custom.Rtsne User-defined Rtsne object using dt.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvec_tSNE.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize word vectors with dimensionality reduced using t-SNE. — plot_wordvec_tSNE","text":"2-D: ggplot object. may extract data object using $data. 3-D: Nothing data invisibly returned, rgl::plot3d() \"called side effect drawing plot\" thus return 3-D plot object.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvec_tSNE.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Visualize word vectors with dimensionality reduced using t-SNE. — plot_wordvec_tSNE","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvec_tSNE.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Visualize word vectors with dimensionality reduced using t-SNE. — plot_wordvec_tSNE","text":"Hinton, G. E., & Salakhutdinov, R. R. (2006). Reducing dimensionality data neural networks. Science, 313(5786), 504--507. van der Maaten, L., & Hinton, G. (2008). Visualizing data using t-SNE. Journal Machine Learning Research, 9, 2579--2605.","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/plot_wordvec_tSNE.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize word vectors with dimensionality reduced using t-SNE. — plot_wordvec_tSNE","text":"","code":"d = data_wordvec_normalize(demodata) #> ✔ All word vectors have now been normalized.  dt = get_wordvecs(d, cc(\"   man, woman,   king, queen,   China, Beijing,   Japan, Tokyo\"))  ## 2-D (default): plot_wordvec_tSNE(dt, seed=1234)   plot_wordvec_tSNE(dt, seed=1234)$data #>      word         V1        V2 #> 1     man  -8.353420 106.35721 #> 2   woman -12.259647  90.98599 #> 3    king  44.206630  59.69335 #> 4   queen  28.452442  57.70974 #> 5   China   2.112208 -66.22363 #> 6 Beijing  11.349107 -79.93868 #> 7   Japan -32.370634 -75.89034 #> 8   Tokyo -33.136686 -92.69364  colors = c(rep(\"#2B579A\", 4), rep(\"#B7472A\", 4)) plot_wordvec_tSNE(dt, colors=colors, seed=1234)   category = c(rep(\"gender\", 4), rep(\"country\", 4)) plot_wordvec_tSNE(dt, colors=category, seed=1234) +   scale_x_continuous(limits=c(-200, 200),                      labels=function(x) x/100) +   scale_y_continuous(limits=c(-200, 200),                      labels=function(x) x/100) +   scale_color_manual(values=c(\"#B7472A\", \"#2B579A\"))   ## 3-D: colors = c(rep(\"#2B579A\", 4), rep(\"#B7472A\", 4)) plot_wordvec_tSNE(dt, dims=3, colors=colors, seed=1) #> Warning: RGL: unable to open X11 display #> Warning: 'rgl.init' failed, running with 'rgl.useNULL = TRUE'."},{"path":"https://psychbruce.github.io/PsychWordVec/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. bruceR cc","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/sum_wordvec.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the sum vector of multiple words. — sum_wordvec","title":"Calculate the sum vector of multiple words. — sum_wordvec","text":"Calculate sum vector multiple words.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/sum_wordvec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the sum vector of multiple words. — sum_wordvec","text":"","code":"sum_wordvec(data, x, verbose = TRUE)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/sum_wordvec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the sum vector of multiple words. — sum_wordvec","text":"data data.table (new class wordvec) loaded data_wordvec_load. x Can one following: single word:  \"China\" list words:  c(\"king\", \"queen\") cc(\" king , queen ; man | woman\") R formula (~ xxx) specifying   words positively negatively   contribute similarity (word analogy):  ~ boy - +  ~ king - man + woman  ~ Beijing - China + Japan verbose Print information console? Defaults TRUE.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/sum_wordvec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the sum vector of multiple words. — sum_wordvec","text":"Normalized sum vector.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/sum_wordvec.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Calculate the sum vector of multiple words. — sum_wordvec","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/sum_wordvec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the sum vector of multiple words. — sum_wordvec","text":"","code":"sum_wordvec(demodata, ~ king - man + woman) #> ! Results may be inaccurate if word vectors are not normalized. #> ✔ All word vectors have now been normalized. #>   [1] -0.0055029992 -0.0670502053 -0.0451917763  0.0387523404 -0.0028627068 #>   [6] -0.0311541918  0.0722744639 -0.0547962213 -0.0020958381  0.1267937027 #>  [11] -0.1177978229 -0.1116193754 -0.0432157999  0.0087648568 -0.0212594939 #>  [16] -0.0361886454 -0.0145757764 -0.0391255380  0.0236719225  0.0679695260 #>  [21]  0.0825006128  0.0179503178 -0.0102744498  0.0096805959  0.0097418516 #>  [26] -0.0583066547 -0.1043284916  0.0066451406  0.0774529645 -0.0630043398 #>  [31]  0.0176932143 -0.0357660002 -0.0127675640  0.0952755743 -0.1004202304 #>  [36] -0.0307725420  0.0191817489  0.0342936599 -0.0381161430 -0.0036010425 #>  [41] -0.0222736867 -0.0654523468  0.0581761410  0.0386649344  0.0625108822 #>  [46]  0.0144455335 -0.0298451219  0.0179820393 -0.0070950816  0.0410382461 #>  [51]  0.0193902936  0.0009054039 -0.0824461123  0.0896711960 -0.1071550823 #>  [56]  0.0090518215 -0.0452947722 -0.0242425101  0.0499748883  0.0017788248 #>  [61]  0.0896162255  0.0778694536  0.0043599153  0.0777055479  0.0090822422 #>  [66] -0.0272763870  0.0142941177  0.0595841030 -0.0188500428  0.0720441406 #>  [71]  0.0662750676  0.0333189751 -0.0275026411  0.0473774977  0.0302065682 #>  [76]  0.0363555415 -0.0305552844  0.0234168307  0.1017575023  0.0411732703 #>  [81]  0.0094623327  0.0278133126 -0.0067087575  0.0304040180 -0.0629132792 #>  [86]  0.0630248067 -0.0340541402  0.0388550926  0.0546983130  0.0218934638 #>  [91]  0.0433205797 -0.1001378189 -0.0860689768 -0.0450885649  0.0072867091 #>  [96]  0.0317569448  0.0645638429 -0.0160440819  0.0639956819 -0.0188932674 #> [101]  0.0053826412 -0.0191800667 -0.0261438897 -0.1247332173 -0.1010494892 #> [106]  0.0587875995 -0.1849463247 -0.0674423763  0.0657176846 -0.0205700926 #> [111]  0.0666514408  0.0815598320  0.0569012270 -0.0101953971  0.0484093475 #> [116] -0.1185521372  0.0030313936 -0.0933326263  0.0544090501  0.0927743854 #> [121]  0.0572259111  0.0394707134 -0.0271925329 -0.0265718034  0.0412028096 #> [126]  0.0438836936  0.0418592306  0.0034390046  0.0029262269  0.0134619940 #> [131]  0.0060545735  0.0513732397  0.0327279879  0.1054238844  0.0668562286 #> [136]  0.0769686804 -0.0227940919 -0.0422079731  0.0703340490 -0.0155774872 #> [141]  0.0372660027 -0.1356710009  0.0350093190  0.0236513594  0.0484942080 #> [146] -0.0400414708  0.0188626798  0.0274173715 -0.0486294492  0.0038126260 #> [151]  0.0133522101  0.0559058596  0.0022117502 -0.0837040985 -0.0849908637 #> [156] -0.0533659570 -0.0608346184  0.0319297479  0.0282798544  0.0078445029 #> [161] -0.0496067834 -0.0011413436  0.1585030627 -0.0257539005 -0.0556869160 #> [166] -0.0376501897  0.0119025195 -0.0722575073 -0.0134277815  0.0713891951 #> [171] -0.1004959954  0.0377733481  0.1079291741  0.0487607486  0.0176890786 #> [176] -0.0642323537  0.0939240962  0.0323515907  0.0062917470  0.0628913182 #> [181] -0.1168540632  0.0042852646 -0.1256198725 -0.0767676420  0.0159300785 #> [186]  0.0294246554  0.0809427655  0.0357663118 -0.0218166578  0.0638628914 #> [191]  0.0283086395 -0.0002039863 -0.0330815284  0.0067946163  0.0198031884 #> [196]  0.0572634491  0.0025753907 -0.0179017809  0.0275942861 -0.0879453025 #> [201] -0.0565598458  0.0206358787  0.0228473515 -0.0461282294 -0.0145729556 #> [206] -0.0143933569  0.0095678299  0.0631869552  0.0659021305  0.0042216349 #> [211]  0.0293649478  0.1033812547 -0.0516775577 -0.1253424596  0.0270994610 #> [216]  0.0385621533 -0.0036571112 -0.0278117809  0.0295294855  0.0024058853 #> [221] -0.0144733111  0.0180931805  0.0579378103 -0.0116669835  0.0087559466 #> [226] -0.0283575865 -0.0278965844 -0.0272467575  0.0342410954  0.0878225024 #> [231] -0.0741101358 -0.0684089597 -0.1740935244 -0.0044601583  0.0376029740 #> [236]  0.0221138776  0.0207988624 -0.0113099903 -0.0275133395 -0.0000199454 #> [241]  0.0563714279 -0.1243700668  0.0584321684  0.0294712221  0.0939427080 #> [246] -0.0011363452 -0.0734356129  0.0425710531  0.0715560074 -0.0782001568 #> [251]  0.0274794287  0.0326764105  0.0565690520 -0.0124150119  0.0545865281 #> [256]  0.0131866584 -0.1333997208 -0.0673208573 -0.0237475804 -0.0439831482 #> [261]  0.0048958304  0.0900008028 -0.0692655794  0.0177480897 -0.0185615675 #> [266] -0.0438003657  0.0037071559  0.0152207093  0.0955843067  0.1068395259 #> [271]  0.0044241142  0.1174178922 -0.0404572738  0.0672502162 -0.0424039690 #> [276] -0.1056443456 -0.0766069551  0.0035567801 -0.0748190562  0.0503630702 #> [281]  0.0909204843 -0.0668049051  0.0951348101 -0.0171847044  0.0625944510 #> [286] -0.0035357800 -0.0205028927 -0.0161002229 -0.0182009416  0.0601725192 #> [291] -0.0265177060 -0.0849906153 -0.0761131339 -0.1206830347  0.0004351351 #> [296]  0.0062921084 -0.0137075363  0.0623639418  0.0806824072  0.0592501645 #> attr(,\"formula\") #> [1] \"king - man + woman\" #> attr(,\"x.words\") #> [1] \"king\"  \"woman\" \"man\""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tab_similarity.html","id":null,"dir":"Reference","previous_headings":"","what":"Tabulate cosine similarity/distance of word pairs. — tab_similarity","title":"Tabulate cosine similarity/distance of word pairs. — tab_similarity","text":"Tabulate cosine similarity/distance word pairs.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tab_similarity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tabulate cosine similarity/distance of word pairs. — tab_similarity","text":"","code":"tab_similarity(   data,   words = NULL,   pattern = NULL,   unique = FALSE,   distance = FALSE )  tab_similarity_cross(data, words1, words2, distance = FALSE)"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tab_similarity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tabulate cosine similarity/distance of word pairs. — tab_similarity","text":"data data.table (new class wordvec) loaded data_wordvec_load. words [Option 1] Word strings (NULL; single word; vector words). pattern [Option 2] Pattern regular expression (see str_subset). unique Word pairs: unique pairs (TRUE) full pairs duplicates (FALSE; default). distance Compute cosine distance instead? Defaults FALSE (cosine similarity). words1, words2 [Used tab_similarity_cross] Two sets words computing similarities n1 * n2 word pairs.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tab_similarity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tabulate cosine similarity/distance of word pairs. — tab_similarity","text":"data.table words, word pairs, cosine similarity (cos_sim) cosine distance (cos_dist).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tab_similarity.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Tabulate cosine similarity/distance of word pairs. — tab_similarity","text":"tab_similarity(): Tabulate data word pairs. tab_similarity_cross(): Tabulate data n1 * n2 word pairs.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tab_similarity.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Tabulate cosine similarity/distance of word pairs. — tab_similarity","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tab_similarity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tabulate cosine similarity/distance of word pairs. — tab_similarity","text":"","code":"tab_similarity(demodata, cc(\"king, queen, man, woman\")) #>     word1 word2    wordpair   cos_sim #>  1:  king  king   king-king 1.0000000 #>  2:  king queen  king-queen 0.6510958 #>  3:  king   man    king-man 0.2294268 #>  4:  king woman  king-woman 0.1284797 #>  5: queen  king  queen-king 0.6510958 #>  6: queen queen queen-queen 1.0000000 #>  7: queen   man   queen-man 0.1665822 #>  8: queen woman queen-woman 0.3161813 #>  9:   man  king    man-king 0.2294268 #> 10:   man queen   man-queen 0.1665822 #> 11:   man   man     man-man 1.0000000 #> 12:   man woman   man-woman 0.7664012 #> 13: woman  king  woman-king 0.1284797 #> 14: woman queen woman-queen 0.3161813 #> 15: woman   man   woman-man 0.7664012 #> 16: woman woman woman-woman 1.0000000 tab_similarity(demodata, cc(\"king, queen, man, woman\"),                unique=TRUE) #>    word1 word2    wordpair   cos_sim #> 1:  king queen  king-queen 0.6510958 #> 2:  king   man    king-man 0.2294268 #> 3:  king woman  king-woman 0.1284797 #> 4: queen   man   queen-man 0.1665822 #> 5: queen woman queen-woman 0.3161813 #> 6:   man woman   man-woman 0.7664012  tab_similarity(demodata, cc(\"Beijing, China, Tokyo, Japan\")) #>       word1   word2        wordpair   cos_sim #>  1: Beijing Beijing Beijing-Beijing 1.0000000 #>  2: Beijing   China   Beijing-China 0.7648461 #>  3: Beijing   Tokyo   Beijing-Tokyo 0.5229628 #>  4: Beijing   Japan   Beijing-Japan 0.3995245 #>  5:   China Beijing   China-Beijing 0.7648461 #>  6:   China   China     China-China 1.0000000 #>  7:   China   Tokyo     China-Tokyo 0.3814305 #>  8:   China   Japan     China-Japan 0.5967756 #>  9:   Tokyo Beijing   Tokyo-Beijing 0.5229628 #> 10:   Tokyo   China     Tokyo-China 0.3814305 #> 11:   Tokyo   Tokyo     Tokyo-Tokyo 1.0000000 #> 12:   Tokyo   Japan     Tokyo-Japan 0.7002254 #> 13:   Japan Beijing   Japan-Beijing 0.3995245 #> 14:   Japan   China     Japan-China 0.5967756 #> 15:   Japan   Tokyo     Japan-Tokyo 0.7002254 #> 16:   Japan   Japan     Japan-Japan 1.0000000 tab_similarity(demodata, cc(\"Beijing, China, Tokyo, Japan\"),                unique=TRUE) #>      word1 word2      wordpair   cos_sim #> 1: Beijing China Beijing-China 0.7648461 #> 2: Beijing Tokyo Beijing-Tokyo 0.5229628 #> 3: Beijing Japan Beijing-Japan 0.3995245 #> 4:   China Tokyo   China-Tokyo 0.3814305 #> 5:   China Japan   China-Japan 0.5967756 #> 6:   Tokyo Japan   Tokyo-Japan 0.7002254  ## only n1 * n2 word pairs crossing two sets of words w1 & w2 w1 = cc(\"king, queen\") w2 = cc(\"man, woman\") tab_similarity_cross(demodata, w1, w2) #>    word1 word2    wordpair   cos_sim #> 1:  king   man    king-man 0.2294268 #> 2:  king woman  king-woman 0.1284797 #> 3: queen   man   queen-man 0.1665822 #> 4: queen woman queen-woman 0.3161813"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/test_RND.html","id":null,"dir":"Reference","previous_headings":"","what":"Relative Norm Distance (RND) analysis. — test_RND","title":"Relative Norm Distance (RND) analysis. — test_RND","text":"Tabulate data conduct permutation test significance Relative Norm Distance (RND; also known Relative Euclidean Distance). alternative method Single-Category WEAT.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/test_RND.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Relative Norm Distance (RND) analysis. — test_RND","text":"","code":"test_RND(   data,   T1,   A1,   A2,   use.pattern = FALSE,   labels,   p.perm = TRUE,   p.nsim = 10000,   p.side = 2,   seed = NULL )"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/test_RND.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Relative Norm Distance (RND) analysis. — test_RND","text":"data data.table (new class wordvec) loaded data_wordvec_load. T1 Target words single category (vector words pattern regular expression). A1, A2 Attribute words (vector words pattern regular expression). must specified. use.pattern Defaults FALSE (using vector words). use regular expression T1, T2, A1, A2, please specify argument TRUE. labels Labels target attribute concepts (named list), (default) list(T1=\"Target\", A1=\"Attrib1\", A2=\"Attrib2\"). p.perm Permutation test get exact approximate p value overall effect. Defaults TRUE. See also sweater package. p.nsim Number samples resampling permutation test. Defaults 10000. p.nsim larger number possible permutations (rearrangements data), ignored exact permutation test conducted. Otherwise (cases real data always SC-WEAT), resampling test performed, takes much less computation time produces approximate p value (comparable exact one). p.side One-sided (1) two-sided (2) p value. Defaults 2. Caliskan et al.'s (2017) article, reported one-sided p value WEAT. , suggest reporting two-sided p value conservative estimate. users take full responsibility choice. one-sided p value calculated proportion sampled permutations         difference means greater test statistic. two-sided p value calculated proportion sampled permutations         absolute difference greater test statistic. seed Random seed reproducible results permutation test. Defaults NULL.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/test_RND.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Relative Norm Distance (RND) analysis. — test_RND","text":"list object new class rnd: words.valid Valid (actually matched) words words..found Words found data.raw data.table (absolute relative) norm distances eff.label Description difference two attribute concepts eff.type Effect type: RND eff Raw effect p value (p.perm=TRUE) eff.interpretation Interpretation RND score","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/test_RND.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Relative Norm Distance (RND) analysis. — test_RND","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/test_RND.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Relative Norm Distance (RND) analysis. — test_RND","text":"Garg, N., Schiebinger, L., Jurafsky, D., & Zou, J. (2018). Word embeddings quantify 100 years gender ethnic stereotypes. Proceedings National Academy Sciences, 115(16), E3635--E3644. Bhatia, N., & Bhatia, S. (2021). Changes gender stereotypes time: computational analysis. Psychology Women Quarterly, 45(1), 106--125.","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/test_RND.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Relative Norm Distance (RND) analysis. — test_RND","text":"","code":"rnd = test_RND(   demodata,   labels=list(T1=\"Occupation\", A1=\"Male\", A2=\"Female\"),   T1=cc(\"     architect, boss, leader, engineer, CEO, officer, manager,     lawyer, scientist, doctor, psychologist, investigator,     consultant, programmer, teacher, clerk, counselor,     salesperson, therapist, psychotherapist, nurse\"),   A1=cc(\"male, man, boy, brother, he, him, his, son\"),   A2=cc(\"female, woman, girl, sister, she, her, hers, daughter\"),   seed=1) rnd #>  #> ── Relative Norm Distance (RND) ──────────────────────────────────────────────── #>  #> 21 Occupation (T1) words #> 8 Male (A1) words #> 8 Female (A2) words #>   #> Relative norm distances (differences): #> ───────────────────────────────────────────────────────────── #>                       rnd closer_to norm_dist_A1 norm_dist_A2 #> ───────────────────────────────────────────────────────────── #>  \"architect\"       -0.105      Male        1.138        1.243 #>  \"boss\"            -0.110      Male        1.028        1.138 #>  \"leader\"          -0.102      Male        1.105        1.206 #>  \"engineer\"        -0.093      Male        1.123        1.216 #>  \"CEO\"             -0.065      Male        1.235        1.300 #>  \"officer\"         -0.063      Male        1.098        1.161 #>  \"manager\"         -0.051      Male        1.171        1.222 #>  \"lawyer\"          -0.054      Male        1.048        1.102 #>  \"scientist\"       -0.040      Male        1.135        1.175 #>  \"doctor\"          -0.052      Male        0.965        1.017 #>  \"psychologist\"    -0.036      Male        1.080        1.115 #>  \"investigator\"    -0.035      Male        1.095        1.130 #>  \"consultant\"      -0.029      Male        1.172        1.202 #>  \"programmer\"      -0.027      Male        1.172        1.199 #>  \"teacher\"          0.029    Female        1.028        0.999 #>  \"clerk\"            0.039    Female        1.046        1.007 #>  \"counselor\"        0.025    Female        1.089        1.065 #>  \"salesperson\"      0.040    Female        1.123        1.083 #>  \"therapist\"        0.030    Female        1.059        1.029 #>  \"psychotherapist\"  0.050    Female        1.115        1.066 #>  \"nurse\"            0.125    Female        1.053        0.927 #> ───────────────────────────────────────────────────────────── #> If RND < 0: Occupation is more associated with Male than Female #> If RND > 0: Occupation is more associated with Female than Male #>  #> Overall effect (raw): #> ────────────────────────────────────────── #>      Target       Attrib rnd_sum     p     #> ────────────────────────────────────────── #>  Occupation  Male/Female  -0.523  .076 .   #> ────────────────────────────────────────── #> Permutation test: approximate p value = 7.57e-02 (two-sided) #>"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/test_WEAT.html","id":null,"dir":"Reference","previous_headings":"","what":"Word Embedding Association Test (WEAT) and Single-Category WEAT. — test_WEAT","title":"Word Embedding Association Test (WEAT) and Single-Category WEAT. — test_WEAT","text":"Tabulate data (cosine similarity standardized effect size) conduct permutation test significance Word Embedding Association Test (WEAT) Single-Category Word Embedding Association Test (SC-WEAT). WEAT, two-samples permutation test conducted (.e., rearrangements data). SC-WEAT, one-sample permutation test conducted (.e., rearrangements +/- signs data).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/test_WEAT.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Word Embedding Association Test (WEAT) and Single-Category WEAT. — test_WEAT","text":"","code":"test_WEAT(   data,   T1,   T2,   A1,   A2,   use.pattern = FALSE,   labels = list(),   p.perm = TRUE,   p.nsim = 10000,   p.side = 2,   seed = NULL,   pooled.sd = \"Caliskan\" )"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/test_WEAT.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Word Embedding Association Test (WEAT) and Single-Category WEAT. — test_WEAT","text":"data data.table (new class wordvec) loaded data_wordvec_load. T1, T2 Target words (vector words pattern regular expression). T1 specified, tabulate data single-category WEAT (SC-WEAT). A1, A2 Attribute words (vector words pattern regular expression). must specified. use.pattern Defaults FALSE (using vector words). use regular expression T1, T2, A1, A2, please specify argument TRUE. labels Labels target attribute concepts (named list), (default) list(T1=\"Target1\", T2=\"Target2\", A1=\"Attrib1\", A2=\"Attrib2\"). p.perm Permutation test get exact approximate p value overall effect. Defaults TRUE. See also sweater package. p.nsim Number samples resampling permutation test. Defaults 10000. p.nsim larger number possible permutations (rearrangements data), ignored exact permutation test conducted. Otherwise (cases real data always SC-WEAT), resampling test performed, takes much less computation time produces approximate p value (comparable exact one). p.side One-sided (1) two-sided (2) p value. Defaults 2. Caliskan et al.'s (2017) article, reported one-sided p value WEAT. , suggest reporting two-sided p value conservative estimate. users take full responsibility choice. one-sided p value calculated proportion sampled permutations         difference means greater test statistic. two-sided p value calculated proportion sampled permutations         absolute difference greater test statistic. seed Random seed reproducible results permutation test. Defaults NULL. pooled.sd Method used calculate pooled SD effect size estimate WEAT. Defaults \"Caliskan\": sd(data.diff$cos_sim_diff), highly suggested         identical Caliskan et al.'s (2017) original approach. Otherwise specified, calculate pooled SD :         \\(\\sqrt{[(n_1 - 1) * \\sigma_1^2 + (n_2 - 1) * \\sigma_2^2] / (n_1 + n_2 - 2)}\\).        suggested may overestimate effect size,         especially T1 T2 words small variances.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/test_WEAT.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Word Embedding Association Test (WEAT) and Single-Category WEAT. — test_WEAT","text":"list object new class weat: words.valid Valid (actually matched) words words..found Words found data.raw data.table cosine similarities word pairs data.mean data.table mean cosine similarities     across attribute words data.diff data.table differential mean cosine similarities     two attribute concepts eff.label Description difference two attribute concepts eff.type Effect type: WEAT SC-WEAT eff Raw effect, standardized effect size, p value (p.perm=TRUE)","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/test_WEAT.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Word Embedding Association Test (WEAT) and Single-Category WEAT. — test_WEAT","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/test_WEAT.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Word Embedding Association Test (WEAT) and Single-Category WEAT. — test_WEAT","text":"Caliskan, ., Bryson, J. J., & Narayanan, . (2017). Semantics derived automatically language corpora contain human-like biases. Science, 356(6334), 183--186.","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/test_WEAT.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Word Embedding Association Test (WEAT) and Single-Category WEAT. — test_WEAT","text":"","code":"## Remember: cc() is more convenient than c()!  weat = test_WEAT(   demodata,   labels=list(T1=\"King\", T2=\"Queen\", A1=\"Male\", A2=\"Female\"),   T1=cc(\"king, King\"),   T2=cc(\"queen, Queen\"),   A1=cc(\"male, man, boy, brother, he, him, his, son\"),   A2=cc(\"female, woman, girl, sister, she, her, hers, daughter\"),   seed=1) weat #>  #> ── WEAT (Word Embedding Association Test) ────────────────────────────────────── #>  #> 2 King (T1) words #> 2 Queen (T2) words #> 8 Male (A1) words #> 8 Female (A2) words #>   #> Relative semantic similarities (differences): #> ─────────────────────────────────────────────── #>          Target cos_sim_diff std_diff closer_to #> ─────────────────────────────────────────────── #>  \"king\"    King        0.104    1.587      Male #>  \"King\"    King        0.090    1.598      Male #>  \"queen\"  Queen       -0.176   -1.751    Female #>  \"Queen\"  Queen       -0.129   -1.781    Female #> ─────────────────────────────────────────────── #>  #> Mean differences for single target category: #> ───────────────────────────────────────────── #>  Target mean_raw_diff mean_std_diff     p     #> ───────────────────────────────────────────── #>    King         0.097         1.592 <.001 *** #>   Queen        -0.152        -1.766 <.001 *** #> ───────────────────────────────────────────── #> Permutation test: approximate p values (forced to two-sided) #>  #> Overall effect (raw and standardized mean differences): #> ───────────────────────────────────────────────────────── #>      Target       Attrib mean_diff_raw eff_size     p     #> ───────────────────────────────────────────────────────── #>  King/Queen  Male/Female         0.249    1.716 <.001 *** #> ───────────────────────────────────────────────────────── #> Permutation test: exact p value = 0.00e+00 (two-sided) #>   sc_weat = test_WEAT(   demodata,   labels=list(T1=\"Occupation\", A1=\"Male\", A2=\"Female\"),   T1=cc(\"     architect, boss, leader, engineer, CEO, officer, manager,     lawyer, scientist, doctor, psychologist, investigator,     consultant, programmer, teacher, clerk, counselor,     salesperson, therapist, psychotherapist, nurse\"),   A1=cc(\"male, man, boy, brother, he, him, his, son\"),   A2=cc(\"female, woman, girl, sister, she, her, hers, daughter\"),   seed=1) sc_weat #>  #> ── SC-WEAT (Single-Category Word Embedding Association Test) ─────────────────── #>  #> 21 Occupation (T1) words #> 8 Male (A1) words #> 8 Female (A2) words #>   #> Relative semantic similarities (differences): #> ────────────────────────────────────────────────── #>                    cos_sim_diff std_diff closer_to #> ────────────────────────────────────────────────── #>  \"architect\"              0.087    1.048      Male #>  \"boss\"                   0.081    1.027      Male #>  \"leader\"                 0.079    0.985      Male #>  \"engineer\"               0.071    0.775      Male #>  \"CEO\"                    0.045    0.637      Male #>  \"officer\"                0.033    0.448      Male #>  \"manager\"                0.022    0.342      Male #>  \"lawyer\"                 0.020    0.238      Male #>  \"scientist\"              0.008    0.170      Male #>  \"doctor\"                 0.013    0.169      Male #>  \"psychologist\"           0.001    0.030      Male #>  \"investigator\"           0.001    0.012      Male #>  \"consultant\"            -0.003   -0.055    Female #>  \"programmer\"            -0.006   -0.144    Female #>  \"teacher\"               -0.067   -0.800    Female #>  \"clerk\"                 -0.078   -1.035    Female #>  \"counselor\"             -0.064   -1.196    Female #>  \"salesperson\"           -0.083   -1.207    Female #>  \"therapist\"             -0.069   -1.245    Female #>  \"psychotherapist\"       -0.092   -1.323    Female #>  \"nurse\"                 -0.162   -1.491    Female #> ────────────────────────────────────────────────── #>  #> Overall effect (raw and standardized mean differences): #> ───────────────────────────────────────────────────────── #>      Target       Attrib mean_diff_raw eff_size     p     #> ───────────────────────────────────────────────────────── #>  Occupation  Male/Female        -0.008   -0.124  .599     #> ───────────────────────────────────────────────────────── #> Permutation test: approximate p value = 5.99e-01 (two-sided) #>   if (FALSE) {  ## the same as the first example, but using regular expression weat = test_WEAT(   demodata,   labels=list(T1=\"King\", T2=\"Queen\", A1=\"Male\", A2=\"Female\"),   use.pattern=TRUE,  # use regular expression below   T1=\"^[kK]ing$\",   T2=\"^[qQ]ueen$\",   A1=\"^male$|^man$|^boy$|^brother$|^he$|^him$|^his$|^son$\",   A2=\"^female$|^woman$|^girl$|^sister$|^she$|^her$|^hers$|^daughter$\",   seed=1) weat  ## replicating Caliskan et al.'s (2017) results ## WEAT7 (Table 1): d = 1.06, p = .018 ## (requiring installation of the `sweater` package) Caliskan.WEAT7 = test_WEAT(   as_wordvec(sweater::glove_math),   labels=list(T1=\"Math\", T2=\"Arts\", A1=\"Male\", A2=\"Female\"),   T1=cc(\"math, algebra, geometry, calculus, equations, computation, numbers, addition\"),   T2=cc(\"poetry, art, dance, literature, novel, symphony, drama, sculpture\"),   A1=cc(\"male, man, boy, brother, he, him, his, son\"),   A2=cc(\"female, woman, girl, sister, she, her, hers, daughter\"),   p.side=1, seed=1234) Caliskan.WEAT7 # d = 1.055, p = .0173 (= 173 counts / 10000 permutation samples)  ## replicating Caliskan et al.'s (2017) supplemental results ## WEAT7 (Table S1): d = 0.97, p = .027 Caliskan.WEAT7.supp = test_WEAT(   demodata,   labels=list(T1=\"Math\", T2=\"Arts\", A1=\"Male\", A2=\"Female\"),   T1=cc(\"math, algebra, geometry, calculus, equations, computation, numbers, addition\"),   T2=cc(\"poetry, art, dance, literature, novel, symphony, drama, sculpture\"),   A1=cc(\"male, man, boy, brother, he, him, his, son\"),   A2=cc(\"female, woman, girl, sister, she, her, hers, daughter\"),   p.side=1, seed=1234) Caliskan.WEAT7.supp # d = 0.966, p = .0221 (= 221 counts / 10000 permutation samples) }"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tokenize.html","id":null,"dir":"Reference","previous_headings":"","what":"Tokenize raw texts for training word vectors. — tokenize","title":"Tokenize raw texts for training word vectors. — tokenize","text":"Tokenize raw texts training word vectors.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tokenize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tokenize raw texts for training word vectors. — tokenize","text":"","code":"tokenize(   text,   tokenizer = text2vec::word_tokenizer,   split = \" \",   remove = \"_|'|<br/>|<br />|e\\\\.g\\\\.|i\\\\.e\\\\.\",   encoding = \"UTF-8\",   simplify = TRUE,   verbose = TRUE )"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tokenize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tokenize raw texts for training word vectors. — tokenize","text":"text character vector text, file path disk containing text. tokenizer Function used tokenize text. Defaults text2vec::word_tokenizer. split Separator tokens, used simplify=TRUE. Defaults \" \". remove Strings (regular expression) removed text. Defaults \"_|'|<br/>|<br />|e\\\\.g\\\\.|\\\\.e\\\\.\". may turn specifying remove=NULL. encoding Text encoding. Defaults \"UTF-8\". simplify Return character vector (TRUE) list character vectors (FALSE). Defaults TRUE. verbose Print information console? Defaults TRUE.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tokenize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tokenize raw texts for training word vectors. — tokenize","text":"simplify=TRUE: tokenized character vector,   element sentence. simplify=FALSE: list tokenized character vectors,   element vector tokens sentence.","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/tokenize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tokenize raw texts for training word vectors. — tokenize","text":"","code":"txt1 = c(   \"I love natural language processing (NLP)!\",   \"I've been in this city for 10 years. I really like here!\",   \"However, my computer is not among the \\\"Top 10\\\" list.\" ) tokenize(txt1, simplify=FALSE) #> ✔ Tokenized: 4 sentences (time cost = 0.4 secs) #> [[1]] #> [1] \"I\"          \"love\"       \"natural\"    \"language\"   \"processing\" #> [6] \"NLP\"        #>  #> [[2]] #> [1] \"Ive\"   \"been\"  \"in\"    \"this\"  \"city\"  \"for\"   \"10\"    \"years\" #>  #> [[3]] #> [1] \"I\"      \"really\" \"like\"   \"here\"   #>  #> [[4]] #>  [1] \"However\"  \"my\"       \"computer\" \"is\"       \"not\"      \"among\"    #>  [7] \"the\"      \"Top\"      \"10\"       \"list\"     #>  tokenize(txt1) %>% cat(sep=\"\\n----\\n\") #> ✔ Tokenized: 4 sentences (time cost = 0.3 secs) #> I love natural language processing NLP #> ---- #> Ive been in this city for 10 years #> ---- #> I really like here #> ---- #> However my computer is not among the Top 10 list  txt2 = text2vec::movie_review$review[1:5] texts = tokenize(txt2) #> ✔ Tokenized: 81 sentences (time cost = 0.3 secs)  txt2[1] #> [1] \"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\" texts[1:20]  # all sentences in txt2[1] #>  [1] \"With all this stuff going down at the moment with MJ ive started listening to his music watching the odd documentary here and there watched The Wiz and watched Moonwalker again\"                                       #>  [2] \"Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent\"                                                  #>  [3] \"Moonwalker is part biography part feature film which i remember going to see at the cinema when it was originally released\"                                                                                             #>  [4] \"Some of it has subtle messages about MJs feeling towards the press and also the obvious message of drugs are bad mkay\"                                                                                                  #>  [5] \"Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring\"                                                        #>  [6] \"Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him\"                                          #>  [7] \"The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord\"                         #>  [8] \"Why he wants MJ dead so bad is beyond me\"                                                                                                                                                                               #>  [9] \"Because MJ overheard his plans\"                                                                                                                                                                                         #> [10] \"Nah Joe Pescis character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno maybe he just hates MJs music\"                                                                             #> [11] \"Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence\"                                                                                                                  #> [12] \"Also the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene\" #> [13] \"Bottom line this movie is for people who like MJ on one level or another which i think is most people\"                                                                                                                  #> [14] \"If not then stay away\"                                                                                                                                                                                                  #> [15] \"It does try and give off a wholesome message and ironically MJs bestest buddy in this movie is a girl\"                                                                                                                  #> [16] \"Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty\"                                                                                                                    #> [17] \"Well with all the attention ive gave this subject\"                                                                                                                                                                      #> [18] \"hmmm well i dont know because people can be different behind closed doors i know this for a fact\"                                                                                                                       #> [19] \"He is either an extremely nice but stupid guy or one of the most sickest liars\"                                                                                                                                         #> [20] \"I hope he is not the latter\""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/train_wordvec.html","id":null,"dir":"Reference","previous_headings":"","what":"Train word vectors using the Word2Vec, GloVe, or FastText algorithm. — train_wordvec","title":"Train word vectors using the Word2Vec, GloVe, or FastText algorithm. — train_wordvec","text":"Train word vectors using Word2Vec, GloVe, FastText algorithm multi-threading.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/train_wordvec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Train word vectors using the Word2Vec, GloVe, or FastText algorithm. — train_wordvec","text":"","code":"train_wordvec(   text,   method = c(\"word2vec\", \"glove\", \"fasttext\"),   dims = 300,   window = 5,   min.freq = 5,   threads = 8,   model = c(\"skip-gram\", \"cbow\"),   loss = c(\"ns\", \"hs\"),   negative = 5,   subsample = 1e-04,   learning = 0.05,   ngrams = c(3, 6),   x.max = 10,   convergence = -1,   stopwords = character(0),   encoding = \"UTF-8\",   tolower = FALSE,   normalize = FALSE,   iteration,   tokenizer,   remove,   file.save,   compress = \"bzip2\",   verbose = TRUE )"},{"path":"https://psychbruce.github.io/PsychWordVec/reference/train_wordvec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Train word vectors using the Word2Vec, GloVe, or FastText algorithm. — train_wordvec","text":"text character vector text, file path disk containing text. method Training algorithm: \"word2vec\" (default): using word2vec package \"glove\": using rsparse text2vec packages \"fasttext\": using fastTextR package dims Number dimensions word vectors trained. Common choices include 50, 100, 200, 300, 500. Defaults 300. window Window size (number nearby words behind/ahead current word). defines many surrounding words included training: [window] words behind [window] words ahead ([window]*2 total). Defaults 5. min.freq Minimum frequency words included training. Words appear less value times excluded vocabulary. Defaults 5 (take words appear least five times). threads Number CPU threads used training. modest value produces fastest training. many threads always helpful. Defaults 8. model <Word2Vec / FastText> Learning model architecture: \"skip-gram\" (default): Skip-Gram, predicts surrounding words given current word \"cbow\": Continuous Bag--Words, predicts current word based context loss <Word2Vec / FastText> Loss function (computationally efficient approximation): \"ns\" (default): Negative Sampling \"hs\": Hierarchical Softmax negative <Negative Sampling Word2Vec / FastText> Number negative examples. Values range 5~20 useful small training datasets, large datasets value can small 2~5. Defaults 5. subsample <Word2Vec / FastText> Subsampling frequent words (threshold occurrence words). appear higher frequency training data randomly -sampled. Defaults 0.0001 (1e-04). learning <Word2Vec / FastText> Initial (starting) learning rate, also known alpha. Defaults 0.05. ngrams <FastText> Minimal maximal ngram length. Defaults c(3, 6). x.max <GloVe> Maximum number co-occurrences use weighting function. Defaults 10. convergence <GloVe> Convergence tolerance SGD iterations. Defaults -1. stopwords <Word2Vec / GloVe> character vector stopwords excluded training. encoding Text encoding. Defaults \"UTF-8\". tolower Convert upper-case characters lower-case? Defaults FALSE. normalize Normalize word vectors unit length? Defaults FALSE. See data_wordvec_normalize. iteration Number training iterations. iterations makes precise model, computational cost linearly proportional iterations. Defaults 5 Word2Vec FastText 10 GloVe. tokenizer Function used tokenize text. Defaults text2vec::word_tokenizer. remove Strings (regular expression) removed text. Defaults \"_|'|<br/>|<br />|e\\\\.g\\\\.|\\\\.e\\\\.\". may turn specifying remove=NULL. file.save File name --saved R data (must .RData). compress Compression method saved file. Defaults \"bzip2\". Options include: 1 \"gzip\": modest file size (fastest) 2 \"bzip2\": small file size (fast) 3 \"xz\": minimized file size (slow) verbose Print information console? Defaults TRUE.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/train_wordvec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Train word vectors using the Word2Vec, GloVe, or FastText algorithm. — train_wordvec","text":"data.table (new class wordvec) two variables: word words (tokens) vec raw normalized word vectors","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/train_wordvec.html","id":"download","dir":"Reference","previous_headings":"","what":"Download","title":"Train word vectors using the Word2Vec, GloVe, or FastText algorithm. — train_wordvec","text":"Download pre-trained word vectors data (.RData): https://psychbruce.github.io/WordVector_RData.pdf","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/reference/train_wordvec.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Train word vectors using the Word2Vec, GloVe, or FastText algorithm. — train_wordvec","text":"--one package: https://CRAN.R-project.org/package=wordsalad Word2Vec: https://code.google.com/archive/p/word2vec/ https://CRAN.R-project.org/package=word2vec https://github.com/maxoodf/word2vec GloVe: https://nlp.stanford.edu/projects/glove/ https://text2vec.org/glove.html https://CRAN.R-project.org/package=text2vec https://CRAN.R-project.org/package=rsparse FastText: https://fasttext.cc/ https://CRAN.R-project.org/package=fastTextR","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/reference/train_wordvec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Train word vectors using the Word2Vec, GloVe, or FastText algorithm. — train_wordvec","text":"","code":"review = text2vec::movie_review  # a data.frame' text = review$review  ## Note: All the examples train 50 dims for faster code check.  ## Word2Vec (SGNS) dt1 = train_wordvec(   text,   method=\"word2vec\",   model=\"skip-gram\",   dims=50, window=5,   normalize=TRUE) #> ✔ Tokenized: 70105 sentences (time cost = 3 secs) #> ✔ Text corpus: 5242249 characters, 1185427 tokens (roughly words) #>  #> ── Training model information ────────────────────────────────────────────────── #> - Method:      Word2Vec (Skip-Gram with Negative Sampling) #> - Dimensions:  50 #> - Window size: 5 (5 words behind and 5 words ahead the current word) #> - Subsampling: 1e-04 #> - Min. freq.:  5 occurrences in text #> - Iterations:  5 training iterations #> - CPU threads: 8 #>  #> ── Training...  #> ✔ Word vectors trained: 14205 unique tokens (time cost = 12 secs) #> ✔ All word vectors have now been normalized.  as_tibble(dt1)  # just check #> # A tibble: 14,205 × 3 #>    word  vec         freq #>    <chr> <list>     <int> #>  1 the   <dbl [50]> 58797 #>  2 and   <dbl [50]> 32193 #>  3 a     <dbl [50]> 31783 #>  4 of    <dbl [50]> 29142 #>  5 to    <dbl [50]> 27218 #>  6 is    <dbl [50]> 21447 #>  7 in    <dbl [50]> 17861 #>  8 I     <dbl [50]> 14703 #>  9 that  <dbl [50]> 13618 #> 10 it    <dbl [50]> 13453 #> # … with 14,195 more rows most_similar(dt1, \"Ive\")  # evaluate performance #> [Word Vector] =~ Ive #> (normalized to unit length) #>           word   cos_sim row_id #>  1:        ive 0.8887614   3487 #>  2:       seen 0.8210021    110 #>  3:  Bollywood 0.8104229   3585 #>  4:       ever 0.8068716    124 #>  5:     lately 0.8057448   5137 #>  6:       Weve 0.8000092   6928 #>  7:      Hands 0.7974401   7224 #>  8: criticized 0.7969465   8457 #>  9:      WORST 0.7914292   6904 #> 10:      Youve 0.7896730   5989 most_similar(dt1, ~ man - he + she, topn=5)  # evaluate performance #> [Word Vector] =~ man - he + she #> (normalized to unit length) #>      word   cos_sim row_id #> 1:  woman 0.8553832    260 #> 2:   girl 0.7527926    299 #> 3:   lady 0.7464193   1140 #> 4: lonely 0.7252392   2338 #> 5:  young 0.7112162    197 most_similar(dt1, ~ boy - he + she, topn=5)  # evaluate performance #> [Word Vector] =~ boy - he + she #> (normalized to unit length) #>     word   cos_sim row_id #> 1:   kid 0.7647311    674 #> 2:  girl 0.7611924    299 #> 3: woman 0.7435880    260 #> 4: witch 0.7084293   2513 #> 5:  aged 0.7014026   1839  ## GloVe dt2 = train_wordvec(   text,   method=\"glove\",   dims=50, window=5,   normalize=TRUE) #> ✔ Tokenized: 70105 sentences (time cost = 3 secs) #> ✔ Text corpus: 5242249 characters, 1185427 tokens (roughly words) #>  #> ── Training model information ────────────────────────────────────────────────── #> - Method:      GloVe #> - Dimensions:  50 #> - Window size: 5 (5 words behind and 5 words ahead the current word) #> - Subsampling: N/A #> - Min. freq.:  5 occurrences in text #> - Iterations:  10 training iterations #> - CPU threads: 8 #>  #> ── Training...  #> ✔ Word vectors trained: 14207 unique tokens (time cost = 16 secs) #> ✔ All word vectors have now been normalized.  as_tibble(dt2)  # just check #> # A tibble: 14,207 × 3 #>    word  vec         freq #>    <chr> <list>     <int> #>  1 the   <dbl [50]> 58797 #>  2 and   <dbl [50]> 32193 #>  3 a     <dbl [50]> 31783 #>  4 of    <dbl [50]> 29142 #>  5 to    <dbl [50]> 27218 #>  6 is    <dbl [50]> 21447 #>  7 in    <dbl [50]> 17861 #>  8 I     <dbl [50]> 14703 #>  9 that  <dbl [50]> 13618 #> 10 it    <dbl [50]> 13453 #> # … with 14,197 more rows most_similar(dt2, \"Ive\")  # evaluate performance #> [Word Vector] =~ Ive #> (normalized to unit length) #>        word   cos_sim row_id #>  1:    seen 0.9411882    110 #>  2:    ever 0.8944266    124 #>  3:   worst 0.7543924    261 #>  4:   heard 0.7518399    515 #>  5:   since 0.7300758    262 #>  6:   youve 0.7200879    950 #>  7: watched 0.7051912    305 #>  8:  movies 0.6895728     91 #>  9: already 0.6770569    468 #> 10:    been 0.6765670     74 most_similar(dt2, ~ man - he + she, topn=5)  # evaluate performance #> [Word Vector] =~ man - he + she #> (normalized to unit length) #>     word   cos_sim row_id #> 1: woman 0.8718137    260 #> 2: young 0.7610926    198 #> 3:  girl 0.7610592    299 #> 4:   hit 0.7481330    594 #> 5: child 0.7409089    523 most_similar(dt2, ~ boy - he + she, topn=5)  # evaluate performance #> [Word Vector] =~ boy - he + she #> (normalized to unit length) #>     word   cos_sim row_id #> 1:  girl 0.8163652    299 #> 2: young 0.7775646    198 #> 3: woman 0.7455849    260 #> 4: named 0.7199464    867 #> 5:   man 0.7007369    153  ## FastText dt3 = train_wordvec(   text,   method=\"fasttext\",   model=\"skip-gram\",   dims=50, window=5,   normalize=TRUE) #> ✔ Tokenized: 70105 sentences (time cost = 3 secs) #> ✔ Text corpus: 5242249 characters, 1185427 tokens (roughly words) #>  #> ── Training model information ────────────────────────────────────────────────── #> - Method:      FastText (Skip-Gram with Negative Sampling) #> - Dimensions:  50 #> - Window size: 5 (5 words behind and 5 words ahead the current word) #> - Subsampling: 1e-04 #> - Min. freq.:  5 occurrences in text #> - Iterations:  5 training iterations #> - CPU threads: 8 #>  #> ── Training...  #> ✔ Word vectors trained: 14207 unique tokens (time cost = 26 secs) #> ✔ All word vectors have now been normalized.  as_tibble(dt3)  # just check #> # A tibble: 14,207 × 3 #>    word  vec         freq #>    <chr> <list>     <int> #>  1 the   <dbl [50]> 58797 #>  2 and   <dbl [50]> 32193 #>  3 a     <dbl [50]> 31783 #>  4 of    <dbl [50]> 29142 #>  5 to    <dbl [50]> 27218 #>  6 is    <dbl [50]> 21447 #>  7 in    <dbl [50]> 17861 #>  8 I     <dbl [50]> 14703 #>  9 that  <dbl [50]> 13618 #> 10 it    <dbl [50]> 13453 #> # … with 14,197 more rows most_similar(dt3, \"Ive\")  # evaluate performance #> [Word Vector] =~ Ive #> (normalized to unit length) #>         word   cos_sim row_id #>  1:    Youve 0.8343096   5898 #>  2:     Weve 0.8205638   6913 #>  3:     seen 0.8151530    110 #>  4:    youve 0.7832114    945 #>  5:       ve 0.7762302  12894 #>  6:    WORST 0.7755833   7108 #>  7:  funnier 0.7494815   3105 #>  8:  Columbo 0.7456561   3250 #>  9: funniest 0.7444905   1539 #> 10:      ive 0.7390809   3494 most_similar(dt3, ~ man - he + she, topn=5)  # evaluate performance #> [Word Vector] =~ man - he + she #> (normalized to unit length) #>        word   cos_sim row_id #> 1:    woman 0.8709031    261 #> 2:     girl 0.7755713    299 #> 3: salesman 0.7740031   5594 #> 4: henchman 0.7690305  12263 #> 5:  caveman 0.7432090  12884 most_similar(dt3, ~ boy - he + she, topn=5)  # evaluate performance #> [Word Vector] =~ boy - he + she #> (normalized to unit length) #>        word   cos_sim row_id #> 1:     girl 0.7899726    299 #> 2:    woman 0.7634606    261 #> 3:     boys 0.7044068   1045 #> 4: teenager 0.6781507   2364 #> 5:      kid 0.6713795    676"},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/news/index.html","id":"new-features-0-1-4","dir":"Changelog","previous_headings":"","what":"New Features","title":"PsychWordVec 0.1.4 (ongoing…)","text":"New sum_wordvec() function: Calculate sum vector multiple words. New dict_expand() function: Expand dictionary similar words, based most_similar(). New dict_reliability() function: Reliability analysis (Cronbach’s α) Principal Component Analysis (PCA) dictionary. Note Cronbach’s α may misleading number items/words large. New plot_similarity() function: Visualize cosine similarities word pairs style correlation matrix plot. New tab_similarity_cross() function: wrapper tab_similarity() tabulate cosine similarities n1 * n2 word pairs two sets words (arguments: words1, words2). New orth_procrustes() function: Orthogonal Procrustes matrix alignment. Users can input either two matrices word embeddings two wordvec objects loaded data_wordvec_load() transformed matrices as_wordvec().","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/news/index.html","id":"minor-changes-0-1-4","dir":"Changelog","previous_headings":"","what":"Minor Changes","title":"PsychWordVec 0.1.4 (ongoing…)","text":"Improved print.weat() print.rnd().","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/news/index.html","id":"psychwordvec-012-nov-2022","dir":"Changelog","previous_headings":"","what":"PsychWordVec 0.1.2 (Nov 2022)","title":"PsychWordVec 0.1.2 (Nov 2022)","text":"CRAN release: 2022-11-03","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/news/index.html","id":"new-features-0-1-2","dir":"Changelog","previous_headings":"","what":"New Features","title":"PsychWordVec 0.1.2 (Nov 2022)","text":"Added permutation test significance test_WEAT() test_RND(): Users can specify number permutation samples choose calculate either one-sided two-sided p value. can well reproduce results Caliskan et al.’s (2017) article. Added pooled.sd argument test_WEAT(): Users can choose method used calculate pooled SD effect size estimate WEAT. However, original approach proposed Caliskan et al. (2017) default highly suggested. Wrapper functions as_matrix() as_wordvec() data_wordvec_reshape(), can make easier reshape word embeddings data matrix “wordvec” data.table vice versa.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/news/index.html","id":"major-changes-0-1-2","dir":"Changelog","previous_headings":"","what":"Major Changes","title":"PsychWordVec 0.1.2 (Nov 2022)","text":"test_WEAT() test_RND() now changed element names S3 print method returned objects (new class weat rnd, respectively): elements $eff.raw, $eff.size, $eff.sum now deprecated replaced $eff, data.table containing overall raw/standardized effects permutation p value. new S3 print methods print.weat() print.rnd() can make tidy report test results directly type print returned object (see code examples). Improved command line interfaces using cli package. Improved welcome messages library(PsychWordVec).","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/news/index.html","id":"psychwordvec-010-aug-2022","dir":"Changelog","previous_headings":"","what":"PsychWordVec 0.1.0 (Aug 2022)","title":"PsychWordVec 0.1.0 (Aug 2022)","text":"CRAN release: 2022-08-22 CRAN initial release. Fixed issues CRAN manual inspection.","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/news/index.html","id":"new-features-0-0-8","dir":"Changelog","previous_headings":"","what":"New Features","title":"PsychWordVec 0.0.8 (Aug 2022)","text":"Added wordvec primary class word vectors data: Now data classes contain wordvec, data.table, data.frame, actually perform data.table. New train_wordvec() function: Train word vectors using Word2Vec, GloVe, FastText algorithm multi-threading. New tokenize() function: Tokenize raw texts training word vectors. New data_wordvec_reshape() function: Reshape word vectors data dense (data.table new classs wordvec two variables word vec) plain (matrix word vectors) vice versa. New test_RND() function, tab_WEAT() renamed test_WEAT(): two functions serve convenient tools word semantic similarity analysis conceptual association test. New plot_wordvec_tSNE() function: Visualize 2-D 3-D word vectors dimensionality reduced using t-Distributed Stochastic Neighbor Embedding (t-SNE) method.","code":""},{"path":[]},{"path":"https://psychbruce.github.io/PsychWordVec/news/index.html","id":"new-features-0-0-6","dir":"Changelog","previous_headings":"","what":"New Features","title":"PsychWordVec 0.0.6 (Jul 2022)","text":"Enhanced functions. New data_wordvec_subset() function. Added unique argument tab_similarity(). Added support use regular expression pattern test_WEAT().","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/news/index.html","id":"psychwordvec-004-apr-2022","dir":"Changelog","previous_headings":"","what":"PsychWordVec 0.0.4 (Apr 2022)","title":"PsychWordVec 0.0.4 (Apr 2022)","text":"Initial public release GitHub functions.","code":""},{"path":"https://psychbruce.github.io/PsychWordVec/news/index.html","id":"psychwordvec-001-mar-2022","dir":"Changelog","previous_headings":"","what":"PsychWordVec 0.0.1 (Mar 2022)","title":"PsychWordVec 0.0.1 (Mar 2022)","text":"Basic functions WordVector_RData.pdf file.","code":""}]
