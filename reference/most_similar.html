<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="description" content="Find the Top-N most similar words, which replicates the results produced
by the Python gensim module most_similar() function.
(Exact replication of gensim requires the same word vectors data,
not the demodata used here in examples.)"><title>Find the Top-N most similar words. — most_similar • PsychWordVec</title><!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png"><link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png"><link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png"><link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png"><link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png"><link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png"><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.1.3/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.1.3/bootstrap.bundle.min.js"></script><link href="../deps/Nunito-0.4.2/font.css" rel="stylesheet"><link href="../deps/Nunito_Sans-0.4.2/font.css" rel="stylesheet"><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous"><!-- bootstrap-toc --><script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Find the Top-N most similar words. — most_similar"><meta property="og:description" content="Find the Top-N most similar words, which replicates the results produced
by the Python gensim module most_similar() function.
(Exact replication of gensim requires the same word vectors data,
not the demodata used here in examples.)"><meta property="og:image" content="https://psychbruce.github.io/PsychWordVec/logo.png"><!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">PsychWordVec</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.3</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="active nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul><form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off"></form>

      <ul class="navbar-nav"><li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/psychbruce/PsychWordVec/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul></div>

    
  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Find the Top-N most similar words.</h1>
      <small class="dont-index">Source: <a href="https://github.com/psychbruce/PsychWordVec/blob/HEAD/R/PsychWordVec.R" class="external-link"><code>R/PsychWordVec.R</code></a></small>
      <div class="d-none name"><code>most_similar.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>Find the Top-N most similar words, which replicates the results produced
by the Python <code>gensim</code> module <code>most_similar()</code> function.
(Exact replication of <code>gensim</code> requires the same word vectors data,
not the <code>demodata</code> used here in examples.)</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">most_similar</span><span class="op">(</span><span class="va">data</span>, <span class="va">x</span>, topn <span class="op">=</span> <span class="fl">10</span>, keep <span class="op">=</span> <span class="cn">FALSE</span>, above <span class="op">=</span> <span class="cn">NULL</span>, verbose <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>
    <dl><dt>data</dt>
<dd><p>A <code>data.table</code> (of new class <code>wordvec</code>)
loaded by <code><a href="data_wordvec_load.html">data_wordvec_load</a></code>.</p></dd>


<dt>x</dt>
<dd><p>Can be one of the following:</p><ul><li><p>a single word:  <code>"China"</code></p></li>
<li><p>a list of words:  <code>c("king", "queen")</code></p>
<p><code>cc(" king , queen ; man | woman")</code></p></li>
<li><p>an R formula (<code>~ xxx</code>) specifying
  words that positively and negatively
  contribute to the similarity (for word analogy):  <code>~ boy - he + she</code>  <code>~ king - man + woman</code>  <code>~ Beijing - China + Japan</code></p></li>
</ul></dd>


<dt>topn</dt>
<dd><p>Top-N most similar words. Defaults to <code>10</code>.</p></dd>


<dt>keep</dt>
<dd><p>Keep words specified in <code>x</code> in results?
Defaults to <code>FALSE</code>.</p></dd>


<dt>above</dt>
<dd><p>Defaults to <code>NULL</code>. Can be one of the following:</p><ul><li><p>a threshold value to find all words with cosine similarities
  higher than this value</p></li>
<li><p>a critical word to find all words with cosine similarities
  higher than that with this critical word</p></li>
</ul><p>If both <code>topn</code> and <code>above</code> are specified, <code>above</code> wins.</p></dd>


<dt>verbose</dt>
<dd><p>Print information to the console? Defaults to <code>TRUE</code>.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    

<p>A <code>data.table</code> with the most similar words and their cosine similarities.
The row number of each word in the raw data is also returned,
which may help determine the relative word frequency in some cases.</p>


<p>Two attributes are appended to the returned <code>data.table</code> (see examples):
<code>wordvec</code> and <code>wordvec.formula</code>.
Users may extract them for further use.</p>
    </div>
    <div class="section level2">
    <h2 id="download">Download<a class="anchor" aria-label="anchor" href="#download"></a></h2>
    

<p>Download pre-trained word vectors data (<code>.RData</code>):
<a href="https://psychbruce.github.io/WordVector_RData.pdf" class="external-link">https://psychbruce.github.io/WordVector_RData.pdf</a></p>
    </div>
    <div class="section level2">
    <h2 id="see-also">See also<a class="anchor" aria-label="anchor" href="#see-also"></a></h2>
    <div class="dont-index"><p><code><a href="cosine_similarity.html">cosine_similarity</a></code></p>
<p><code><a href="pair_similarity.html">pair_similarity</a></code></p>
<p><code><a href="tab_similarity.html">tab_similarity</a></code></p></div>
    </div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="va">d</span> <span class="op">=</span> <span class="fu"><a href="data_wordvec_normalize.html">data_wordvec_normalize</a></span><span class="op">(</span><span class="va">demodata</span><span class="op">)</span></span></span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> <span style="color: #00BB00;">✔</span> All word vectors have now been normalized.</span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="fu">most_similar</span><span class="op">(</span><span class="va">d</span>, <span class="st">"China"</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> <span style="color: #00BBBB;">[Word Vector]</span> =~ China</span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> (normalized to unit length)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>          word   cos_sim row_id</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  1:   Chinese 0.7678081    924</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  2:   Beijing 0.7648461   2086</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  3:    Taiwan 0.7081156   3011</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  4:  Shanghai 0.6727434   3932</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  5:  Shenzhen 0.6239033   7984</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  6: Guangzhou 0.6223897   7986</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  7:      yuan 0.6005429   4619</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  8:     India 0.6004212    485</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  9:     Japan 0.5967756    821</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> 10:        Li 0.5882897   7558</span>
<span class="r-in"><span><span class="fu">most_similar</span><span class="op">(</span><span class="va">d</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"king"</span>, <span class="st">"queen"</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> <span style="color: #00BBBB;">[Word Vector]</span> =~ king + queen</span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> (normalized to unit length)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>          word   cos_sim row_id</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  1:     royal 0.5985594   6754</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  2:     Queen 0.5487501   4267</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  3:      King 0.4662653   1449</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  4:    legend 0.3730853   5428</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  5:     crown 0.3673733   5174</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  6: superstar 0.3652350   7218</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  7:  champion 0.3587134   1509</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  8:      lady 0.3581903   4631</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  9:     lover 0.3425792   7947</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> 10:      pope 0.3420032   6388</span>
<span class="r-in"><span><span class="fu">most_similar</span><span class="op">(</span><span class="va">d</span>, <span class="fu"><a href="https://psychbruce.github.io/bruceR/reference/cc.html" class="external-link">cc</a></span><span class="op">(</span><span class="st">" king , queen ; man | woman "</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> <span style="color: #00BBBB;">[Word Vector]</span> =~ king + queen + man + woman</span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> (normalized to unit length)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>            word   cos_sim row_id</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  1:        girl 0.6387542   1257</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  2:         boy 0.5966774   1379</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  3:    teenager 0.5941694   4345</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  4:        lady 0.5923386   4631</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  5: grandmother 0.5081522   5364</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  6:      mother 0.5000261    754</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  7:       lover 0.4826535   7947</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  8:      victim 0.4806608   1537</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  9:   boyfriend 0.4717627   4811</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> 10:  girlfriend 0.4709186   3922</span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># the same as above:</span></span></span>
<span class="r-in"><span><span class="fu">most_similar</span><span class="op">(</span><span class="va">d</span>, <span class="op">~</span> <span class="va">China</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> <span style="color: #00BBBB;">[Word Vector]</span> =~ China</span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> (normalized to unit length)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>          word   cos_sim row_id</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  1:   Chinese 0.7678081    924</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  2:   Beijing 0.7648461   2086</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  3:    Taiwan 0.7081156   3011</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  4:  Shanghai 0.6727434   3932</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  5:  Shenzhen 0.6239033   7984</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  6: Guangzhou 0.6223897   7986</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  7:      yuan 0.6005429   4619</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  8:     India 0.6004212    485</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  9:     Japan 0.5967756    821</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> 10:        Li 0.5882897   7558</span>
<span class="r-in"><span><span class="fu">most_similar</span><span class="op">(</span><span class="va">d</span>, <span class="op">~</span> <span class="va">king</span> <span class="op">+</span> <span class="va">queen</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> <span style="color: #00BBBB;">[Word Vector]</span> =~ king + queen</span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> (normalized to unit length)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>          word   cos_sim row_id</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  1:     royal 0.5985594   6754</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  2:     Queen 0.5487501   4267</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  3:      King 0.4662653   1449</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  4:    legend 0.3730853   5428</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  5:     crown 0.3673733   5174</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  6: superstar 0.3652350   7218</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  7:  champion 0.3587134   1509</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  8:      lady 0.3581903   4631</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  9:     lover 0.3425792   7947</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> 10:      pope 0.3420032   6388</span>
<span class="r-in"><span><span class="fu">most_similar</span><span class="op">(</span><span class="va">d</span>, <span class="op">~</span> <span class="va">king</span> <span class="op">+</span> <span class="va">queen</span> <span class="op">+</span> <span class="va">man</span> <span class="op">+</span> <span class="va">woman</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> <span style="color: #00BBBB;">[Word Vector]</span> =~ king + queen + man + woman</span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> (normalized to unit length)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>            word   cos_sim row_id</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  1:        girl 0.6387542   1257</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  2:         boy 0.5966774   1379</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  3:    teenager 0.5941694   4345</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  4:        lady 0.5923386   4631</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  5: grandmother 0.5081522   5364</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  6:      mother 0.5000261    754</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  7:       lover 0.4826535   7947</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  8:      victim 0.4806608   1537</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  9:   boyfriend 0.4717627   4811</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> 10:  girlfriend 0.4709186   3922</span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="fu">most_similar</span><span class="op">(</span><span class="va">d</span>, <span class="op">~</span> <span class="va">boy</span> <span class="op">-</span> <span class="va">he</span> <span class="op">+</span> <span class="va">she</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> <span style="color: #00BBBB;">[Word Vector]</span> =~ boy - he + she</span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> (normalized to unit length)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>            word   cos_sim row_id</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  1:        girl 0.8635271   1257</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  2:       woman 0.6822032    558</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  3:      mother 0.6530156    754</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  4:    daughter 0.6431009   1078</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  5:    teenager 0.6310561   4345</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  6:       child 0.5933921    702</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  7: grandmother 0.5800967   5364</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  8:        teen 0.5755065   3542</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  9:        baby 0.5672891   1838</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> 10:       girls 0.5599151   1195</span>
<span class="r-in"><span><span class="fu">most_similar</span><span class="op">(</span><span class="va">d</span>, <span class="op">~</span> <span class="va">Jack</span> <span class="op">-</span> <span class="va">he</span> <span class="op">+</span> <span class="va">she</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> <span style="color: #00BBBB;">[Word Vector]</span> =~ Jack - he + she</span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> (normalized to unit length)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>        word   cos_sim row_id</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  1:    Jane 0.6338590   5502</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  2: Rebecca 0.6054167   6896</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  3:   Julie 0.5988102   5721</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  4:   Sarah 0.5867900   3717</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  5:     Amy 0.5831094   4899</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  6:   Susan 0.5812214   4141</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  7:    Lisa 0.5766206   4521</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  8:     Ann 0.5765654   4955</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  9:   Alice 0.5649283   7340</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> 10:   Carol 0.5647291   5861</span>
<span class="r-in"><span><span class="fu">most_similar</span><span class="op">(</span><span class="va">d</span>, <span class="op">~</span> <span class="va">Rose</span> <span class="op">-</span> <span class="va">she</span> <span class="op">+</span> <span class="va">he</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> <span style="color: #00BBBB;">[Word Vector]</span> =~ Rose - she + he</span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> (normalized to unit length)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>         word   cos_sim row_id</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  1:  Leonard 0.4443190   6245</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  2:   Martin 0.4206247   1425</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  3:   Thomas 0.4078561   1214</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  4:  Wallace 0.3833057   4350</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  5:  Francis 0.3793813   5220</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  6:  Johnson 0.3757961    761</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  7:    Allen 0.3736620   2257</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  8: Robinson 0.3725750   2698</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  9:    Evans 0.3639734   3379</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> 10:   Duncan 0.3635448   4738</span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="fu">most_similar</span><span class="op">(</span><span class="va">d</span>, <span class="op">~</span> <span class="va">king</span> <span class="op">-</span> <span class="va">man</span> <span class="op">+</span> <span class="va">woman</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> <span style="color: #00BBBB;">[Word Vector]</span> =~ king - man + woman</span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> (normalized to unit length)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>         word   cos_sim row_id</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  1:    queen 0.7118192   7852</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  2:    royal 0.4938203   6754</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  3:    Queen 0.4346379   4267</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  4:     King 0.3749903   1449</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  5:      she 0.3341126     65</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  6:     lady 0.3282869   4631</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  7:   mother 0.3241257    754</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  8:    crown 0.3164823   5174</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  9:     hers 0.3073009   7981</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> 10: daughter 0.3021213   1078</span>
<span class="r-in"><span><span class="fu">most_similar</span><span class="op">(</span><span class="va">d</span>, <span class="op">~</span> <span class="va">Tokyo</span> <span class="op">-</span> <span class="va">Japan</span> <span class="op">+</span> <span class="va">China</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> <span style="color: #00BBBB;">[Word Vector]</span> =~ Tokyo - Japan + China</span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> (normalized to unit length)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>          word   cos_sim row_id</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  1:   Beijing 0.8216199   2086</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  2:  Shanghai 0.7951419   3932</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  3: Guangzhou 0.6529652   7986</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  4:   Chinese 0.6439487    924</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  5:  Shenzhen 0.6439113   7984</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  6:     Seoul 0.5868835   5180</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  7:      yuan 0.5821307   4619</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  8:        Li 0.5712056   7558</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  9:      Wang 0.5192575   7083</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> 10:    Moscow 0.5082187   3163</span>
<span class="r-in"><span><span class="fu">most_similar</span><span class="op">(</span><span class="va">d</span>, <span class="op">~</span> <span class="va">Beijing</span> <span class="op">-</span> <span class="va">China</span> <span class="op">+</span> <span class="va">Japan</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> <span style="color: #00BBBB;">[Word Vector]</span> =~ Beijing - China + Japan</span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> (normalized to unit length)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>          word   cos_sim row_id</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  1:     Tokyo 0.8115592   3017</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  2:     Seoul 0.6568831   5180</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  3:  Japanese 0.6475989   1562</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  4: Pyongyang 0.5348969   6322</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  5:   Bangkok 0.4677356   6510</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  6:     Korea 0.4660699   3768</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  7:       yen 0.4631333   2695</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  8:    Taiwan 0.4330458   3011</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  9:    Moscow 0.4217667   3163</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> 10: Guangzhou 0.4154183   7986</span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="fu">most_similar</span><span class="op">(</span><span class="va">d</span>, <span class="st">"China"</span>, above<span class="op">=</span><span class="fl">0.7</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> <span style="color: #00BBBB;">[Word Vector]</span> =~ China</span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> (normalized to unit length)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>       word   cos_sim row_id</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> 1: Chinese 0.7678081    924</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> 2: Beijing 0.7648461   2086</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> 3:  Taiwan 0.7081156   3011</span>
<span class="r-in"><span><span class="fu">most_similar</span><span class="op">(</span><span class="va">d</span>, <span class="st">"China"</span>, above<span class="op">=</span><span class="st">"Shanghai"</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> <span style="color: #00BBBB;">[Word Vector]</span> =~ China</span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> (normalized to unit length)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>        word   cos_sim row_id</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> 1:  Chinese 0.7678081    924</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> 2:  Beijing 0.7648461   2086</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> 3:   Taiwan 0.7081156   3011</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> 4: Shanghai 0.6727434   3932</span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># automatically normalized for more accurate results</span></span></span>
<span class="r-in"><span><span class="va">ms</span> <span class="op">=</span> <span class="fu">most_similar</span><span class="op">(</span><span class="va">demodata</span>, <span class="op">~</span> <span class="va">king</span> <span class="op">-</span> <span class="va">man</span> <span class="op">+</span> <span class="va">woman</span><span class="op">)</span></span></span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> <span style="color: #BBBB00;">!</span> Results may be inaccurate if word vectors are not normalized.</span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> <span style="color: #00BB00;">✔</span> All word vectors have now been normalized.</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> <span style="color: #00BBBB;">[Word Vector]</span> =~ king - man + woman</span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> (normalized to unit length)</span>
<span class="r-in"><span><span class="va">ms</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span>         word   cos_sim row_id</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  1:    queen 0.7118192   7852</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  2:    royal 0.4938203   6754</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  3:    Queen 0.4346379   4267</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  4:     King 0.3749903   1449</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  5:      she 0.3341126     65</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  6:     lady 0.3282869   4631</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  7:   mother 0.3241257    754</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  8:    crown 0.3164823   5174</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  9:     hers 0.3073009   7981</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> 10: daughter 0.3021213   1078</span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/utils/str.html" class="external-link">str</a></span><span class="op">(</span><span class="va">ms</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Classes ‘wordvec’, ‘data.table’ and 'data.frame':	10 obs. of  3 variables:</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  $ word   : chr  "queen" "royal" "Queen" "King" ...</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  $ cos_sim: num  0.712 0.494 0.435 0.375 0.334 ...</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  $ row_id : int  7852 6754 4267 1449 65 4631 754 5174 7981 1078</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  - attr(*, ".internal.selfref")=&lt;externalptr&gt; </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  - attr(*, "dims")= int 300</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  - attr(*, "normalized")= logi TRUE</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  - attr(*, "wordvec")= num [1:300] -0.0055 -0.06705 -0.04519 0.03875 -0.00286 ...</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  - attr(*, "wordvec.formula")=Class 'formula'  language ~king - man + woman</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   .. ..- attr(*, ".Environment")=&lt;environment: 0x55fb31efff88&gt; </span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/attr.html" class="external-link">attr</a></span><span class="op">(</span><span class="va">ms</span>, <span class="st">"dims"</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [1] 300</span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/attr.html" class="external-link">attr</a></span><span class="op">(</span><span class="va">ms</span>, <span class="st">"normalized"</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [1] TRUE</span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/attr.html" class="external-link">attr</a></span><span class="op">(</span><span class="va">ms</span>, <span class="st">"wordvec.formula"</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> ~king - man + woman</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> &lt;environment: 0x55fb31efff88&gt;</span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/attr.html" class="external-link">attr</a></span><span class="op">(</span><span class="va">ms</span>, <span class="st">"wordvec"</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   [1] -0.0055029992 -0.0670502053 -0.0451917763  0.0387523404 -0.0028627068</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   [6] -0.0311541918  0.0722744639 -0.0547962213 -0.0020958381  0.1267937027</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [11] -0.1177978229 -0.1116193754 -0.0432157999  0.0087648568 -0.0212594939</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [16] -0.0361886454 -0.0145757764 -0.0391255380  0.0236719225  0.0679695260</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [21]  0.0825006128  0.0179503178 -0.0102744498  0.0096805959  0.0097418516</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [26] -0.0583066547 -0.1043284916  0.0066451406  0.0774529645 -0.0630043398</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [31]  0.0176932143 -0.0357660002 -0.0127675640  0.0952755743 -0.1004202304</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [36] -0.0307725420  0.0191817489  0.0342936599 -0.0381161430 -0.0036010425</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [41] -0.0222736867 -0.0654523468  0.0581761410  0.0386649344  0.0625108822</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [46]  0.0144455335 -0.0298451219  0.0179820393 -0.0070950816  0.0410382461</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [51]  0.0193902936  0.0009054039 -0.0824461123  0.0896711960 -0.1071550823</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [56]  0.0090518215 -0.0452947722 -0.0242425101  0.0499748883  0.0017788248</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [61]  0.0896162255  0.0778694536  0.0043599153  0.0777055479  0.0090822422</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [66] -0.0272763870  0.0142941177  0.0595841030 -0.0188500428  0.0720441406</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [71]  0.0662750676  0.0333189751 -0.0275026411  0.0473774977  0.0302065682</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [76]  0.0363555415 -0.0305552844  0.0234168307  0.1017575023  0.0411732703</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [81]  0.0094623327  0.0278133126 -0.0067087575  0.0304040180 -0.0629132792</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [86]  0.0630248067 -0.0340541402  0.0388550926  0.0546983130  0.0218934638</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [91]  0.0433205797 -0.1001378189 -0.0860689768 -0.0450885649  0.0072867091</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [96]  0.0317569448  0.0645638429 -0.0160440819  0.0639956819 -0.0188932674</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [101]  0.0053826412 -0.0191800667 -0.0261438897 -0.1247332173 -0.1010494892</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [106]  0.0587875995 -0.1849463247 -0.0674423763  0.0657176846 -0.0205700926</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [111]  0.0666514408  0.0815598320  0.0569012270 -0.0101953971  0.0484093475</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [116] -0.1185521372  0.0030313936 -0.0933326263  0.0544090501  0.0927743854</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [121]  0.0572259111  0.0394707134 -0.0271925329 -0.0265718034  0.0412028096</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [126]  0.0438836936  0.0418592306  0.0034390046  0.0029262269  0.0134619940</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [131]  0.0060545735  0.0513732397  0.0327279879  0.1054238844  0.0668562286</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [136]  0.0769686804 -0.0227940919 -0.0422079731  0.0703340490 -0.0155774872</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [141]  0.0372660027 -0.1356710009  0.0350093190  0.0236513594  0.0484942080</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [146] -0.0400414708  0.0188626798  0.0274173715 -0.0486294492  0.0038126260</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [151]  0.0133522101  0.0559058596  0.0022117502 -0.0837040985 -0.0849908637</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [156] -0.0533659570 -0.0608346184  0.0319297479  0.0282798544  0.0078445029</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [161] -0.0496067834 -0.0011413436  0.1585030627 -0.0257539005 -0.0556869160</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [166] -0.0376501897  0.0119025195 -0.0722575073 -0.0134277815  0.0713891951</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [171] -0.1004959954  0.0377733481  0.1079291741  0.0487607486  0.0176890786</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [176] -0.0642323537  0.0939240962  0.0323515907  0.0062917470  0.0628913182</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [181] -0.1168540632  0.0042852646 -0.1256198725 -0.0767676420  0.0159300785</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [186]  0.0294246554  0.0809427655  0.0357663118 -0.0218166578  0.0638628914</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [191]  0.0283086395 -0.0002039863 -0.0330815284  0.0067946163  0.0198031884</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [196]  0.0572634491  0.0025753907 -0.0179017809  0.0275942861 -0.0879453025</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [201] -0.0565598458  0.0206358787  0.0228473515 -0.0461282294 -0.0145729556</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [206] -0.0143933569  0.0095678299  0.0631869552  0.0659021305  0.0042216349</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [211]  0.0293649478  0.1033812547 -0.0516775577 -0.1253424596  0.0270994610</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [216]  0.0385621533 -0.0036571112 -0.0278117809  0.0295294855  0.0024058853</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [221] -0.0144733111  0.0180931805  0.0579378103 -0.0116669835  0.0087559466</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [226] -0.0283575865 -0.0278965844 -0.0272467575  0.0342410954  0.0878225024</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [231] -0.0741101358 -0.0684089597 -0.1740935244 -0.0044601583  0.0376029740</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [236]  0.0221138776  0.0207988624 -0.0113099903 -0.0275133395 -0.0000199454</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [241]  0.0563714279 -0.1243700668  0.0584321684  0.0294712221  0.0939427080</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [246] -0.0011363452 -0.0734356129  0.0425710531  0.0715560074 -0.0782001568</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [251]  0.0274794287  0.0326764105  0.0565690520 -0.0124150119  0.0545865281</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [256]  0.0131866584 -0.1333997208 -0.0673208573 -0.0237475804 -0.0439831482</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [261]  0.0048958304  0.0900008028 -0.0692655794  0.0177480897 -0.0185615675</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [266] -0.0438003657  0.0037071559  0.0152207093  0.0955843067  0.1068395259</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [271]  0.0044241142  0.1174178922 -0.0404572738  0.0672502162 -0.0424039690</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [276] -0.1056443456 -0.0766069551  0.0035567801 -0.0748190562  0.0503630702</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [281]  0.0909204843 -0.0668049051  0.0951348101 -0.0171847044  0.0625944510</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [286] -0.0035357800 -0.0205028927 -0.0161002229 -0.0182009416  0.0601725192</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [291] -0.0265177060 -0.0849906153 -0.0761131339 -0.1206830347  0.0004351351</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [296]  0.0062921084 -0.0137075363  0.0623639418  0.0806824072  0.0592501645</span>
<span class="r-in"><span><span class="co"># final word vector computed according to the formula</span></span></span>
<span class="r-in"><span></span></span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p></p><p>Developed by Han-Wu-Shuang Bao.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p><p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.6.</p>
</div>

    </footer></div>

  

  

  </body></html>

